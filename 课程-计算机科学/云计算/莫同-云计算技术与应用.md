
技术：编程模型+分布式海量数据存储+大数据管理+资源虚拟化+资源管理调度

基本信息：
- 成绩：10平时+40实验+50考试=100
- 分组：3-6
- 形式：ppt+报告

240920：

![[Pasted image 20240920143944.png]]

## sec01-初识云计算

什么是云计算：
- 云：规模大、可扩展的虚拟网络介质
- 计算

云计算发展历程：
- 单个超级计算机
- 计算集群：1960-
- 分布式计算：任务分割->分布计算->结果合并
- 网格计算：1990-
- 效用计算
- 云计算：云+网络，通过网络自动分划/执行计算

资源虚拟化

面向服务的架构（service-oriented architecture，SOA）：
- 开放式数据模型+统一通信标准+更丰富的服务+更松散灵活的IT架构

软件即服务（software as a service，SaaS）
- 大公司负责基础设施，小公司负责创新
- 长尾理论

Web 2.0

*云计算的概念*：
- 云计算是一种能够将动态伸缩的虚拟化资源通过互联网以服务的方式提供给用户的计算模式。-by wiki
- 云计算是一种共享的网络交付信息服务的模式，云服务的使用者看到的只有服务本身，而不用关心相关基础设施的具体实现。

*云计算的分类*：灵活度和难易度递减
- 基础设施云：仅提供计算和存储设施
- 平台云：应用托管
- 应用云：个性化应用需求

服务方式：公有云、私有云、混合云

## sec03-云计算与服务

topics：
- 服务
- 云与服务
- 典型应用

### 3.1-服务

服务的供需：
- 供给驱动/需求驱动
- 搭建一个交互平台，使供需双方交互

交互平台的本质：
- 一个资源池，供需双方放入/取出
- 宏观上服务能力无限（可动态扩展），微观上服务能力有限（需要分配回收管控）
- 共性问题：
	- 虚拟化：服务的描述和访问
	- 分布式计算：大规模处理的效率问题
	- 管控策略：资源分配回收及其他管控问题

虚拟化：
- 资源描述：我有什么，你要什么
- 访问方式描述：怎么取出，怎么放入
- 虚拟化是资源在IT 世界中的抽象化“描述”技术

*分布式计算*：
- 背景：单机模式不能满足需求，分布式并行化是大势所趋
- 海量、高并发、存储要求、计算要求
- 通过架构的设计，将问题的瓶颈进行转换，以便提升

*管控策略*：
- 有了资源池之后，资源的放入和取出是关键
- 资源的管理
- 几乎所有的业务逻辑实际上都是资源的分配和回收策略

*什么是服务*：
- 通过一系列**活动**，满足对方的**需求**
- 以用户需求的满足度为核心，活动为主、实物为辅或无实物
- 提供服务与提供产品的区别（*服务的特点*）：
	- 产品趋于标准化、流水线化，服务趋于*个性化*
	- 服务中，用户的*参与度*更高
	- 产品的评价方式简单，服务则以顾客的*满意度*衡量
	- 产品注重实体属性，服务注重*价值*体现

### 3.2-云与服务

*云计算*：
- 资源服务化
- 计算和存储资源虚拟化
- 服务的可伸缩性、可用性、安全性

云平台模式：
- 输入资源->云平台->输出服务

对IT的需求：
- 应用层SaaS：软件即服务，面向最终用户
- 平台层PaaS：平台即服务，面向应用软件
- 基础设施层IaaS：基础设施即服务，面向软件

*云平台特点/优势*：
- 硬件与软件都是资源
- 资源可以根据需要进行动态扩展与配置：按需分配
- 按用计费、无需管理
- 物理上分布式共享，逻辑上以单一整体呈现
- 优化产业布局
- 推进专业分工：云计算厂商+科研+运维+开发
- 提升资源利用率
- 减少初期投资：更少的基础设施/人力/软件投入，更灵活的转型支持
- 降低管理开销：服务化管理，随机应变

### 3.3-典型应用

案例：
- Google：大数据存储与分析处理
- 12306：资源的弹性配置管理
- VMWare：IT资源管理与快速配置
- BAE/GAE：快速部署与实施

*DevOps*：
- Dev=开发，Ops=运维
- 目标：软件周期全流程自动化+全面监控
- 实施：组织、技术、流程、文化
- 动因：开发与上线分离，敏捷开发的诉求，配套技术（微服务+容器）的发展，开发的自动化
- 核心功能：自动打包/升级部署，用例管理和自动执行，用例开发框架，用例执行框架

*中台*：
- 概念：
	- 与前台（用户交互部分）和后台（数据管理+运行逻辑）相对的概念
	- 将“后台”中那些针对技术，业务，组织的通用“模块 服务”从原来特定的项目中抽离出来，并且使之能够成为一个自治的服务复用到多个项目中提供给更多的“前台”使用
- 分类：
	- 业务中台：
	- 应用中台：
	- 技术中台：数据存储与管理，数据分析处理，功能API，消息发布订阅
	- 数据中台

## sec04-典型云服务

topics：
- IaaS
- PaaS
- SaaS

### 4.1-IaaS

基础设施层服务（*IaaS*）：
- 经过虚拟化后的硬件资源和相关管理功能的集合
- 硬件资源：计算、存储和网络等资源
- 管理功能：物理资源虚拟化、内部流程自动化、资源管理优化，对外提供动态灵活的资源服务

*IaaS核心功能*：
- 资源抽象：
	- 定义：硬件虚拟化，屏蔽不同平台的差异，实行统一管理
	- 核心问题：对资源进行粒度划分并管理
	- 粒度分级：虚拟机<集群<虚拟数据中心<云
- 资源监控：
	- 对不同资源采取不同监控方式，保障基础设施层高效工作，是负载管理的前提
	- 监控层次：根据资源力度层次而定
	- 监控对象：物理单元、逻辑单元、解决方案
- 负载管理：
	- 动机：负载过低造成浪费，负载过高影响上层服务
	- 核心问题：负载平衡与优化问题（负载对性能/资源的影响，优化算法）
- 数据管理：
	- 多种数据并存，物理上分布式存储
	- 挑战：
		- 完整性：状态确定且一致，写操作同步，读写要么成功要么回滚
		- 可靠性：数据损坏和丢失概率最小，可从备份中恢复
		- 可管理性：相应的操作支持粗粒度、逻辑简单的管理
- 资源部署：
	- 分次、自动化部署，使基础设施服务变得可用
	- 典型场景：
		- 支持动态资源可伸缩性：负载过高时，增加实例
		- 故障恢复与硬件维护：多节点的数据冗余
- 安全管理：
	- 保证基础设施资源被合法地访问和使用
	- 云安全特点：环境更加开放，用户不保有数据，云内部破坏力更大
	- 影响云安全的因素：恶意程序、硬件故障、管理人员泄露、外在因素
- 计费管理：变买为租，按量计费

*IaaS实现步骤*：
- 总体设计：物理设施->虚拟化->管理系统->服务
- 服务流程：规划->部署->运行
- 规划阶段：
	- 虚拟化：准备物理环境、虚拟化改造、整合计算资源、建立统一接口
	- 用户访问及管理：允许用户通过客户端程序远程访问、获取资源
	- 数据管理：业务数据的管理、虚拟镜像文件的管理
- 部署阶段：分配资源->描述资源配置->接收激活命令->启动配置实例
- 运行阶段：
	- 虚拟机管理
	- 资源监控：黑盒管理（虚拟机资源监控）、白盒管理（软件资源监控）
	- 负载管理：基于资源，黑盒进行负载协调，白盒得到虚拟机负载
	- 安全管理：管理域、集成管理
	- 用户服务（账户计费、运行监控）

IaaS总体设计：

![[Pasted image 20241121162834.png]]

### 4.2-PaaS

*平台层服务*（PaaS）：
- 介于基础设施层和应用层之间
- 具有通用性和可复用性的软件资源集合，为云应用提供开发、运行、管理和监控的环境
- 更好的满足云应用的可伸缩性、可用性和安全性

*PaaS核心功能*：
- 开发平台：平台层是其上运行应用的开发平台
	- 应用模型：编程语言、元数据模型、打包发布格式
	- API代码库
	- 开发测试环境：离线开发测试+上传模式
- 运行时环境：
	- 任务：打包/部署应用上线，并根据元数据进行配置
	- 隔离性：业务隔离、数据隔离、应用间隔离、用户间隔离
	- 可伸缩性：根据业务规模和工作负载，分配计算能力、存储、网络带宽等资源
	- 资源可复用性：宏观上无限，微观上有限
- 运营环境：
	- 应用更新
	- 应用升级：升级/上传补丁，升级失败恢复
	- 应用监控：应用运行状态（负载/异常）、监控手段（响应时间/吞吐量/负载/请求量）、图形化展示
	- 资源消耗监控
	- 应用卸载
	- 应用计费统计：业务使用/资源使用统计

*PaaS实现步骤*：
- 总体设计
- 开发平台：应用模型（如J2EE）、服务提供方式（REST、SOAP）、核心API（SDK）、发布格式（JAR包）、开发测试环境
- 运行环境：可伸缩实现机制
	- 方案A：每个应用与其数据库打包为一个实例。部署简单，灵活性受限，延迟较低。
	- 方案B：每个应用和每个数据库单独打包。灵活性更高，部署较复杂，延迟较高。
- 运营环境

### 4.3-SaaS

*应用层服务*（SaaS）：
- 建立在平台层之上，云上应用软件的集合
- 种类：
	- 面向广大受众的标准化应用
	- 定制的个性化服务应用
	- 用户开发的多元化应用

*应用层的特征*：
- 网络访问模式：浏览器模式、客户端模式
- 租用付费：按用量付费
- 高度整合：用户需求的综合性，用户体验为导向

![[Pasted image 20241121205551.png]]

*应用层的应用分类*：
- 标准应用：运行在平台层，服务质量约等于本地运行，功能丰富，通信效率高
- 客户应用：通用模板，部分定制化，迁移较困难
- 多元应用：高度定制化

![[Pasted image 20241121211114.png]]

*Serverless*：
- 组成：Serverless=FaaS+BaaS
	- FaaS：Serverless架构允许用户以一种更加“代码碎片化”的方式开发软件，将软件服务更加细粒度到函数层面，通过函数即服务Function as a Service（FaaS）的组合构建自己的代码
	- BaaS：FaaS和对应的支撑服务后端即服务Backend as a Service（BaaS）共同构成了Serverless
- 工作流程：编写代码->上传平台->触发函数实例->实例弹性执行各函数->按量计费
- 函数操作（P86）
	- 版本控制、别名
	- 事件源与函数关联
	- 函数及工作量要求
	- 函数调用：同步请求（HTTPS等）、异步消息队列（RabbitMQ、AWS、SNS等）、消息/记录流（Kafka等）、批量作业（ETL等）
- Serverless vs SaaS：
	- Serverless是SaaS基础上演进的结果，不是简单归属于SaaS

## sec05-虚拟化技术

### 5.1-虚拟化技术概述

云计算的核心思想是服务：
- 计算资源服务化：
	- 核心挑战：解耦、建立资源池、资源分配回收、分配资源间独立性
- 开发平台服务化
- 应用软件服务化
- XaaS

*什么是虚拟化*：
- 将原本存在于真实环境上的计算机系统或组件运行在虚拟出来的环境中，解除真实环境中各个层次的耦合关系
- 虚拟化与云的关系：
	- 云是一种平台模式，把资源汇聚，然后再以服务方式对外提供
	- 本质上是虚拟表示的汇聚：实际的使用仍然是“实体”，但通过虚拟化接口访问
- 虚拟化本质：在使用者和实体间增加一层抽象 ![[Drawing 2024-11-22 10.55.24.excalidraw|300]]
- 虚拟化发展历程：虚拟内存->JVM->服务器虚拟化
- 虚拟化典型类型：
	- 基础设施虚拟化：网络（VPN/VLAN）、存储虚拟化（RAID/NAS/区域存储网）
	- 系统虚拟化：Windows/Linux
	- 软件虚拟化：JVM
- 虚拟化典型架构：硬件层-VM-OS
- 为什么进行虚拟化：
	- 降低运营成本，提高应用兼容性，加速应用部署
	- 提高服务可用性，提升资源利用率，动态调度资源，降低能源消耗

### 5.2-服务器虚拟化

基本概念/特性/核心技术

服务器管理的主要问题：
- 整合：增加服务器的平均性能，降低服务器数量
- 集中：多个设备的运行内容集中到单个设备
- 数据集成、应用集成

*服务器虚拟化*：
- 将虚拟化技术运用到服务器上，将物理服务器虚拟化为数个虚拟服务器
- 目标：高性能、可扩展和稳定，支持多种IT需求

服务器虚拟化的*核心技术*：
- 计算虚拟化：
	- CPU虚拟化：
		- 全虚拟化：软件模拟硬件设备，拦截执行VM内特权指令
		- 半虚拟化：改造OS内核，使VM与实际机共享底层调用
	- CPU硬件辅助虚拟化：硬件集成虚拟化逻辑
	- GPU虚拟化：
		- 虚拟显卡：虚拟网络计算机VNC、OpenGL伪库VMGL
		- 显卡直接调用：显卡分配给VM，VM使用对应驱动
		- 显卡虚拟化：VM获取和使用显卡时间片
- 存储虚拟化：
	- 内存虚拟化：
		- VM传统访存方式：VM逻辑地址GVA->VM物理地址GPA->OS逻辑地址HVA->OS物理地址HPA
		- 影子页表法SPT：为VM系统的每个进程维护一套影子页表，提供GVA到HPA的直接映射，由Hypervisor维护，由物理机MMU直接映射。缺页由Hypervisor处理
		- EPT/NPT：硬件上固化扩展的MMU，为每个VM维护GPA->HPA的映射。GVA->GPA由VM内部管理。缺页由宿主机OS处理
	- 磁盘虚拟化：
		- 本地存储虚拟化
		- 存储区域网络SAN（storage area network）：面向存储空间，用于块I/O
			- 磁盘阵列RAID：RAID1镜像备份、RAID3校验信息备份、RAID5交叉存储、RAID7独立运行
		- 网络接入存储NAS（network attached storage）：面向存储内容，用于文件I/O
- 设备与I/O虚拟化：
	- DMA
	- 软件方式实现：物理硬件I/O接口虚拟化
		- Intel VT-d：介于外设、内存、CPU之间，借助设备标记映射等方式，将外设到VM的DMA请求重定向到VM中
		- VMDirectPath I/O
- 网络虚拟化：网卡虚拟化、VM间网络通信、组织多台机器形成虚拟网络
	- 网卡资源虚拟化：虚拟网卡（vNIC）、物理网卡分时等
	- VM间通信：虚拟交换机VEB、虚拟网络设备TAP/TUN/Bridge
	- 虚拟网络：
		- 虚拟局域网VLAN：
		- VPN
- 实时迁移技术：将整个虚拟机的运行状态完整、快速地从原宿主机的硬件平台转移到新的宿主机硬件平台
	- 热迁移应用领域：计算机共享、关键备份、环境重现、系统硬件维护

*容器虚拟化*：
- 容器思想
- LXC（Linux Container）：提供轻量级虚拟化的内核虚拟化技术，轻量级隔离，比纯虚拟化节省开销
	- 实现：资源管理依赖cgroups子系统，隔离控制依赖namespace特性
- Docker：基于进程容器的轻量级VM解决方案
	- 高级封装
	- 额外功能：统一的打包部署运行方案，版本控制，镜像重用/共享
	- 特点：跨平台，运行在主机OS中，数个容器可共享镜像
	- 概念：
		- 镜像：创建容器用的只读模板
		- 容器：可运行的实例，包含运行应用的所有内容
		- 仓库：存储镜像的服务器
	- 实现：隔离性、可配额、移动性、安全性

### 5.3-其他虚拟化技术

![[Pasted image 20241123170805.png]]

桌面虚拟化技术：
- 架构：瘦终端、网络接入、控制台、身份认证、OS与应用
- 特点：桌面环境与终端分离、安全问题、简化轻量级客户端架构、方便备份恢复

虚拟机 vs 容器 vs Serverless：

![[Pasted image 20241123171114.png]]

代表性虚拟机：
- KVM（kernel-based virtual machine）
- Xen
- VMWare ESXi
- Hyper-V

虚拟化的作用：
- 增加一个逻辑表示层
- 将直接操作变为对逻辑表示内容的操作
- 便于增加管控逻辑+高效实现管控
- 实现隔离

## sec06-虚拟化资源管理

### 6.1-虚拟化资源管理工具概述

开源IaaS平台：
- Eucalyptus：最早复刻AWS的平台
	- 云控制台（CLC）：核心控制组件，转发各类请求
	- 集群控制器（CC）：通过与NC交互实现实例管理
	- 存储控制器（SC）、Walrus：提供存储服务
	- 节点控制器（NC）：实际控制各虚拟机实例
- OpenNebula
	- 接口层：原生XML-RPC接口，实现多种API
	- 核心层：OpenNebula core 提供统一的Hook 插件管理、Request 请求管理、VM 生命周期管理、Hypervisor 管理、网络资源管理和存储资源管理等核心功能。core 配合Scheduler 对外提供计算和存储网络资源管理服务
	- 最底层：由各种Driver 构成的驱动层与虚拟化软件（KVM 、XENXEN）和物理基础设施交互
- CloudStack：
	- 管理节点：负责整个CloudStack的IaaS平台管控
	- 计算节点Host：提供真实的计算资源
	- 资源集群Cluster：由一组型号相同的计算节点组成的集群
	- Pod：对应现实世界的机柜，组织若干计算节点的逻辑单元
	- Zone：对应现实世界的数据中心，包含多个Pod
- OpenStack：AWS开源实现
	- 核心功能需求：云拥有者、服务提供者、服务使用者
	- 功能架构、概念架构、逻辑架构
	- 单节点/双节点/多节点部署

AWS：Amazon Web Services
- 特点：一切皆服务，通过web service接口开放数据和功能，完全SOA（面向服务架构）模式，认证授权、按需使用、按用计费
- 服务：访问层、通用服务层、PaaS层服务、IaaS层服务
- 层次：IaaS资源->PaaS+通用服务->用户应用

私有云需求：
- 选择不同的计算虚拟技术
- 存储技术、网络技术和设备的多样支持
- API的多样支持
- 松耦合（通过组合组件、模块和服务构成整个系统）、功能内聚（组件/模块/服务功能结合，以便开发维护）

### 6.2-镜像管理

镜像：计算、存储服务的载体

### 6.3-计算管理

OpenStack计算管理：Nova

Compute架构（P37）

Compute工作过程（P38-49）

虚机创建过程

Nova支持的调度器+过滤器

### 6.4-权限控制

OpenStack权限管理：Keystone服务：
- Identity服务：凭据校验、用户/租户/角色数据
	- User：使用OpenStack的实际用户/系统/服务的代表
	- Tenant/Project：一个隔离或分类资源的容器，对应现实中的一个客户
	- Role：一系列权限和优先级的集合
- Token服务：提供对token的校验和管理
	- token是服务和客户间交换的凭据
	- 类别：scoped、unscoped
- Catalog服务：
	- Service：指代一个OpenStack服务，如Nova、Glance等，提供一或多个endpoints
	- Endpoints：一个网络访问点
- Policy服务：基于规则的授权和策略管理
	- 基于角色规则：要求提交请求的用户具有特定身份
	- 基于领域规则：要求请求访问的资源符合某种条件
	- 通用规则：要求请求资源的某些属性与发起请求用户的某些属性符合均满足某些要求

权限控制过程（P59-60）

Keystone特点：
- 基于token：用户通过认证取得token，通过token访问服务
- 基于租户管理资源

### 6.5-网络管理

OpenStack网络管理：Quantum（Neutron）
- 提供被管理设备间的网络连接服务，允许用户自定义网络并使用
- 支持plugin（SDN 和openflow）
- 自定义子网地址，公有/私有网络，以及floating IP规则
- API：访问接口
- 抽象层：网络、子网、端口、路由

原理：

![[Drawing 2024-11-24 15.25.00.excalidraw|100%]]

Quantum好处都有啥：~~云服务亩产一千八~~
- 租户可创建/操作/监控若干私有网络
- API扩展：
- 新网络技术应对现有挑战
- Plugin技术允许支持更多的技术

网络架构：
- 管理网络：用作OpenStack组件的内部交流
- 数据网络：在云内交换VM数据
- 外部网络：为需要外部网络的VM提供网络服务
- API网络：对租户提供包括Quantum API在内的OpenStack API

网络类型：
- 单平面网络：路由器和租户同在单个子网
- 多平面网络：路由器连接数个子网，每个子网连接一系列租户
- 平面和私有混合网络：部分租户与路由器连接到公共子网，数个私网也会连接另一部分租户
- 带内部路由（资料称Provider Router）的私有网络：外部路由与内部路由连接到公共子网，内部路由连接到其他私有子网
- 带每租户路由的私有网络：每个租户对应一个内部路由，通过内网连接该租户的所有实例；所有内部路由与外部路由再连接

网络管理模式：
- IP模式：固定/浮动IP
- 连接模式：Flat/FlatDHCP/VLAN

### 6.6-存储管理

*块存储*：OpenStack Cinder
- 块存储单元：*卷*
	- 特点：可在VM间移动，多个卷可挂载到同一VM，单个卷同一时间只能用于一个VM
	- 基于卷的块存储管理：块设备到VM的创建/挂载/卸载
- Cinder架构：
	- API：接受REST请求并放入队列
	- Schedule：卷的分配（H版采用simple策略，即选择卷数量最少的活跃节点）
	- Volume Service管理存储空间
- Cinder存储：
	- 存储方式：本地、网络、iSCSI、EMC、IBM、etc.
	- H版新增：卷大小调整、卷备份、用户间的卷透明转移和交换、限速等QoS、更多的driver

*对象存储*：OpenStack Swift
- 存储：存储内容的组织形式+读写协议
- 核心概念：
	- Object：对象，基本存储实体
	- Container：容器，装载/组织若干对象
	- Account：账户，拥有若干容器
- 组件：Replication（副本）、Updaters（更新器）、Auditors（校验器）
- 架构：
	- 宏观架构：授权Node和代理Node对用户可见，代理Node提供对内网存储结点的访问
	- 具体架构：每个Zone对应一个代理Server，每个代理Server可访问数个存储节点；用户与代理之间访问需要进行负载均衡（load balancing）

![[Pasted image 20241125191450.png]]

### 6.7-容器管理

Kubernetes（k8s）：Google开源的容器编排引擎
- 特点：可移植（到不同云环境）、可扩展（模块/插件化、可挂载/组合）、自动化
- 架构
- Master组件：集群管理中心，在一台设备上运行
	- kube-apiserver：暴露k8s API
	- etcd：默认存储系统
	- kube-controller-manager：运行管理控制器，在后台处理常规任务。包括：节点（Node）控制器、副本（Replication）控制器、端点（Endpoints）控制器、Service Account 和Token 控制器
	- cloud-controller-manager：云控制器管理器，与底层云提供商的平台交互。包括节点（Node）控制器、路由（Route）控制器、Service 控制器、卷（Volume）控制器
	- kube-scheduler 监视新创建没有分配到Node 的Pod，为Pod 选择一个Node
	- 插件（addon）是实现集群pod 和Services 功能
- 节点组件：运行在Node，提供k8s运行环境，维护Pod
	- kubelet：主要的节点代理，监视已分配给节点的Pod
	- kube-proxy：通过在主机上维护网络规则并执行连接转发来实现Kubernetes 服务抽象
	- docker（或rkt）：运行容器
	- supervisord：轻量级监控系统，确保容器正常运行
	- fluentd：守护进程，提供cluster级别的日志
- 访问认证：身份认证（client、HTTP、token）、授权（功能/接口/端口的HTTP 访问许可）、配额
- 调度：
	- 预选：过滤不符合资源要求的节点
	- 优选：选择最为合适（策略最优）的节点
- Kubelet进程：
	- 每个Node 节点会启动一个kubelet 进程，处理master 发送到本节点的任务
	- 定期向master 汇报节点的资源使用情况
	- 创建、监控、销毁pod
	- Pod通过探针判断容器是否健康以及是否就绪

k8s架构：

![[Pasted image 20241125195114.png]]

## sec07-云存储

### 7.1-云存储基础

云计算提出的由来：
- 传统模式遭遇挑战
- 数据存储的需求：数据保存、数据共享/访问
- 对存储容量的需求

需要注意：
- 核心关键问题：组织结构+业务逻辑
- 索引技术
- 文件系统
- 适应各种情况：各种数据类型、各种读写要求、各种查找策略、扩展、负载均衡、容错

### 7.2-分布式文件系统

问题与挑战：成本、容量、可靠性、易用性

| \      | 传统文件系统       | 分布式文件系统    |
| ------ | ------------ | ---------- |
| 存储位置   | 元数据与内容数据集中存储 | 分离存储       |
| 容量扩展方式 | 纵向扩展         | 分布式横向扩展    |
| 冗余     | 较少           | 较多，用于提高可靠性 |
| 文件存储位置 | 集中存储         | 分块存储       |

文件系统设计：
- 考虑因素：最小存储单元、访问速度和使用率间的选择
- 安全性：多用户环境下的文件安全、R/W权限、访问控制
- 缓存：提升读写效率

*GFS*：Google分布式文件存储系统
- 假设与目标：
	- 认为*硬件出错是常态*而非错误，而非追求单节点的健壮
	- *主要负载是流数据读写*：主要是追加而非插入
	- 需要存储大尺寸文件
- 实现技巧：
	- 文件划分为大小一致的块（chunk）存储
	- 通过冗余提高可靠性
	- 通过单个master协调数据访问、元数据存储
	- 无缓存
- 架构：见下图
	- Client：GFS提供给应用的库文件形式访问接口
	- Master：唯一管理节点，保存系统元数据
	- chunkserver：数据存储节点，大小固定（一般为64MB）
- 写流程：见下图
- 系统特点：
	- 采用中心服务器模式：扩展chunkserver方便，Master掌握一切，不存在一致性问题
	- 不缓存数据：缓存对流式读写的性能提升不大，且与实际数据的一致性维护复杂
	- 用户态下实现：实现简单，调试方便
	- 专用访问接口：降低实现复杂度（不提供POSIX接口）
- Master节点：
	- 任务：存储元数据、文件系统目录管理、chunkserver周期性通信、数据块创建/复制/负载均衡、垃圾回收（先隐藏后回收）、陈旧数据块探测/删除
	- 瓶颈：减少Master在数据存储的参与（仅保存不读取，修改交由chunkserver）、采用大数据块、客户端缓存元数据等
- 争议：中心服务器模式的内在问题
	- 内容：不允许第二个Master，系统容量受制于唯一的Master
	- 解决：影子Master（用于在故障中替补Master）

GFS架构：

![[Pasted image 20241127161803.png]]

GFS写流程：

![[Pasted image 20241127163010.png]]

### 7.3-云存储应用

著名应用：
- iCloud
- Windows Live SkyDrive：25G容量，单文件限额50MB
- Dropbox：与本地文件同步的网络存储，支持多设备同步
- Amazon Cloud Drive
- 百度云

知名应用的通用特点：
- 通用的设备支持
- 数据同步与共享
- 任意格式/大小文件
- 免费/增值付费模式

案例：校园网盘iStudy（见P55-59）

## sec08-批量计算

背景：
- 摩尔定律失效，单机性能受限
- 现实世界问题规模日渐巨大，远超单机处理能力范围
- 如果有大量结构一致的数据要处理，且数据可以分解成相同大小的部分， 那我们就可以设法使这道处理变成并行
- 设计一个新的计算模型，将并行化、容错、数据分布、负载均衡等细节问题放在一个库里，使并行编程时不必关心它们，是高度必要的工作

*批量计算*（batch computing）：
- 一类由用户驱动的固定规模数据的简单计算问题，对速度要求较高，实时性要求一般
- 问题：弹性可扩展（理论上可应对无限数据），涉及存储、计算逻辑、计算执行过程控制等一系列问题

*Google MapReduce*：一种代表性的大规模批量计算架构
- 功能：Map和Reduce可能同时运行
	- Map：把一个函数应用于集合中的所有成员，然后返回一个基于这个处理的结果集
	- Reduce：对结果集进行分类和归纳
- 架构：![[Drawing 2024-11-29 19.35.22.excalidraw|100%]]
- 实现原理：![[Pasted image 20241129210105.png]]
	- 在Google内部，源文件来自GFS，Map的处理结果暂存在本地，而Reduce处理结果和日志都存入GFS
	- 高效：在1000台计算机间进行1TB数据排序仅需68s，4000台进行1PB数据排序则需要约362分钟

案例：*文本单词计数问题*。从一份多行的英文文本中提取每个单词的出现次数。
- 操作：
	- Map：对输入的一行文本，对其中每个单词输出一个k-v对`(word,1)`
	- Reduce：将单词相同的p个`(word,1)`合并为`(word,p)`
- 步骤：
	- 按行分割文本，形成若干行包含若干单词的单行文本
	- 对每行执行map操作，形成对应的k-v对序列
	- MapReduce框架会自动将单词相同的k-v对归拢、排序在一起（shuffle）
	- 对单词相同的k-v对执行Reduce操作，生成最终结果
	- ![[Drawing 2024-11-30 00.04.31.excalidraw]]
MapReduce*算法级调优*：
- 优化k-v对设置
- Map算法
- Combiner算法：
	- 自定义combiner
	- 对key的解析，对value的解析/计算
- Partition算法：
	- 控制分发策略：原则上按照key分发
	- 如果key是多元key，则需要对key解析后分发
- Reduce算法：
	- 对key的解析
	- 对value的解析和计算

MapReduce*参数级调优*：
- 调优属性
- Map属性
- Reduce属性

Hadoop中是如何执行MapReduce的：
- Map：
	- 从磁盘上读数据->执行map函数->combine结果
	- 结果写入本地磁盘：以部分性能为代价提高可靠性
- Reduce：
	- 从各个map task的磁盘上读取相应的数据（shuffle）
		- 在Hadoop中，这一步使用HTTP协议
	- 排序sort
	- 执行reduce函数，并将结果写回HDFS

*针对Shuffle的调优*：
- Map端：
	- Map任务由JVM处理，计算结果先由内存缓存，并进行预排序
	- 内存中缓存达到一定阈值（如80%容量）时，会写入本地磁盘，称作*spill*。如果缓存填满，则会暂停Map任务直到缓存容量恢复
	- 写磁盘之前，线程根据最终要传送到的reducer，对数据进行分区（*partition*），每个partition内按照key排序（K路归并排序）
	- 必要时压缩Map输出以提升效率
	- 向Reducer提供输出文件分区的工作线程由tasktracker控制，与具体的map任务线程独立
- Reduce端：
	- Reduce任务由JVM处理
	- 只要一个Map端有输出，Reduce端便开始复制其输出
	- 优先复制Map输出到内存缓冲区，超出阈值（如2/3容量）或达到输出阈值时写到磁盘中
	- 随着磁盘上的副本的增多，后台线程会根据合并因子（如：10）将它们合并成更大的排好序的文件，压缩的map输出都在内存中解压缩。合并策略是：除第一次合并外，每次合并的文件数恰好等于合并因子。
	- 从map 处获取数据之后，则开始reduce 过程，reduce 开始时，内存中map 输出大小不能超过输入内存阈值，以便为reduce 提供尽可能多的内存，如果reduce 需要内存较少，可以增加此值来减少访问磁盘次数
	- 输出结果写入HDFS
- 调优原则：
	- shuffle过程的内存空间应尽可能多
	- Map和Reduce函数尽量少用内存
	- 运行Map和Reduce任务的JVM内存尽量大
	- Map端尽量估算输出大小，减少溢出写磁盘的次数
	- Reduce端的中间数据尽可能驻留内存
	- 增加Hadoop的文件缓冲区

## sec09-流式计算与图数据计算

### 9.1-流式计算

P5-23

*数据流*：一系列动态数据的集合体（如：计算机网络传输的若干数据包）
- 特点：微观上有独立的组成单元（如：数据包），宏观上呈现为流，实时性强，随时间不断到来，具有一定的次序
- 应用领域：物联网、日志、系统运维，etc.

*批量计算vs流式计算*：
- 核心区别：实时性，是每来一个都要处理还是批量处理
- 主要挑战：处理数据与数据到达速度的匹配、保障数据顺利流动、处理逻辑表示

| \\     | 批量计算            | 流式计算     |
| ------ | --------------- | -------- |
| 计算实时性  | 离线计算，存在一定延时     | 实时计算，延时低 |
| 数据处理时机 | 先缓存，缓存到一定量时批量处理 | 来一个处理一个  |
| 计算请求驱动 | 用户驱动计算请求        | 数据驱动计算请求 |
| 计算结果获取 | 拉式获取计算结果        | 推式获取计算结果 |

流式计算结构：（以wordCount为例）

![[Drawing 2024-11-30 13.54.06.excalidraw|100%]]

*流式计算技巧*：
- 增加分词节点个数（即上图第二列节点）
- 增加统计节点个数（即上图第三列节点）
- 分发策略：随机分发、按特定值分发、广播分发、etc.
	- 设置原则：目的是分散单一节点压力、解决节点压力问题，关键是如何保证全局处理逻辑不变
- 增加*前序节点*，用于在处理前转换数据（过滤、格式化、映射、etc.）
- 增加相同功能节点，使用*随机分发*均衡负载
- 将处理功能分散，使用*特定值分发*
- 如果多个后续节点有相同的数据需求，则可设置一批*转发节点*，将数据广播给这些后续节点
- 某些节点可以作为*同步节点*，接收到来自多个上游的数据后触发下一个步骤

流式处理流程：
- 数据采集：获取数据
- 数据计算：处理数据
- 数据查询：提供结果

典型流式处理框架：
- Storm@Twitter
- S4@Yahoo
- Data Freeway@Meta
- Puma@Meta
- TimeStream@Microsoft
- Hstreaming@Hadoop
- StreamBase@IBM

### 9.2-图数据计算

P25-59

案例：*PageRank计算*

有A、B、C、D共4个网页，网页间链接关系如下图。假设用户初始时随机访问其中一个网页，并随机跳转，求用户访问各网页的概率。

![[Pasted image 20241130153204.png]]

假设用户初始访问4个网页的概率向量为$v_0=(1/4,1/4,1/4,1/4)^T$. 第一次跳转后，用户跳转到各网页的概率向量为：
$$
v_1=Mv_0=\left(\begin{matrix}
0 & 1/2 & 1 & 0 \\
1/3 & 0 & 0 & 1/2 \\
1/3 & 0 & 0 & 1/2 \\
1/3 & 1/2 & 0 & 0
\end{matrix}\right)v_0=(3/8,5/24,5/24,5/24)^T
$$

最终收敛的概率向量表示了用户访问各网页的概率，即为这4个网页的*PageRank*：
$$
v'=(1/3,2/9,2/9,2/9)^T
$$

*图计算的挑战*：
- 存在部分极高度数的节点，难以由单个节点处理，必须由数个节点切分处理
	- 点切分、边切分：![[Pasted image 20241130160619.png|200]]
- 如果图特别大，连接的边特别多，如何计算？
	- 对图进行分割存储，计算过程并行+迭代
	- 并行化思路：迭代各个轮次，每一轮次每个节点分别计算当前轮次的权值

*图上并行计算的思路*：
- Gather（Reduce）：收集当前节点的邻居信息
- Apply：将收集得来的值应用于当前结点
- Scatter：根据当前节点信息，更新周围邻居节点

![[Pasted image 20241130161620.png]]

整体同步并行模式（bulk synchronous parallel，*BSP模式*）：
- 特点：
	- 将计算分成一系列的超步（superstep）的迭代（iteration）
	- 纵向上看是串行模式，逐轮顺序化串行，横向上看是并行模式
	- 每两个superstep 之间设置一个栅栏（barrier），作为整体同步点
	- 确定所有并行的计算都完成后再启动下一轮superstep
- 步骤：
	- 计算：每个处理器进行本地计算
	- 传递：每个处理器计算完毕后，将消息传递到与之关联的其它处理器
	- 整体同步：在每个superstep末尾进行同步
- 瓶颈：整体运算效率受制于每个superstep中最耗时的运算
- 其他模式：
	- 完全异步模式：所有处理器只算自己的
	- SSP（stale synchronous parallel）：放宽了同步要求，要求最快的处理器和最慢的之间相差不超过k个轮次

*图数据计算的技巧*：
- 图的拆分
- 每个子图的处理逻辑
- 同步逻辑

例子：分布式Dijkstra，给定一个有向带权简单图，计算节点1到其余各个节点的最短路径。

![[Pasted image 20241130174935.png]]

过程略，见PPT P48-58

典型框架：
- Titan@Hadoop
- GraphLab@CMU
- GraphX@Spark
- Pregel@Google

## sec10-lab2：计算练习

### 10.1-批量计算练习

如何写MapReduce：
- 将计算最终结果所需的内容设计为`(k, v)`组合
- 根据实际情况决定`key`和`value`的组成
- 决定Map/Combiner/Partition/Reduce策略：
	- Map：`(k,v)`对的生成策略，分配依据为`key`的内容
	- Combiner：Map结果的合并策略，减少单个Map 传递给单个Reduce 的中间结果的重复
	- Partition：分发策略，reduce 计算所需所有内容需要分配到一个节点
	- Reduce：计算得到最终结果的策略

*倒排索引*（inverted index）：
- 以wordCount为例，普通索引是`(文件, 词)`，则倒排索引为`(词, 文件)`

文本相似度计算：
- 分词
- 两份文本：衡量两份文本中词的重复程度
- 多份文本：
	- 对每份文本建立单词到文本的倒排索引，记录每个单词分别在哪些文本出现，以及出现的次数。如：`(rabbit, {text1: 2, text3: 1, text6: 3})`
	- 对每个单词，对所有包含该词的文本运用Map方法，两两生成匹配对`(text1, text3)_k`，其中`k`为两文本中该词出现的最高次数
	- 执行Reduce，将所有相同的匹配对累加，输出匹配数`k`最高的若干对

数据表的自然连接：

![[Pasted image 20241201174602.png]]

大矩阵乘法：

![[Pasted image 20241201174918.png]]

### 10.2-流式计算练习

设计拓扑结构的技巧：
- 数据生成、处理的点设计
- 点和点连接的边：分发策略

### 10.3-图数据计算练习

图数据计算的技巧：
- 图的切分
- 每个子图的处理逻辑
- 同步逻辑

## sec11-分布式计算框架

outline:
- Hadoop
- Hadoop扩展
- Spark
- Storm
- Kafka
- Pregel

### Hadoop及其扩展

P4-65

起源于2002年前后的Apache Nutch：
- NDFS：参考GFS的开源实现
- MapReduce：2005

Hadoop也是一组项目的通称：
- MapReduce：分布式数据处理模型和执行环境
- HDFS：分布式文件系统
- HBase：分布式、按列存储数据库
- ZooKeeper：分布式高可用性协调服务
- Pig：数据流语言和运行环境
- Hive：分布式、按列存储数据仓库

### Hadoop-HDFS

P7-14

![[Drawing 2024-12-22 15.41.15.excalidraw|100%]]

HDFS主要概念：
- 数据块
- namenode：存储文件对应的数据块信息
- datanode：存储数据块的一或若干复本
- 命令行接口
- 基本文件系统操作

HDFS的GFS对应物：
- HDFS namenode=GFS master
- HDFS datanode=GFS chunkserver

*HDFS读文件流程*：
- 客户端调用DFS对象的open方法
- DFS通过RPC联系namenode，得到所有数据块信息，namenode对每个块返回存有该块副本的datanode地址，并且这些datanode根据它们与客户端的距离进行排序
- DFS类返回一个FSDataInputStream对象给客户端并读取数据
- 客户端对该对象调用read方法读数据
- FSDataInputStream链接距离最近的datanode读取数据，读取完毕后通过close方法关闭与datanode的连接，随后寻找下一个块的datanode（这个过程有可能并行，即同一时刻读取数个datanode）

*Hadoop网络拓扑*：
- node之间的距离是node到它们最近公共祖先（LCA）的距离和
	- 距离：0=同一节点\<同一机架不同节点\<同一数据中心不同机架节点\<不同数据中心节点

*Hadoop写文件流程*：
- 客户端调用DFS的create方法
- DFS通过RPC联系namenode，namenode检查文件不存在，且客户端拥有创建该文件的权限
- 如果检查不通过则返回IOException，否则namenode为新文件创建一条记录
- DFS返回一个FSDataOutputStream写数据
- FSDataOutputStream将待写入数据分成数据包，并放入内部队列dataqueue
- DataStreamer处理dataqueue，根据datanode列表要求namenode分配适合的新块存储数据备份
- namenode分配的数据备份datanode（通常3个）形成一个管线，DataStreamer将数据包传输给管线的第一个节点，第一个节点存储后发给第二节点，以此类推

*复本选择*：
- 规律：分布性越高，复本效果越好，但传输代价越高
- 第一个复本：节点本身
- 第二个复本：与第一复本不同机架，机架位置随机
- 第三个复本：与第二复本同机架，但节点随机
- 其它复本：集群内随机选择节点

### Hadoop-MapReduce实现

P15-28

MapReduce术语：
- job：在一整个数据集上执行一次完整Mapper和Reducer的过程
- task：在一个数据片上执行Mapper或Reducer的过程
- task attempt：在某设备上执行某个特定task的尝试
- node：节点，分master和worker (slave)两类
- JobTracker：由master节点运行的实例，用于从客户端接受job请求
- TaskTracker：由worker节点运行的实例，追踪一个task

MapReduce在Hadoop的大致实现：
- 用户程序向master节点要求执行一个job
- master节点将job分解为task序列，向数个worker节点分发
- worker节点执行一或数个task

*MapReduce任务分发*：
- MapReduce程序由一个jar文件和XML组成：
	- jar为JVM可执行代码，XML为程序配置
- MapReduce程序被传送到各个数据所在HDFS的datanode上运行
	- Mapper/Reducer的并发
- job在执行之前就被序列化和分布式

*MapReduce数据分发*：
- 计算数据分布存储在HDFS上
- Mapper被传送到数据节点进行计算
- 中间结果传输
- 中间结果传送到Reducer上进行计算
- Reducer将计算结果分布式保存到HDFS

MapReduce实现结构：
- 任务的jar包可在任意client提交，提交后生成JobClient
- task tracher和job tracker协调分配task，优先向空闲的worker分配

![[Pasted image 20241223004021.png]]

Client的工作流程：（P22）
- 创建一个Job
- 标识Mapper接口（`Map.class`）和Reducer接口（`Reduce.class`）的实现类
	- `job.setMapperClass(cls)`、`job.setReducerClass(cls)`
- 指定输入/输入的格式和路径，以及其他选项
- 提交任务

JobTracker的工作流程：
- 将Job放入等待队列，由Scheduler处理
- 将jar文件和JarConf（序列化为XML）插入共享位置
- 向运行队列提交一个JobInProgress

TaskTracker的工作流程：
- 心跳机制：周期性向JobTracker请求任务，并获取任务jar包和配置
- 在独立Java实例中启动task
- 每个TaskTracker有一定数量、受资源限制的map/reduce task slots，优先执行map task

Task：
- `TaskTracker.Child.main()`
- 独立task实例：通过`TaskRunner`运行task，通过脐带接口与TaskTracker通信，崩溃不会影响后者

TaskRunner：
- 实际运行用户提交的Mapper或Reducer程序
- 提前知晓InputSplits，并对每条从InputSplits取出的记录执行一次Map/Reduce

### Hadoop-Yarn

P30-37

MapReduce实现的缺陷：
- JobTracker负载过高，大概只能支撑4000个节点
- 方案没有考虑到每个task的资源消耗，如果多个资源密集的task分配到同一节点，容易出现故障
- 资源强制划分为map/reduce slots，会导致潜在的资源浪费
- 代码易读性和可维护性差
- 更新方式不友好：任何变化都要强制进行系统级别的更新，而且要求所有用户端同步更新

Yarn架构：（P33）
- 客户端不作改动
- 原框架中核心的Task/JobTracker拆分为：
	- ResourceManager、ApplicationMaster、NodeManager

![[Pasted image 20241223124904.png]]

*Yarn的变化*：
- ResourceManager：调度/启动/监控ApplicationMaster的中心服务
	- 作业与资源调度：接收JobSubmitter提交的作业，根据上下文信息和NodeManager状态信息，分配一个Container作为AppMaster
- ApplicationMaster：负责一个Job生命周期所有工作，包括task重启/监控等
- NodeManager：维护Container状态，向ResManager维持心跳

*Yarn的优点*：
- 降低了JobTracker负载，让监视每个task状态的程序分布式化
- task监控转移给AppMaster，AppMaster由ResManager监控
- AppMaster程序可由用户变更
- 通过内存用量标识task的资源消耗
- 提出了资源隔离思想（Container）

### Hadoop-HBase

P38-54

*引入HBase的原因*：
- 关系数据库系统不适用于大型分布式数据存储：难于安装维护，需求不对口

类似于Bigtable，基于HDFS

*HBase数据模型——”行“*：
- 下表的每行实际代表一个单元格
- 每行有一个可排序的键和任意列项，键可以是大部分常见类型。全表根据行键进行排序
- 表是稀疏表，不同行的列数差距可能极大
- 每行配一个锁，对行的读写是原子操作

| Row Key（行键）   | Timestamp | Column Contents      | Anchors                     | MIME      |
| ------------- | --------- | -------------------- | --------------------------- | --------- |
| "com.cnn.www" | T9        | -                    | `{"cnnsi.com": "CNN"}`      | -         |
| "com.cnn.www" | T8        | -                    | `{"my.look.ca": "CNN.com"}` | -         |
| "com.cnn.www" | T6        | `"<html>...</html>"` | `{}`                        | Text/html |
| "com.cnn.www" | T5        | `"<html>...</html>"` | `{}`                        | Text/html |
| "com.cnn.www" | T3        | `"<html>...</html>"` | `{}`                        | Text/html |

HBase数据模型——”列“：
- 定义为族（family），格式为`"族: 标签"`。族和标签可以为任意格式的串
- 同族数据相邻存储
- 数据通过时间戳区分版本

HBase结构：

![[Drawing 2024-12-23 18.07.30.excalidraw|100%]]

*HBase数据存储实体——区域*：（P45）
- 表按照”水平“方式分为一或若干区域（region），每个区域包含一个随机ID，区域内的行按行键有序
- 区域大小不超过特定阈值，否则表会划分出一个新区域
- 区域以分布式的方式存在于集群内

HBase区域管理：
- 区域服务器（region server）：负责区域分割和数据持久化，为用户访问区域提供服务
	- 职能：处理用户的读写请求，向master server汇报状态，获取自己需要服务的区域
	- 写操作：数据先写入预写日志进行缓存，缓存到达阈值后再批量写入文件系统。写完后在日志中作标记
	- 读操作：先在内存缓存中查找，命中则直接服务；如果存在多个版本，则返回顺序按从最新到最老
	- 合并操作：如果Map File数量超过阈值，或者达到某个周期，则区域服务器进行一次合并；合并期间读写请求会被挂起
	- 分割操作：区域文件超过阈值后，区域文件按行对半分割；被分割区域先离线，由区域服务器生成子表元信息，将子表分配给新的区域服务器；被分割区域通过垃圾回收机制回收
- 主服务器（master server）：管理区域服务器，对特定服务区域指派区域服务器，恢复失效的区域服务器

HBase特殊目录数据：
- 元数据（META）：
	- 保存全部用户区域的属性数据
	- 包括区域中数据起止行信息、区域在线状态等
	- 保存区域服务器地址
	- 元数据表也可包含多个区域，区域属性数据存储在根结点上
- 根表（ROOT）：
	- 只存储一个区域
	- 将元数据中的区域映射到区域服务器
	- 存储元数据服务器的位置，以及映射了哪些元数据区域

*HBase失效恢复*：
- 由于检测没有心跳，主服务器能够探知区域服务器的失效
- 主服务器将失效服务器所提供服务的区域重新分配给其它区域服务器
- 原失效区域服务器的“预写”日志由主服务器进行分割并派送给新的区域服务器

### Hadoop-Pig

P55-61

Pig与HDFS区别：HDFS存储海量数据，Pig则是数据流语言和运行环境，用于检索大规模数据集

Why Pig: Pig eats anything.

Pig两部分：
- Pig Latin：数据流描述语言
- 运行Pig Latin的执行环境

Pig Latin VS SQL：
- Pig Latin是数据流编程语言，是对输入的一步步操作，每一步是对数据进行的一个简单变换
- SQL是一种描述编程语言，是一个约束集合，这些约束定义了输出
- Pig Latin类似SQL，但语法简单，可以显式设置并行数目

Pig Latin操作：（P59）

Pig Latin其他内容：
- 命令：文件系统交互
- 表达式：算术/条件/布尔/函数/字段名，etc.
- 类型：数值/文本/二进制/元组/集合/键值对，etc.
- 函数：计算、过滤、加载、存储函数等

### Hadoop-Hive

P62-65

Hive：
- 将SQL查询转换成一系列在Hadoop集群上的MapReduce作业
- 将数据组织成表
- 通过HiveQL命令执行

Hive VS SQL：

| 特性   | SQL                  | HiveQL                 |
| ---- | -------------------- | ---------------------- |
| 更新   | update/insert/delete | insert/overwrite table |
| 事务   | 支持                   | 不支持                    |
| 索引   | 支持                   | 不支持                    |
| 多表插入 | 不支持                  | 支持                     |
| 选择   | 多表、多视图               | FROM子句只允许一个表/视图        |
| 连接   | FROM连接表，WHERE连接条件    | 内/外/映射/半连接             |
| 子查询  | 支持                   | 不支持                    |
| 视图   | 可更新、可物化、可非物化         | 只读，不支持物化视图             |
| 扩展点  | 用户定义函数、存储过程          | 用户定义函数、MapReduce脚本     |

*Hive分区、桶*：
- 分区（partition）：对表的进一步划分
	- 比如：对日志文件按照日期进行分区，则同一天记录被存放在同一个分区中，以缩小处理范围
- 桶（bucket）：分区的子单元
	- 为数据提供额外的结构以获得更高效的处理
	- 比如：北京市机动车牌尾号除以5 的余数分成5 个桶

Hive实例：（P65）

### Spark

P66-74

Spark：开源集群计算环境
- from UC Berkeley
- 在大规模的特定数据集上的迭代运算或重复查询检索
- 批处理，与Hadoop MapReduce类似
- RDD（分区的分布数据集），实现中间结果的内存存储

![[Pasted image 20241224105440.png]]
![[Pasted image 20241224105449.png]]

Spark特点：
- 基于HDFS
- 采用driver-worker主从结构，driver负责节点调度/任务分配/资源安排/结果汇总/容错，worker负责数据计算和存储
	- 读写在外设进行，计算在内存进行
- 实际操作使用MapReduce+SQL混合语法

*Spark适用场景*：
- 反复迭代运算的场景
- 深度学习、数据挖掘
- 不适用于异步细粒度更新状态的应用：如增量爬虫、索引等

*Spark运行模式*：
- local：本地模式
- standalone：独立集群模式。典型的master-slave模式
- on Yarn：运行在Yarn资源管理器框架上，Spark负责任务调度/计算
- on Mesos：运行在Mesos资源管理器框架上，Spark负责任务调度/计算
- on cloud：云集群模式

Spark组件：



### Storm

P75-81

### Kafka

P82-96

### Pregel

P97-100

## 思考题汇总

sec01：WIP

sec02：OpenStack实验，略

sec03：
- 服务的概念
- 云的技术范畴
- 云服务的基本层次
- 云的特征
- 云的优势
- 什么是DevOps，其作用是什么
- 什么是中台，中台作用有哪些，主要中台有哪些

sec04：
- IaaS的基本功能
- PaaS的基本功能
- SaaS的典型应用

sec05：
- 虚拟化的概念
- 服务器虚拟化的特性
- 服务器虚拟化的关键技术
- 其他虚拟化的相关技术
- 典型虚拟机
- 虚拟化与云计算的关系

sec06：
- 镜像管理相关：
	- 如果把自己的机器制作为一个镜像，应如何管理
	- 时间维度，配置，内容，操作
	- 多台机器镜像如何处理
- AWS模式是什么，有什么优点？
- IaaS模式核心需求有哪些？
- Openstack都包含哪些核心项目，作用是什么？
- 镜像和实例有什么区别和联系？
- Nova有哪些核心模块，工作过程是什么？
- Keystone权限控制过程是什么？
- Quantum原理是什么？
- Cinder存储的机制是什么？
- Swift的核心概念有哪些？
- Swift的组件有哪些，都有什么作用？

sec07：
- 大规模数据存储面临的新问题与挑战
- 索引技术
- GFS体系结构
- 云存储应用的特点

sec08：
- 并行化思想
- 批量计算特点
- MapReduce算法的架构
- MapReduce算法设计思想
- MapReduce算法调优
- MapReduce运行过程中的各种参数及其作用
- MapReduce参数调优

sec09：
- 流式计算与批量计算的区别
- 流式计算的关键要素
- 数据分发机制
- 运用流式计算方法解决实际问题
- 图的切分方式
- BSP计算模式
- 图数据计算的并行思想
- 运用图数据计算方法解决实际问题

sec10：（课后作业）
- 自定义计算问题
- 寻找计算问题的数据
- 选择与计算问题匹配的计算方案
- 搭建计算环境及框架
- 部署计算环境、调试、计算
- 展示结果

## 往年考题

5道选择5道判断，有1-2个隐藏知识点在课件中，需要认真看才能发现

大题有：
- 虚拟化和云的关系
- kafka的消息发布和消费机制
- 流式计算和批量计算完成wordCount任务并说明优劣和适用条件
- 图计算（含有高纬度结点）
- PBFT三阶段及含义
- paxos和PBFT协议的区别