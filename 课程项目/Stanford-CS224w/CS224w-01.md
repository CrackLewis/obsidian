
## lec01-图机器学习简介

[link](https://blog.csdn.net/PolarisRisingWar/article/details/117287432)

常用工具：NetworkX, PyTorch Geometric, DeepSNAP, GraphGym, SNAP.PY

课程任务：神经网络模型 --> 图

![](https://i-blog.csdnimg.cn/blog_migrate/09246c0a0303fb6eda1c887da66b8265.png)

传统机器学习：特征工程（原始数据到特征） --> 机器学习（特征到模型）

图机器学习：
- *表示学习*：每个原始结点 --> 表示为embedding，相似结点的embedding也相似
- 基于GNN的机器学习

内容：
- 传统方法：
- 图节点嵌入方法
- 图神经网络
- 知识图和推理
- 深层图生成模型
- 图机器学习应用

四类图机器学习任务：
- 社区/子图级别任务：社区检测（comm'ty search/detection）
	- 路程长度/耗时预测
- 边级别任务：预测两个节点是否存在隐藏联系
	- 推荐系统
- 节点级别任务：预测节点属性
	- 蛋白质折叠问题：根据氨基酸折叠方式，计算蛋白质的3D结构
- 图级别任务：分类预测、图生成、图进化（graph evolution）等
	- 高分子生成/优化问题、粒子活动预测

用图表示数据的选择：
- 组成成分：点、边、图
- 合适的表示方式：用论文间引用关系，还是共同关键字表示一张图？
- 什么是边，什么是顶点？

扩展阅读：
- [paper](https://arxiv.org/pdf/1806.01973.pdf)：一个基于GNN和random walk的大型推荐系统
- [paper](https://arxiv.org/pdf/1802.00543.pdf)：Decagon，一个由蛋白质-药物交互关系组成的多模态图
- [paper](https://deepmind.com/blog/article/traffic-prediction-with-advanced-graph-neural-networks)：DeepMind用图机器学习预测交通状况

## lec02-传统图机器学习方法

[link](https://blog.csdn.net/PolarisRisingWar/article/details/117336622)

传统图机器学习pipeline：
- 设计图特征，并从原始数据提取
	- 图本身的结构特征：structural feat's
	- 边和点的数据特征：attr's and prop's
- 使用设计的算法训练模型，并将其应用
	- SVM/随机森林/神经网络, etc.

### 基于特征的传统方法-节点特征

特征抽取目标：找到能够描述节点在网络中结构与位置的特征
- 度数
- 重要性（centrality）
- 簇系数（clustering coeffn't）
- graphlets

*度数*：简单，但把所有邻居视为同样重要

*特征向量重要性*（eigenvector centrality）： ^86a1c9
- 递归定义：等于所有邻居重要性总和的$1/\lambda$倍：$$
c_v=\dfrac{1}{\lambda}\sum_{u\in N(v)} c_u
$$
- 矩阵定义：线性方程组$(A-\lambda I)x=0$的解。其中$A$是邻接矩阵，$\lambda$是正常数，$x$是重要性向量。
	- 根据Perron-Frobenius定理，非负且非零矩阵$A$的最大特征值$\lambda_{\max}$必为正，其对应的特征向量即为$x$，满足$(A-\lambda_\max I)x=0$

*中心重要性*（betweenness centrality）： ^47559b
- 如果一个节点处于很多节点对间的最短路径上，则认为它是相对重要的
	- 如：苏伊士运河、巴拿马运河、马六甲海峡，等
- 公式：$$
c_v=\sum_{s\neq v\neq t}\dfrac{\text{count}(\text{shortest paths btwn' s and t containing v})}{\text{count}(\text{shortest paths btwn' s and t})}
$$

*距离重要性*（closeness centrality）： ^1dc785
- 该标准认为，一个节点距其他所有节点的距离总和，与其重要性成反比。即离其它节点总体越近，结点越重要
- 公式：$$
c_v=\dfrac{1}{\sum_{u\neq v} \text{distance}(u,v)}
$$

*聚类系数*（clustering coefficient）： ^3e635f
- 该标准认为，一个节点的所有邻居间的连接越紧密，该节点越重要
- 公式：$$
e_v=\dfrac{\#(\text{edges among nodes in}\ N_v)}{\text{C}_{k_v}^2}
$$
- 特别地：若$e_v=1$，$v$和$N_v$的所有节点形成一个clique

*graphlets*：一种有根异构连通子图
- 对于给定节点数$k$，会有$n_k$个异构连通子图
- 度向量（graphlet degree vector, GDV）： ^cb5c3e
	- 计算以某个节点$v$为根的每类graphlet的个数
	- 通过graphlet数目衡量节点重要性。与通过度数、邻接三角形（clustering coeffn't）衡量重要性相区分

![](https://i-blog.csdnimg.cn/blog_migrate/a6ca5492cdd38d53f420a3af48c002c1.png)

![[Drawing 2025-04-25 14.28.51.excalidraw|100%]]

### 节点特征-讨论

基于重要性（importance）的特征：
- 节点度数
- 节点的重要度（centrality）：
	- [[#^86a1c9|特征向量重要性]]（eigenvector cent'ty）
	- [[#^47559b|中心重要性]]（btwn'ness cent'ty）
	- [[#^1dc785|距离重要性]]（closeness cent'ty）

基于结构的特征：
- 节点度数
- [[#^3e635f|聚类系数]]（clustering coeffn't）
- [[#^cb5c3e|graphlet度向量]]（GDV）

重要性特征 vs 结构特征：
- 前者可预测重要节点，后者可预测节点的具体角色

### 基于特征的传统方法-边特征

这里所称边特征可以是任意两个节点间的特征，并不一定是存在的边

- 任务：基于已知的边信息，预测最可能存在的未知边
- 任务的两种类型：
	- 随机缺失边（links missing at random）
	- 随时间演化边（links over time）：给定图$G$在时段$[t_0,t_0']$的状态，预测在$[t_1,t_1']$时段会新出现的边

基于相似度的预测：对每个节点对$(u,v)$预测相似度$c(u,v)$，取最高的$n$对作为新边

*最短路径*：两个节点$u,v$间的最短路径
- 最短路径信息没有考虑两个点邻居的重合度

*局部邻居重数*（local neighborhood overlap）：
- 共同邻居数：$\left|N(v_1)\cap N(v_2)\right|$
- Jaccard's coeffn't：$$
\text{Jaccard}(v_1,v_2)=\dfrac{|N(v_1)\cap N(v_2)|}{|N(v_1)\cup N(v_2)|}
$$
- Adamic-Adar指数：$$
\text{Adamic-Adar}(v_1,v_2)=\sum_{u\in N(v_1)\cap N(v_2)} \dfrac{1}{\log \deg(u)}
$$
- 共同邻居数的问题在于：度数高的点对就会有更高的结果，某个子图本身可能就很稠密
- Jaccard系数对共同邻居数进行了归一化，使其不再受局部子图稠密度影响
- Adamic-Adar在实践中表现更好：共同好友越出名，该好友的意义越小，反之意义越大

*全局邻域重复度*（global neighborhood overlap）：
- 局部邻居的计算方法是短视的，它忽视两个非邻居节点的关系
- Katz指数：点对之间所有长度路径的条数，对不同长度路径数进行加权求和
	- 设$P_{uv}^{(k)}$表示从$u$到$v$需要$k$跳的路径条数，$A$为邻接矩阵，则$P^{(k)}=A^k$
	- 元素计算：设$0<\beta<1$为衰减因子，则：$$
S_{v_1v_2}=\sum_{l=1}^\infty \beta^l A_{v_1v_2}^l
$$
	- 矩阵计算：利用了几何级数：$$
S=\sum_{i=1}^\infty \beta^i A^i = (I-\beta A)^{-1}-I
$$

### 边特征-讨论

- 最短路：未考虑邻居信息
- 局部邻域重复度：能够捕捉公共邻居信息，但无法捕捉非邻居节点对的临近关系
- 全局邻域重复度：运用全局邻接矩阵计算临近关系

### 基于特征的传统方法-图特征

图级别特征构建目标：找到能够描述全图结构的特征

*核方法*（kernel methods）：通过一个核函数$K(G,G')$衡量图$G,G'$的相似度，存在特征表示$\phi(\cdot)\in R^{m}$使得$K(G,G')=\phi(G)^T \phi(G')$。定义好核函数后便可以运用SVM之类的传统ML模型

半正定矩阵特征值非负的证明：[blog](https://blog.csdn.net/PolarisRisingWar/article/details/115598815)

基于图提出的不同的核：
- graphlet kernel
- Weisfeiler-Lehman kernel
- 其他：Random-walk kernel, Shortest-path graph kernel, etc.

核方法的要点：对图生成一个特征向量$\phi(G)$
- 以word embedding为类比，这里的每个图节点视为一个"word"

"Bag-of-\*"表示：
- bag-of-nodes：两个图节点标签相同，则特征向量一致
- bag-of-node-degrees：对图中各节点的度数计数，转化为embedding
- bag-of-graphlets：graphlet kernel
- bag-of-colors：Weisfeiler-Lehman kernel

*graphlet计数向量*（GCV）：
- graphlet特征：计算图中不同graphlet的数目
	- 与graphlet节点特征不同的是：这里的graphlet不必连通，只是统计同一结点数、异构的graphlet个数
- 形式定义：对于给定的图$G$和graphlet列表$\mathcal G_k=(g_1,g_2,\ldots,g_{n_k})$（表示节点数为$k$的所有异构图），定义GCV $f_G\in R^{n_k}$为：$$
(f_G)_i=\#(g_i\subseteq G), \forall i=1,2,\ldots,n_k.
$$
	- 其中：graphlet在图中的每一次出现都要求，图中对应的$k$个节点间的边与graphlet的边完全匹配，不能多也不能少
	- ![](https://i-blog.csdnimg.cn/blog_migrate/f259c1f93ede4a3de91fff6d3c3b51c3.png)

*graphlet kernel*：对两个图的GCV向量计算相似度
$$
K(G,G')=f_G^T f_{G'}
$$
- 问题：如果$G,G'$规模不同，$f_G,f_{G'}$的值可能存在极大偏差
- 解决方案：对GCV进行正则化
$$
h_G=\dfrac{f_G}{\text{sum}(f_G)}\Rightarrow K_2(G,G')=h_G^T h_{G'}
$$
- 限制：计算代价过高，$\mathcal G_k$随着$k$指数级增长。暴力计算kernel是一个NP-hard问题
	- 但如果节点度数上界为$d>0$，则对规模为$k$的graphlet计数成本可限制为$O(nd^{k-1})$级别

*Weisfeiler-Lehman kernel*：比graphlet计算代价更小，效率更高
- 思想：利用图自带的邻接结构，不断迭代完善，建立每个节点的信息
- 算法：
	- 输入：图$G(V,E)$。
	- 初始步骤：为每个节点$v$指定一个初始颜色$c^{(0)}(v)$
	- 迭代步骤：第$k+1$轮$v$的颜色为$$
c^{(k+1)}(v)=\text{HASH}(\{c^{(k)}(v), \{c^{(k)}(u)\}_{u\in N(v)}\})$$
		- 其中：$\text{HASH}(\cdot)$为根据节点及其邻居颜色，计算新颜色的哈希函数
		- 第$k$轮的$c^{(k)}(v)$编码了与$v$距离$k$跳以内节点的信息
	- $K$轮迭代后，取整个迭代过程中各颜色的出现次数作为Weisfeiler-Lehman graph feature，即：$\phi(G)_i=\#\left(i \in c^{(*)}(*)\right)$

W-L kernel举例见原博客

### 图特征-讨论

- graphlet kernel：计算成本高
- Weisfeiler-Lehman kernel：
	- 为图中每个节点设置一个初始颜色，通过$K$轮迭代算法更新节点颜色，融合与节点距离$K$跳以内的邻居信息
	- 计算成本较低，复杂度为$O(K|E(G)|)$级别
	- 是一个在GNN中很有用的kernel trick

### 小节总结

传统图机器学习方法：指定特征的构造方法+选择ML模型

特征构造方法：
- 点特征：度数、重要度（中心/距离/特征向量）、聚类系数、graphlets
- 边特征：
	- 最短路
	- 局部邻居重数：Jaccard系数、Adamic-Adar指数
	- 全局邻居重数：Katz指数
- 图特征：graphlet kernel、Weisfeiler-Lehman kernel

## lec03-节点嵌入

[link](https://blog.csdn.net/PolarisRisingWar/article/details/117420662)

why embeddings:
- embedding可表示节点特征，并通过余弦相似度指示两个相似节点
- embedding可用于下游任务

### encoder-decoder框架

*编码器*（encoder）：将节点编码为低维向量（64-1000维）
- 相似度函数：衡量两个embedding的相似程度
$$
\text{similarity}(u,v)\approx z_v^T z_u
$$

*浅编码*（shallow encoding）：不进行编码，只在嵌入矩阵中查找：
$$
\text{ENC}(v)=z_v=Z\cdot v
$$
其中：$Z\in \mathbb{R}^{d\times |\mathcal V|}$为embedding矩阵，$v\in \mathbb{I}^{|\mathcal V|}$为指示向量（仅某个维度为1，其余维度为0），表示选择哪个向量。

简单，参数太多，难以映射大型图

*框架总结*：
- 浅编码：查表
- 解码器（encoder）：将embedding转化为节点相似度
- 目标：最大化相似点对的embedding相似度

什么是相似节点：
- 由边相连，共享邻居
- 有相似的structural roles

### 随机游走方法

*随机游走方法*（random walk approach）是一种embedding生成方法

符号表示：
- $z_u$：节点$u$的embedding
- $N_R(u)$：从$u$节点开始，通过随机游走方法$R$得到的游走节点集合
- $P(v\mid z_u)$：基于当前$z_u$预测的，从$u$开始随机游走到$v$的概率
- $P(S\mid z_u)$：基于当前$z_u$预测的，从$u$开始随机游走到$S$中节点的概率
- $\sigma(z)_i=\dfrac{e^{z_i}}{\sum e^{z_j}}$：softmax函数
- $S(x)=\dfrac{1}{1+e^{-x}}$：sigmoid函数

![](https://i-blog.csdnimg.cn/blog_migrate/ecf1cbefa68b676e82eb2d947a637ea0.png)

#### 方法思想

核心思想：
- 选取起始点$u$，每步随机转移到一个邻居
- 通过特定的游走策略，预测从$u$开始游走到$v$的概率
- 调整$z_u,z_v$使得$z_v^T z_u\sim P_R(v\mid u)$，即embedding夹角接近概率

随机游走方法的*目标*：对于节点$u$，使得$P(N_R(u)\mid z_u)$最大化，即：
$$
\forall f: u\rightarrow \mathbb{R}^d:f(u)=z_u\Rightarrow \max_f \sum_{u\in V} \log P(N_R(u)\mid z_u)
$$
等价于最小化损失函数：
$$
\mathcal L=\sum_{u\in V} \sum_{v\in N_R(u)} -\log (P(v\mid z_u))
$$
使用softmax函数参数化概率，并运用下采样方法：
$$
\begin{split}
\mathcal L&=\sum_{u\in V} \sum_{v\in N_R(u)} -\log (P(v\mid z_u))\\
&=\sum_{u\in V} \sum_{v\in N_R(u)} -\log \left(\frac{\exp(z_u^T z_v)}{\sum_{n\in V} \exp(z_u^T z_n)}\right) \\
&=\sum_{u\in V} \sum_{v\in N_R(u)} -\left[\log(S(z_u^T z_v)) - \sum_{i=1}^k \log(S(z_u^T z_{n_i}))\right], n_i\sim P_V \\
&=\sum_{u\in V} |N_R(u)| \sum_{i=1}^k\log S(z_u^T z_{n_i})-\sum_{u\in V} \sum_{v\in N_R(u)} \log S(z_u^Tz_v), n_i\sim P_V
\end{split}
$$
按第二行的写法，需要基于全图节点的embedding做归一化，但第三步近似化为任取$k$个节点归一化，第四行进行整理
- 近似化方法在这篇论文说明：[thesis](https://arxiv.org/pdf/1402.3722)
- $k$越高，方差越低，负样本的bias越高，一般取5-20

优化方法：SGD

#### 随机游走策略

朴素策略：固定长度+完全随机游走
- DeepWalk
- 相似度概念受限

*node2vec*：
- 原因：弹性定义的网络邻居$N_R(u)$使embedding更丰富，因此使用有偏的二阶随机游走策略（biased 2nd-order random walk）产生$N_R(u)$
- 有偏游走：通过弹性有偏游走策略平衡全局（DFS）/局部（BFS）的节点网络结构信息

![nerd](https://i-blog.csdnimg.cn/blog_migrate/300ab48081b790ed9ad732e144111fd2.png)

有偏定长随机游走的*参数*：
- $p$：返回参数，表示从当前节点返回前驱节点的概率
- $q$：内外比例参数。向内走表示与前驱节点等距离游走，向外走表示远离前驱节点
- BFS-like策略会给$p$较低的值，DFS-like策略会给$q$较低的值

假设上一步是$(s_1,w)$：

![](https://i-blog.csdnimg.cn/blog_migrate/9506a6ada3fa2894ee9adf348b0d0782.png)

node2vec算法流程：
- 步骤：
	- 计算随机游走概率
	- 对每个节点$u$，模拟$r$轮随机游走，每轮走$l$步
	- 通过随机梯度下降（SGD）优化node2vec目标
- 三个步骤可并行化，且都是线性复杂度

其他随机游走方法：
- 有偏随机游走：基于节点属性或学习权重
- 另类优化策略：基于1-hop/2-hop游走概率优化
- 网络预处理技巧

#### 随机游走-总结

思想：
- 从每个节点开始，进行固定长度的随机游走
- 对每个节点$u$，统计从其开始游走过的节点$N_R(u)$
- 通过优化损失函数$\mathcal L$的方式，对embedding最优化

### 节点嵌入+随机游走总结

节点嵌入：使embedding的向量距离能够反应原网络中的节点相似度

衡量节点相似度的指标：
- Naive: 节点间有边
- Neighborhood overlap（邻域重复度）
- random walk approaches

需要根据具体情况来选择算法。node2vec在节点分类任务上表现更好，不同的方法在不同数据的链接预测任务上表现不同。random walk approaches整体上更有效。

### 图嵌入

将全图或其中某个子图信息转换为embedding

*聚合节点嵌入*：对所有有关节点embedding求和（或取平均）
$$
z_G = \sum_{v\in G} z_v
$$

*假节点嵌入*：向图中加入一个假节点，将其与子图包含的节点连接。对该假节点计算节点embedding，作为对应子图的embedding

![](https://i-blog.csdnimg.cn/blog_migrate/019d676f184fa52e612769aba0c3a3fb.png)

*匿名游走嵌入*：以节点第一次出现的序号（是第几个出现的节点）作为索引

![](https://i-blog.csdnimg.cn/blog_migrate/802d3962e26c99b688f229c508b70636.png)

匿名游走只会表示$n$步中，哪些是同一节点，不会公布节点身份
- 因此有些不同的随机游走可以对应同一个匿名游走
- 不同匿名游走数量随着游走步数$n$指数增长（$n=3$时，有`[111, 112, 121, 122, 123]`五种，$n=12$时有约$4\times 10^6$种。

匿名游走采样数：假设长度为$l$的匿名游走有$\eta$种，我们希望分布偏差$E$满足$P(|E|\ge \varepsilon)\le \delta$。则采样数$m$应至少取：
$$
m=\left\lceil\dfrac{2}{\varepsilon^2} \left(\log (2^\eta - 2)-\log(\delta)\right)\right\rceil
$$

匿名游走*在图嵌入的应用*：模拟$n$次长度为$l$的匿名随机游走，将图表示为walks的概率分布向量（每类walk有多少次）
- 如：$l=3$，模拟10次walk，结果向量表示为`[3, 2, 1, 2, 2]`，作为图嵌入

*图嵌入总结*：
- 取所有节点嵌入的平均或总和
- 构造伪节点，将其连通到想要嵌入的子图，对该节点计算embedding
- 匿名游走嵌入：
	- 采样若干次匿名游走，用游走分布作为图嵌入
	- 对匿名游走编码，拼接为图embedding

### 小节总结

如何使用节点嵌入：
- 聚类/社区检测
- 节点分类
- 链接预测：基于$(z_i,z_j)$预测边$(i,j)$是否存在
	- 通过concat、Hadamard积、avg、sum、distance
- 图分类

encoder-decoder框架：
- encoder：查表返回embedding
- decoder：基于embedding预测节点相似度

衡量节点相似度：
- DeepWalk
- node2vec
- 扩展到图嵌入：聚合节点嵌入、匿名游走嵌入等

这章很乱，看了好几天，估计还得再看

## lec04-PageRank

[link](https://blog.csdn.net/PolarisRisingWar/article/details/117455491)

- 图的矩阵表示
- PageRank算法：flow模型、矩阵构造、与随机游走的联系、特征向量构造
- PageRank：如何解决
- 带restart的随机游走、个性化的PageRank
- 矩阵分解与节点嵌入

*图的矩阵表示*：最常规的方式是邻接矩阵
- 节点重要性计算：随机游走
- 节点嵌入计算：矩阵分解

PageRank是Google曾用于计算网页重要性的算法
- 网页视作节点，链接视作边，网页重要性等同于节点重要性

3种与PageRank相关的节点重要性：
- PageRank
- Pesonalized PageRank (PPR)
- Random Walk with Restarts (RWR)

### PageRank

*PageRank*：采用flow模型
- 节点入边和出边的权重和相等
- 入边权重的总和将均匀分配给每一条出边

设$d_v$表示节点$v$出度，$r_v$表示节点$v$重要性得分，则：
$$
r_v=\sum_{u\rightarrow v} \dfrac{r_u}{d_u}
$$
所有节点的上述等式形成一个线性方程组$r=Mr$，其中：
$$
M_{vu}=\left\{\begin{matrix}
0, & u\not\rightarrow v, \\
\dfrac{1}{d_u}, & u\rightarrow v.
\end{matrix}\right.
$$
理论上，解线性方程组，或者随即指定$r_0$然后迭代至$|r_{n+1}-r_n|\le\varepsilon$即可。

但这样做会有*两个问题*：
- dead end：如果部分节点没有出边，会导致重要性泄漏
	- 解决：将此类节点连向剩余所有节点，使它们返回网络内
- spider trap：所有出边都指向一个节点组内，使得用户在这个节点组内被困，无法离开
	- 解决：每一步都以一定概率随机跳到一个其他网页

*整体解决方案*：设$N=|V|$。每一步都以$\beta$概率（一般取0.8-0.9）随机选择一条出边，以$1-\beta$随机转移一个其余节点。此时重要性等式变更为：
$$
r_v=\sum_{u\rightarrow v} \beta \dfrac{r_u}{d_u}+(1-\beta)\sum_{u\in V}\dfrac{r_u}{N}
$$
其中$u\rightarrow v$涉及两种情况：
- $u$到$v$连有出边，此时$d_u=\deg_o(u)$。
- $u$是图中一个无出边的节点，此时$d_u=N$。

对应矩阵等式：
$$
r=\left(\beta M + (1-\beta) \left[\dfrac{1}{N}\right]_{N\times N}\right) r
$$
其中：$[1/N]_{N\times N}$为全为$1/N$的$N$阶方阵

这也是最终的PageRank方程，其中$M$为初始邻接矩阵。

### Personalized PageRank

PPR=Personalized PageRank / Topic-Related PageRank

问题背景：推荐问题
- 用户-物料二分图，用户和已知感兴趣的物料有连边
- 根据图信息和节点连接信息，向用户推荐可能感兴趣的物料

![](https://i-blog.csdnimg.cn/blog_migrate/d48f6842932b4566b2e272c68fbd5b3f.png)

一些反直觉的问题：
- 如果按照shared neighbors判断相似度，若用户`A,B`都有大量喜欢的物料，则可能造成相似度误判
- 如果按照distance判断相似度，则无法利用共同邻居这一重要信息

*PPR、RWR与PageRank区别*：
- PageRank：用重要性给节点评级，随机跳到任何节点
- PPR：衡量节点与teleport nodes中节点相似性，用于衡量某个物品与其他物品相似性
- RWR：只能跳转到起始节点，算是PPR特例

![](https://i-blog.csdnimg.cn/blog_migrate/46af608656cfd54f0cb6abda8223914e.png)

*Random Walks with Restarts*：
- 每个节点都有重要性，在其边上均匀分布，传播到邻居节点
- 给定查询节点集$S$，模拟随机游走：
	- 随机走到一个邻居$v$，记录其访问次数$c_v$
	- 以$\alpha$概率从$S$中某点重启
	- 结束随机游走后，$c_v$最高的节点$v_0$与$S$中节点最相似

三种PageRank变体总结：
- PageRank：传送到任意节点
- PPR：传送到指定的节点集合
- RWR：传送到特定节点

![](https://i-blog.csdnimg.cn/blog_migrate/e23f6da6798664077cccfa1d557dde9d.png)

### 矩阵分解和节点嵌入

设$Z$为嵌入矩阵：
$$
Z=\{z_u\}\subseteq \mathbb{R}^{d\times n}, z_u\in \mathbb{R}^d
$$
最简单的节点相似度：若有边连接的节点对是相似节点，则$z_v^T z_u=A_{uv}$，进而：
$$
Z^T Z=A
$$
但现实的图一般$d\ll n$，所以严格分解不可取，所以考虑最小化损失函数：
$$
\min_Z \mathcal L = || A-Z^TZ ||_2
$$
其中采用L2范数，即各元素差的平方和

换句话讲：*由边连接度定义的，到节点相似度的内积decoder与邻接矩阵分解等价*

DeepWalk和node2vec有基于random walk定义的更复杂的节点相似性，但还是可以视作矩阵分解问题，不过矩阵变得更复杂了
- 没看懂，博客也没讲明白，得看CS224w原课程

通过矩阵分解和随机游走进行节点嵌入的限制：
- 无法获取不在训练集中的节点嵌入，每次新增节点都要对全部数据集重新计算嵌入
- 无法捕获结构相似性：即使是两个结构相似的节点，它们的embedding也会相差极大
- 无法使用节点、边和图上的特征信息

### 小节总结

PageRank：衡量图中节点的重要性
- 能够通过邻接矩阵幂迭代计算

PPR：衡量节点相对于特定节点集内节点的重要性
- 可通过随机游走高效计算

可通过矩阵分解的方式基于随机游走策略计算节点embedding

图表示为矩阵是所有PageRank-like算法的基础理论

## colab01-节点嵌入

- networkx
- torch.nn.Embedding

WIP