
## [大模型（LLMs）基础面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_a55uo10835nv.html)

1. 目前 主流的开源模型体系 有哪些？
2. prefix Decoder 和 causal Decoder 和 Encoder-Decoder 区别是什么？
3. 大模型LLM的 训练目标 是什么？
4. 涌现能力是啥原因？
5. 为何现在的大模型大部分是Decoder only结构？
6. 简单 介绍一下 大模型【LLMs】？
7. 大模型【LLMs】后面跟的 175B、60B、540B等 指什么？
8. 大模型【LLMs】具有什么优点？
9. 大模型【LLMs】具有什么缺点？

A:

- **目前主流的开源模型体系有哪些？**
    - **Transformers**（如BERT、GPT系列、T5等）：这些是基于自注意力机制（Self-Attention）的模型，广泛用于NLP任务。
    - **BERT系列**（如RoBERTa, DistilBERT等）：用于文本理解的双向编码器模型。
    - **GPT系列**：基于解码器结构，主要用于生成任务，最近的版本包括GPT-3、GPT-4等。
    - **T5**：Encoder-Decoder架构，擅长多任务学习。
    - **Stable Diffusion**：用于图像生成的扩散模型。
    - **CLIP**：连接图像和文本的多模态模型。
    - **LLaMA系列**：Meta的开源大语言模型，注重在多个不同任务上的表现。
    - **OpenLLaMA**：由社区开发的基于Meta LLaMA架构的开源项目。
- **Prefix Decoder 和 Causal Decoder 和 Encoder-Decoder 区别是什么？**
    - **Prefix Decoder**：在生成时通过输入一个“前缀”来引导解码过程。这种结构通常在少量输入的情况下，借助已知的信息来生成输出。常用于条件生成任务。
    - **Causal Decoder**：这种解码器采用自回归的方式（前一步的输出作为下一步的输入），通常用于生成任务，如GPT模型。其特点是只能考虑到前文信息，具有因果性。
    - **Encoder-Decoder**：这种结构包含两个部分——编码器将输入信息转换为表示向量，解码器则根据这些向量生成输出。这是机器翻译、文本摘要等任务中的经典结构，如T5和BART。
- **大模型LLM的训练目标是什么？**
    - 大模型的训练目标通常是最大化语言建模的对数似然函数，即通过训练使模型能够预测下一个词（或Token）。目标是让模型学习到足够的语义和语法信息，以便在多种自然语言处理任务上具备良好的表现。具体目标可以包括：
        - **自回归语言建模**：生成下一个词。
        - **掩蔽语言建模（Masked Language Modeling, MLM）**：预测被掩蔽的词（如BERT所用）。
        - **多任务学习**：如T5和BART采用了统一框架处理多种任务。
- **涌现能力是啥原因？**
    - **涌现能力**（Emergent Abilities）指的是大规模模型在训练过程中出现的、未显式设计的复杂行为或能力。例如，大模型可能在少量的训练数据或少数几次示例上展现出超出其设计目标的表现。其原因包括：
        - 模型规模的增加导致其对数据的拟合能力和泛化能力增强。
        - 大量的训练数据和训练参数使得模型能学习到更复杂的模式。
        - 自注意力机制等架构使得模型能够更好地理解长距离依赖关系。
- **为何现在的大模型大部分是Decoder only结构？**
    - **Decoder-only模型**（如GPT系列）通常用于生成任务，这类任务的特点是自回归生成文本（每次生成一个词并将其作为输入）。这种结构的优点包括：
        - 生成能力强，适用于长文本的连贯生成。
        - 训练过程中，只需要考虑生成每个词的条件概率，不需要像Encoder-Decoder那样涉及到复杂的编码过程。
        - 更高效：在解码阶段，生成每个词的过程中都基于已经生成的部分，避免了编码-解码间的复杂数据传递。
- **简单介绍一下大模型【LLMs】？**
    - **大模型（LLMs）**是指拥有大量参数的语言模型，通常具有数十亿到数百亿的参数。这些模型通过海量的文本数据进行训练，具备较强的理解、生成能力，可以用于各类NLP任务，如文本生成、翻译、问答、摘要等。
    - 典型的LLMs包括GPT-3、GPT-4、T5、BERT等，它们的特点是使用了Transformer架构，采用自注意力机制，能够捕捉长距离依赖和复杂的语言模式。
- **大模型【LLMs】后面跟的175B、60B、540B等指什么？**
    - **175B、60B、540B等**是大模型中参数的数量，指的是模型中可训练参数的总数。例如，GPT-3有1750亿个参数，GPT-4的参数数量可能更高。参数数量是衡量模型规模的重要指标之一，通常随着参数数量的增加，模型的性能在许多任务上得到提升。
- **大模型【LLMs】具有什么优点？**
    - **强大的性能**：由于参数数量巨大，LLMs在多个NLP任务上通常表现出色，如文本生成、问答、翻译等。
    - **迁移学习能力**：经过预训练的大模型能够通过微调适应多种任务，减少了传统机器学习中的手动特征工程。
    - **处理复杂语言模式**：能够捕捉语法、语义和上下文关系，处理复杂的语言任务。
- **大模型【LLMs】具有什么缺点？**
    - **计算资源要求高**：训练和推理都需要大量的计算资源和内存，对硬件要求很高，成本昂贵。
    - **缺乏透明度**：大模型的内部工作往往不透明，很难解释其做出的决策，这对高风险应用（如医疗、法律等）是一个挑战。
    - **数据偏见问题**：模型可能会学习到训练数据中的偏见，导致不公正或不合适的结果。
    - **能耗高**：大规模模型的训练和使用消耗大量能源，对环境有影响。
    - **难以调优**：调优这样庞大的模型需要专业知识和时间，且在小规模数据集上可能表现欠佳。

## [大模型（LLMs）进阶面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_v6gltxd4qbxd.html)

1. LLMs 复读机问题
	1. 什么是 LLMs 复读机问题？
	2. 为什么会出现 LLMs 复读机问题？
	3. 如何缓解 LLMs 复读机问题？
2. llama 系列问题
	1. llama 输入句子长度理论上可以无限长吗？
3. 什么情况用Bert模型，什么情况用LLaMA、ChatGLM类大模型，咋选？
4. 各个专业领域是否需要各自的大模型来服务？
5. 如何让大模型处理更长的文本？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_v6gltxd4qbxd.html)

A:

1. *什么是 LLMs 复读机问题？*

LLMs（大型语言模型）复读机问题是指，当使用语言模型（如 GPT）进行对话或生成文本时，模型可能会出现过度重复或不断复述相同内容的情况。这个问题表现为模型反复输出类似或完全相同的句子、段落或想法，缺乏足够的多样性和创新性。这种问题通常影响对话的流畅性和互动性，可能使用户体验下降。

2. *为什么会出现 LLMs 复读机问题？*

复读机问题的出现有几个原因：
1. **训练数据问题**：模型的训练数据中可能存在大量重复的信息或者一些特定模式，模型可能在生成时倾向于回到这些熟悉的结构或表述上。
2. **生成策略问题**：LLMs在生成文本时，采用的采样方法（如贪心解码、温度调节等）可能使模型偏向生成相似的输出。例如，低温度的采样策略可能导致模型输出确定性更强的、重复性较高的文本。
3. **缺乏上下文理解**：在某些情况下，LLMs可能没有深入理解对话的上下文，导致它们反复使用相同的短语或句式来回应问题。
4. **有限的模型容量**：尽管LLMs具有强大的语言生成能力，但它们仍然是基于统计学的模型，可能在处理长时间的对话或复杂任务时失去多样性，偏向生成已知的简单响应。

3. *如何缓解 LLMs 复读机问题？*

可以通过以下几种方法缓解复读机问题：
1. **调整生成参数**：
    - **增加温度**：提高温度值（例如，从 0.7 提高到 1.0 或更高）可以让模型的输出更加多样化，避免过于确定的选择，减少重复性。
    - **使用Top-k或Top-p采样**：这些策略通过限制模型只能从概率最高的前k个词或满足一定累计概率阈值的词汇中进行选择，减少了模型输出重复内容的概率。
2. **改进训练数据和方法**：
    - **增强训练数据的多样性**：通过加入多样化的、不同来源和风格的文本数据，减少训练数据中重复内容的比例。
    - **强化学习（RLHF）**：通过强化学习和人类反馈，模型可以更好地学习如何避免无聊或重复的内容生成，增强其生成的创新性。
3. **引入上下文管理**：
    - **更精细的上下文建模**：通过提高模型理解对话上下文的能力，使其能够生成更多符合场景需求的多样化回答。
    - **对话状态跟踪**：在多轮对话中，系统可以跟踪当前对话状态，避免模型在不同对话轮次中输出相同或过度重复的内容。
4. **后处理去重**：
    - **去除重复生成内容**：在模型输出的文本生成后，进行后处理，比如去除连续的重复句子，或选择较为创新和多样化的生成结果。
5. **任务目标引导**：
    - **明确生成目标**：通过明确任务目标或问题的具体需求，引导模型输出更多相关且富有变化的答案，避免过于常规的回复。

通过这些方法，LLMs可以在提高输出质量的同时，减少重复内容的生成，使得其回答更加有趣和多样化。

4. *llama 输入句子长度理论上可以无限长吗？*

理论上，Llama（以及其他基于变压器架构的语言模型）的输入句子长度是有限制的，而不是无限长。这个限制通常由模型的架构和计算资源决定。
1. **模型架构的限制**：大多数现代变压器模型（包括Llama）通常有一个固定的最大输入长度（例如，512个token、1024个token等）。这个长度是由模型的设计和训练过程所决定的，具体取决于输入的词嵌入（embedding）的维度和模型内部的计算需求。Llama系列模型的具体最大长度可能是2048个token或更长，但它不是无限的。
2. **计算资源的限制**：变压器模型的计算复杂度通常是平方级别的，即输入长度增加时，计算和内存需求会显著增加。因此，即便模型本身能够理论上处理更长的序列，硬件资源（如内存和处理能力）也会限制输入的长度。
3. **技术演进**：随着技术进步，研究人员也在尝试解决更长序列的处理问题。例如，通过使用稀疏变压器或其他优化技术（如长序列注意力机制、记忆网络等），可以有效地扩展处理更长序列的能力。

所以，尽管Llama模型在理论上有扩展的可能，但实际应用中它的输入长度是有限的，通常在几千个token左右。

5. *什么时候用BERT模型，什么时候用LLaMA、ChatGLM类大模型，如何选择？*

- BERT模型：
	- 用途：BERT（Bidirectional Encoder Representations from Transformers）是一种基于Transformer的预训练语言模型，特别适用于处理上下文相关的语言理解任务，例如文本分类、命名实体识别（NER）、问答、情感分析等。
	- 选择场景：当任务主要是**文本理解**，并且需要从上下文中捕获深层次的语义关系时，BERT是一个不错的选择。其模型结构优化了双向注意力机制，适用于短文本或需要精确文本表示的场景。
- LLaMA、ChatGLM类大模型：
	- 用途：这些大模型（例如LLaMA、ChatGLM、GPT系列）通常是大规模预训练的生成模型，不仅擅长理解任务，还可以生成自然语言文本。因此，它们适用于对话系统、文本生成、代码生成等任务，甚至可以进行零-shot和少-shot学习。
	- 选择场景：如果任务涉及到更为复杂的交互，或需要大模型进行**文本生成、推理、对话等**更高层次的任务，LLaMA、ChatGLM等大模型更合适。尤其是在对话系统或开放式问题解答中，这类模型的能力会更加突出。

6. *各个专业领域是否需要各自的大模型来服务？*

是的，不同领域的任务可能需要专门针对该领域的模型。随着大模型技术的发展，很多领域都有了专门优化的大模型来提高特定任务的性能。

领域大模型的需求：
- 专业领域模型：例如医学、法律、金融等领域，大型预训练模型通常需要根据该领域的数据进行微调。这是因为这些领域的术语、知识结构和语言特点通常和通用语料库存在较大差异，通用大模型可能无法很好地处理。
- 跨领域模型：虽然也有一些跨领域的通用大模型（如GPT、ChatGLM等），但它们的效果通常不如针对特定领域进行训练的模型。

比如，医学领域的任务通常需要专业的医疗知识，例如疾病诊断、病历总结等，因此需要专门的医学预训练模型，如BioBERT、ClinicalBERT等，来提高准确性。

7. *如何让大模型处理更长的文本？*

大模型处理长文本的挑战主要是由于上下文窗口大小的限制。当前的Transformer架构（如GPT-3、BERT等）通常限制了单次输入的最大长度（例如，512个token、2048个token等）。为了解决这个问题，可以采取以下几种方法：
- 分段处理：将长文本拆分成多个片段，每个片段独立处理，然后将它们的输出合并。这种方法虽然简单，但可能丢失一些长距离的上下文信息。
- Sliding Window（滑动窗口）：通过在长文本上使用滑动窗口机制，逐步处理文本的不同部分，每次处理时将部分上下文信息保留。这种方法可以减少信息丢失，尤其在处理较长的对话或文档时比较有效。
- 长序列模型：一些新型的Transformer变种如Longformer、Reformer、Linformer等，专门优化了长序列的处理能力，能在保留较大上下文的情况下有效地处理更长的文本。它们采用了稀疏注意力机制或其他技术，来减少计算复杂度。
- 模型的分层处理：采用分层架构，先通过较小的上下文窗口对文本进行初步编码，再将多个局部表示聚合起来形成全局表示。这种方法通过逐步缩小范围并融合信息，能够处理更长的文本。
- Memory Augmented Networks：一些方法（如Recurrent Memory Networks、Transformer-XL）引入外部记忆模块，使得模型能够记住长时间段内的信息并跨步骤传递。

通过这些方式，可以让大模型在处理长文本时更有效，尽可能减少上下文丢失。

## [大模型（LLMs）微调面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_u62mcnga3jkd.html)

1. 如果想要在某个模型基础上做[全参数微调](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E5%85%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83&zhida_source=entity)，究竟需要多少显存？
2. 为什么SFT之后感觉LLM傻了?
3. SFT [指令微调数据](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83%E6%95%B0%E6%8D%AE&zhida_source=entity) 如何构建?
4. 领域模型Continue PreTrain 数据选取？
5. 领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力？
6. 领域模型Continue PreTrain ，如何 让模型在预训练过程中就学习到更多的知识？
7. 进行SFT操作的时候，[基座模型](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E5%9F%BA%E5%BA%A7%E6%A8%A1%E5%9E%8B&zhida_source=entity)选用Chat还是Base?
8. [领域模型微调](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E9%A2%86%E5%9F%9F%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83&zhida_source=entity) 指令&数据输入格式 要求？
9. 领域模型微调 领域评测集 构建？
10. 领域模型词表扩增是不是有必要的？
11. 如何训练自己的大模型？
12. 训练中文大模型有啥经验？
13. 指令微调的好处？
14. 预训练和微调哪个阶段注入知识的？
15. 想让模型学习某个领域或行业的知识，是应该预训练还是应该微调？
16. [多轮对话](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D&zhida_source=entity)任务如何微调模型？
17. 微调后的模型出现能力劣化，灾难性遗忘是怎么回事？
18. 微调模型需要多大显存？
19. 大模型LLM进行SFT操作的时候在学习什么？
20. 预训练和SFT操作有什么不同
21. 样本量规模增大，训练出现OOM错
22. 大模型LLM进行SFT 如何对样本进行优化？
23. 模型参数迭代实验
24. 微调大模型的一些建议

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_dh5ln4oerfhk.html)

### [大模型（LLMs）训练经验帖](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_06n25d9wjs0e.html)

- [分布式训练框架](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E6%A1%86%E6%9E%B6&zhida_source=entity)选择？
- LLMs 训练时 有哪些有用的建议？
- 模型大小如何选择？
- 加速卡如何选择？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_06n25d9wjs0e.html)

## 大模型（LLMs）langchain 面

### [大模型（LLMs）langchain 面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ve2dgaiqrjzv.html)

1. 什么是 LangChain?
2. LangChain 包含哪些 核心概念？

- 2.1 LangChain 中 Components and Chains 是什么？
- 2.2 LangChain 中 Prompt Templates and Values 是什么？
- 2.3 LangChain 中 Example Selectors 是什么？
- 2.4 LangChain 中 Output Parsers 是什么？
- 2.5 LangChain 中 Indexes and Retrievers 是什么？
- 2.6 LangChain 中 Chat Message History 是什么？
- 2.7 LangChain 中 Agents and Toolkits 是什么？

1. 什么是 LangChain Agent?
2. 如何使用 LangChain ?
3. LangChain 支持哪些功能?
4. 什么是 LangChain model?
5. LangChain 包含哪些特点?
6. LangChain 如何使用?

- 8.1 LangChain 如何调用 LLMs 生成回复？
- 8.2 LangChain 如何修改 提示模板？
- 8.3 LangChain 如何链接多个组件处理一个特定的下游任务？
- 8.4 LangChain 如何Embedding & vector store？
  
- LangChain 存在哪些问题及方法方案？

1. LangChain 低效的令牌使用问题
2. LangChain 文档的问题
3. LangChain 太多概念容易混淆，过多的“辅助”函数问题
4. LangChain 行为不一致并且隐藏细节问题
5. LangChain 缺乏标准的可互操作数据类型问题

- LangChain 替代方案？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ve2dgaiqrjzv.html)

### [基于LLM+向量库的文档对话 经验面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_m9t1w8pokjpf.html)

- 一、基于LLM+向量库的文档对话 基础面

- 1.1 为什么 大模型 需要 外挂(向量)知识库？
- 1.2. 基于LLM+向量库的文档对话 思路是怎么样？
- 1.3. 基于LLM+向量库的文档对话 核心技术是什么？
- 1.4. 基于LLM+向量库的文档对话 prompt 模板 如何构建？

- 二、基于LLM+向量库的文档对话 存在哪些痛点？
- 三、基于LLM+向量库的文档对话 工程示例面
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_m9t1w8pokjpf.html)

### [LLM文档对话 —— pdf解析关键问题](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_2693k55it84w.html)

- 一、为什么需要进行pdf解析？
- 二、为什么需要 对 pdf 进行解析？
- 三、pdf解析 有哪些方法，对应的区别是什么？
- 四、pdf解析 存在哪些问题？
- 五、如何 长文档（书籍）中关键信息？
- 六、为什么要提取标题甚至是多级标题？
- 七、如何提取 文章标题？
- 八、如何区分单栏还是双栏pdf？如何重新排序？
- 九、如何提取表格和图片中的数据？
- 十、基于AI的文档解析有什么优缺点？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_2693k55it84w.html)

### [基于LLM+向量库的文档对话 经验面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_m9t1w8pokjpf.html)

- 一、基于LLM+向量库的文档对话 基础面

- 1.1 为什么 大模型 需要 外挂(向量)知识库？
- 1.2. 基于LLM+向量库的文档对话 思路是怎么样？
- 1.3. 基于LLM+向量库的文档对话 核心技术是什么？
- 1.4. 基于LLM+向量库的文档对话 prompt 模板 如何构建？

- 二、基于LLM+向量库的文档对话 存在哪些痛点？
- 三、基于LLM+向量库的文档对话 工程示例面
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_m9t1w8pokjpf.html)

## [大模型（LLMs）参数高效微调(PEFT) 面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ahk2br3igwx9.html)

### [大模型（LLMs）参数高效微调(PEFT) 面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ipkod91a939n.html)

- 微调方法是啥？如何微调？
- 为什么需要 PEFT？
- 介绍一下 PEFT？
- PEFT 有什么优点？
- 微调方法批处理大小模式GPU显存速度？
- Peft 和 [全量微调](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E5%85%A8%E9%87%8F%E5%BE%AE%E8%B0%83&zhida_source=entity)区别？
- 多种不同的高效微调方法对比
- 当前[高效微调技术](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF&zhida_source=entity)存在的一些问题
- 高效微调技术[最佳实践](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5&zhida_source=entity)
- PEFT 存在问题？
- 能不能总结一下各种参数高效微调方法？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ipkod91a939n.html)

### [配器微调（Adapter-tuning）篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_h5q2fzq8wvt8.html)

- 一、为什么 需要 [适配器微调](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E9%80%82%E9%85%8D%E5%99%A8%E5%BE%AE%E8%B0%83&zhida_source=entity)（Adapter-tuning）？
- 二、适配器微调（Adapter-tuning）思路？
- 三、 适配器微调（Adapter-tuning）特点是什么？
- 四、AdapterFusion 思路 是什么？
- 五、AdapterDrop 思路 是什么？
- 六、AdapterDrop 特点 是什么？
- 七、MAM Adapter 思路 是什么？
- 八、MAM Adapter 特点 是什么？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_h5q2fzq8wvt8.html)

### [提示学习（Prompting）](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_662wpbw47gtj.html)

- 一、为什么需要 提示学习（Prompting）？
- 二、什么是 提示学习（Prompting）？
- 三、提示学习（Prompting） 有什么优点？
- 四、提示学习（Prompting）有哪些方法，能不能稍微介绍一下它们间？

- 4.1 [前缀微调](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E5%89%8D%E7%BC%80%E5%BE%AE%E8%B0%83&zhida_source=entity)（Prefix-tining）篇

- 4.1.1 为什么需要 前缀微调（Prefix-tining）？
- 4.1.2 前缀微调（Prefix-tining）思路是什么？
- 4.1.3 前缀微调（[Prefix-tining](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=4&q=Prefix-tining&zhida_source=entity)）的优点是什么？
- 4.1.4 前缀微调（Prefix-tining）的缺点是什么？

- 4.2 指示微调（Prompt-tuning）篇

- 4.2.1 为什么需要 指示微调（Prompt-tuning）？
- 4.2.2 指示微调（[Prompt-tuning](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=3&q=Prompt-tuning&zhida_source=entity)）思路是什么？
- 4.2.3 指示微调（Prompt-tuning）优点是什么？
- 4.2.4 指示微调（Prompt-tuning）缺点是什么？
- 4.2.5 指示微调（Prompt-tuning）与 [Prefix-tuning](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=Prefix-tuning&zhida_source=entity) 区别 是什么？
- 4.2.6 指示微调（Prompt-tuning）与 [fine-tuning](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=fine-tuning&zhida_source=entity) 区别 是什么？

- 4.3 P-tuning 篇

- 4.3.1 为什么需要 [P-tuning](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=2&q=P-tuning&zhida_source=entity)？
- 4.3.2 P-tuning 思路是什么？
- 4.3.3 P-tuning 优点是什么？
- 4.3.4 P-tuning 缺点是什么？

- 4.4 P-tuning v2 篇

- 4.4.1 为什么需要 P-tuning v2？
- 4.4.2 P-tuning v2 思路是什么？
- 4.4.3 P-tuning v2 优点是什么？
- 4.4.4 P-tuning v2 缺点是什么？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_662wpbw47gtj.html)

### [LoRA 系列篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ham28l44907e.html)

- 一、LoRA篇

- 1.1 什么是 LoRA？
- 1.2 LoRA 的思路是什么？
- 1.3 LoRA 的特点是什么？

- 二、QLoRA篇

- 2.1 QLoRA 的思路是怎么样的？
- 2.2 QLoRA 的特点是什么？

- 三、AdaLoRA篇

- 3.1 AdaLoRA 的思路是怎么样的？

- 四、LoRA权重是否可以合入原模型？
- 五、ChatGLM-6B LoRA后的权重多大？
- 六、LoRA 微调优点是什么？
- 七、LoRA微调方法为啥能加速训练？
- 八、如何在已有LoRA模型上继续训练？
- 九、LoRA 缺点是什么？
- 十、LoRA这种微调方法和全参数比起来有什么劣势吗？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ham28l44907e.html)

## [大模型（LLMs）推理面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_64vc5vvwpobv.html)

### [大模型（LLMs）推理面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_topiwiusclkr.html)

1. 为什么大模型推理时显存涨的那么多还一直占着？
2. 大模型在gpu和cpu上推理速度如何？
3. 推理速度上，int8和fp16比起来怎么样？
4. 大模型有推理能力吗？
5. 大模型生成时的参数怎么设置？
6. 有哪些省内存的大语言模型训练/微调/推理方法？

- 6.1 如何 估算模型所需的RAM？
- 6.2 Fp16-mixed precision
- 6.3 Int8-bitsandbytes
- 6.4 LoRA
- 6.5 Gradient Checkpointing
- 6.6 Torch FSDP+CPU offload

1. 如何让大模型输出合规化
2. 应用模式变更

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_topiwiusclkr.html)

## 大模型（LLMs）预训练面

### [大模型（LLMs）增量预训练篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_lj47ancwcmv2.html)

1. 为什么要增量预训练？
2. 进行 增量预训练 需要做哪些准备工作？
3. 增量预训练 所用 训练框架？
4. 增量预训练 训练流程 是怎么样？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_lj47ancwcmv2.html)

## [大模型（LLMs）评测面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_j9wcj62eovgc.html)

1. 大模型怎么评测？
2. 大模型的honest原则是如何实现的？模型如何判断回答的知识是训练过的已知的知识，怎么训练这种能力？
3. 如何衡量大模型水平？
4. 大模型评估方法 有哪些？
5. 大模型评估工具 有哪些？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_j9wcj62eovgc.html)

## [大模型（LLMs）强化学习面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_zqs7mjw6c8k7.html)

1. 简单介绍强化学习？
2. 简单介绍一下 RLHF？
3. 奖励模型需要和基础模型一致吗？
4. RLHF 在实践过程中存在哪些不足？
5. 如何解决 人工产生的偏好数据集成本较高，很难量产问题？
6. 如何解决三个阶段的训练（SFT->RM->PPO）过程较长，更新迭代较慢问题
7. 如何解决 PPO 的训练过程同时存在4个模型（2训练，2推理），对计算资源的要求较高 问题？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_zqs7mjw6c8k7.html)

## [大模型（LLMs）软硬件配置面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_m5q8zk3wo84k.html)

1. 建议的软件环境是什么？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_m5q8zk3wo84k.html)

## [大模型（LLMs）训练集面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_jwvpaujrojtt.html)

1. SFT（有监督微调）的数据集格式？
2. RM（奖励模型）的数据格式？
3. PPO（强化学习）的数据格式？
4. 找数据集哪里找？
5. 微调需要多少条数据？
6. 有哪些大模型的训练集？
7. 进行领域大模型预训练应用哪些[数据集](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=4&q=%E6%95%B0%E6%8D%AE%E9%9B%86&zhida_source=entity)比较好？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_jwvpaujrojtt.html)

## [大模型（LLMs）显存问题面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_jhiocx89p3su.html)

1. 大模型大概有多大，模型文件有多大?
2. 能否用4 * v100 32G训练vicuna 65b？
3. 如果就是想要试试65b模型，但是显存不多怎么办？
4. nB模型推理需要多少显存？
5. nB模型训练需要多少显存？
6. 如何 估算模型所需的RAM？
7. 如何评估你的显卡利用率?
8. 测试你的显卡利用率 实现细节篇

1. 如何查看多机训练时的网速？
2. 如何查看服务器上的多卡之间的NVLINK topo？
3. 如何查看服务器上显卡的具体型号?
4. 如何查看训练时的flops？（也就是每秒的计算量）
5. 如何查看对deepspeed的环境配置是否正确？
6. tf32格式有多长？
7. 哪里看各类显卡算力比较？
8. （torch profiler）如何查看自己的训练中通信开销？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_jhiocx89p3su.html)

## 大模型（LLMs）分布式训练面

### [大模型（LLMs）分布式训练面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ah2ibj3z22c7.html)

1. 理论篇

- 1.1 训练 大语言模型 存在问题？
- 1.2 什么是 [点对点通信](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E7%82%B9%E5%AF%B9%E7%82%B9%E9%80%9A%E4%BF%A1&zhida_source=entity)？
- 1.3 什么是 集体通信？
- 1.4 什么是 [数据并行](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C&zhida_source=entity)？
- 1.5 数据并行 如何 提升效率？
- 1.6 什么是 流水线并行？
- 1.7 什么是 张量并行 (intra-layer)？
- 1.8 数据并行 vs 张量并行 vs 流水线并行?
- 1.9 什么是 3D并行？
- 1.10 想要训练1个LLM，如果只想用1张显卡，那么对显卡的要求是什么？
- 1.11 如果有N张显存足够大的显卡，怎么加速训练？
- 1.12 如果显卡的显存不够装下一个完整的模型呢？
- 1.13 PP推理时，是一个串行的过程，1个GPU计算，其他空闲，有没有其他方式？
- 1.14 3种[并行方式](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E5%B9%B6%E8%A1%8C%E6%96%B9%E5%BC%8F&zhida_source=entity)可以叠加吗？
- 1.15 Colossal-AI 有1D/2D/2.5D/3D，是什么情况？
- 1.16 除了3D并行有没有其他方式大规模训练？
- 1.17 有了ZeRO系列，为什么还需要3D并行？
- 1.18 平民适不适合玩3D并行？
- 1.19 平民适不适合直接上多机多卡的ZeRO3（万兆网）？
- 1.20 分布式并行及显存[优化技术](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF&zhida_source=entity)并行技术有哪一些，都有什么特点？
- 1.21 显存优化技术有哪一些，都有什么特点？
- 1.22 常见的分布式训练框架哪一些，都有什么特点？

1. 实践篇

- 2.1 假如有超多的8卡A100节点（DGX A100），如何应用3D并行策略？
- 2.2 如果想构这样一个大规模并行训练系统，训练框架如何选？
- 2.3 训练框架如何选？

1. 并行化策略选择篇

- 3.1 如何选择一款分布式训练框架？
- 3.2 如何选择一款分布式训练框架？
- 3.3 单GPU
- 3.4 单节点多卡
- 3.5 多节点多卡

1. 问题篇

- 4.1 推理速度验证
- 4.2 并行化训练加速
- 4.3 deepspeed 训练过程，报找不主机
- 4.4 为什么 多机训练效率不如单机？
- 4.5 多机训练不通，DeepSPeed配置问题
  
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ah2ibj3z22c7.html)

### [图解分布式训练（一） —— 流水线并行（Pipeline Parallelism）面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ah2ibj3z22c7.html)

- 为什么需要流水线并行（Pipeline Parallelism）？
- 一、流水线并行（Pipeline Parallelism） 优化目标是什么？
- 二、图解 流水线并行（Pipeline Parallelism）模型并行 必要性？
- 三、流水线并行（Pipeline Parallelism） 图解？
- 四、流水线并行（Pipeline Parallelism）优缺点？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_wre1eni0oq7d.html)

### [图解分布式训练（二） —— nn.DataParallel面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ah2ibj3z22c7.html)

- 为什么需要nn.DataParallel？
- 一、pytorch中的GPU操作默认是什么样？
- 二、介绍一下 nn.DataParallel 函数？
- 三、nn.DataParallel 函数 处理逻辑 介绍一下？
- 四、nn.DataParallel 函数 常见问题及解答 有哪些？

- 4.1 多GPU计算减少了程序运行的时间？
- 4.2 如何保存和加载多GPU训练模型呢？
- 4.3 为什么第一块卡的显存会占用的更多一些？
- 4.4 直接使用nn.DataParallel的时候，训练采用多卡训练，会出现一个warning？
- 4.5 device_ids 0 被占用问题

- 五、nn.DataParallel 函数 参数更新方式 ？
- 六、nn.DataParallel 函数 优点 介绍一下？
- 七、nn.DataParallel 函数 缺点 介绍一下？
- 八、nn.DataParallel 函数 实战？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_wre1eni0oq7d.html)

### [图解分布式训练（三） —— nn.parallel.DistributedDataParallel](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_i4s3ia057rmh.html)

- 为什么需要 nn.parallel.DistributedDataParallel ？
- 一、什么是 DistributedDataParallel 核心 —— Ring-AllReduce？
- 二、nn.parallel.DistributedDataParallel 函数 介绍一下？
- 三、nn.parallel.DistributedDataParallel 函数 如何多卡加速训练？
- 四、nn.parallel.DistributedDataParallel 实现流程介绍一下？
- 五、nn.parallel.DistributedDataParallel 参数更新介绍一下？
- 六、nn.DataParallel(以下简称DP) vs DistributedDataParallel(以下简称DDP)介绍一下？
- 七、DistributedDataParallel(以下简称DDP) 优点有哪些？
- 八、DistributedDataParallel(以下简称DDP) 缺点有哪些？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_i4s3ia057rmh.html)

### [图解分布式训练（四） —— torch.multiprocessing 详细解析](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_gu9smpbn510e.html)

- 一、torch.multiprocessing 函数介绍一下？
- 二、torch.multiprocessing 函数如何使用？
- 三、介绍一下 共享CUDA张量？
- 四、介绍一下 共享策略？
- 五、torch.multiprocessing 函数使用
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_gu9smpbn510e.html)

### [图解分布式训练（五） —— AMP混合精度训练 详细解析](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_0slrgoti6gvb.html)

- 为什么需要 AMP混合精度训练？
- 一、什么是自动混合精度训练(AMP)
- 二、为什么需要自动混合精度？
- 三、混合精度训练的优点是什么？
- 四、混合精度训练的缺点是什么？
- 五、混合精度训练的关键技术是什么？
- 六、介绍一下 混合精度训练 动态损失缩放？
- 七、如何在PyTorch中使用自动混合精度？
- 八、如何使用 AMP混合精度训练 ？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_0slrgoti6gvb.html)

### [图解分布式训练（六） —— Pytorch的 DeepSpeed 详细解析](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_2v6wv29ce8nn.html)

- 一、为什么需要 Deepspeed？
- 二、DeepSpeed 基本概念 介绍一下？
- 三、DeepSpeed 通信策略 介绍一下？
- 四、DeepSpeed 如何使用？
- 五、DeepSpeed 代码实现？
- 七、训练精度 介绍一下？
- 八、获取模型参数 介绍一下？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_2v6wv29ce8nn.html)

### [图解分布式训练（七）—— accelerate 分布式训练 详细解析](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_o5wkeionnqr7.html)

- 一、为什么需要 accelerate 分布式训练？
- 二、什么是 accelerate 分布式训练?
- 三、accelerate 分布式训练 原理讲解？
- 四、accelerate 分布式训练 如何实践？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_o5wkeionnqr7.html)

### [图解分布式训练（八）—— ZeRO 学习](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_600z63vou4nj.html)

- 一、什么是 3D 并行？
- 二、3D 并行 策略有哪些？
- 三、为什么需要 ZeRO？
- 四、ZeRO 的 核心思想是什么？
- 五、ZeRO 显存如何分配？
- 六、ZeRO 优化策略是怎么样？
- 七、ZeRO Offload后的计算流程是怎么样？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_600z63vou4nj.html)

## [大模型（LLMs）agent 面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_9dfwi0ooio2z.html)

1. 如何给LLM注入领域知识？
2. 如果想要快速体验各种模型，该怎么办？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_mzfogrjhkp17.html)

## [Token及模型参数准备篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_9oplu4014qx5.html)

1. 预训练数据 Token 重复 是否影响 模型性能？
2. SFT需要训练Token数？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_9oplu4014qx5.html)

## [LLMs 位置编码篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_bmn80nar12c7.html)

- 1 什么是位置编码？
- 2 什么是绝对位置编码？
- 3 什么是相对位置编码？
- 4 旋转位置编码 RoPE篇

- 4.1 旋转位置编码 RoPE 思路是什么？
- 4.2 推导一下 旋转位置编码 RoPE ？
- 4.3 旋转位置编码 RoPE 有什么优点？
- 4.4 旋转位置编码 RoPE 被哪些 LLMs 应用？

- 5 长度外推问题篇

- 5.1 什么是 长度外推问题？
- 5.2 长度外推问题 的 解决方法 有哪些？

- 6 ALiBi (Attention with Linear Biases)篇

- 6.1 ALiBi (Attention with Linear Biases) 思路是什么？
- 6.2 ALiBi (Attention with Linear Biases) 的偏置矩阵是什么？有什么作用？
- 6.3 ALiBi (Attention with Linear Biases) 有什么优点？
- 6.4 ALiBi (Attention with Linear Biases) 被哪些 LLMs 应用？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_bmn80nar12c7.html)

## LLMs Tokenizer 篇

### [LLMs Tokenizer 篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_c1wrizv0im1a.html)

- Byte-Pair Encoding(BPE)篇

- 1 Byte-Pair Encoding(BPE) 如何构建词典？

- WordPiece 篇

- 1 WordPiece 与 BPE 异同点是什么？

- SentencePiece 篇

- 简单介绍一下 SentencePiece 思路？

- 对比篇

- 1 举例 介绍一下 不同 大模型LLMs 的分词方式？
- 2 介绍一下 不同 大模型LLMs 的分词方式 的区别？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_c1wrizv0im1a.html)

### [怎么让英文大语言模型支持中文？（一） —— 构建中文tokenization](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_w0d2q29sueq7.html)

- 一、为什么需要 构建中文tokenization？
- 二、如何对 原始数据预处理？
- 三、如何构建中文的词库？
- 四、如何使用transformers库加载sentencepiece模型？
- 五、如何合并英文词表和中文词表？
- 六、怎么使用修改后的词表？
- 总结一下 构建中文tokenization？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_w0d2q29sueq7.html)

### [怎么让英文大语言模型支持中文？（二） —— 继续预训练篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_jprkwhrvf3tw.html)

- 一、为什么需要进行继续预训练？
- 二、如何对 继续预训练 数据预处理？
- 三、如何 构建模型？
- 四、如何 使用模型？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_jprkwhrvf3tw.html)

### [怎么让英文大语言模型支持中文？（三） —— 对预训练模型进行指令微调](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_p2wj7zadwxwb.html)

- 一、为什么需要对预训练模型进行指令微调？
- 二、对预训练模型进行指令微调 数据 如何处理？
- 三、对预训练模型进行指令微调 tokenization 如何构建？
- 四、对预训练模型进行指令微调 模型 如何构建？
- 五、是否可以结合 其他库 使用？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_p2wj7zadwxwb.html)

## [Layer normalization 篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_pzcgd4ovk098.html)

- Layer normalization-方法篇

- Layer Norm 篇

- Layer Norm 的计算公式写一下？

- RMS Norm 篇 （[均方根](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E5%9D%87%E6%96%B9%E6%A0%B9&zhida_source=entity) Norm）

- RMS Norm 的计算公式写一下？
- RMS Norm 相比于 Layer Norm 有什么特点？

- Deep Norm 篇

- Deep Norm 思路？
- 写一下 Deep Norm 代码实现？

- Deep Norm 有什么优点？

- Layer normalization-位置篇

- 1 LN 在 LLMs 中的不同位置 有什么区别么？如果有，能介绍一下区别么？

- Layer normalization 对比篇

- LLMs 各模型分别用了 哪种 Layer normalization？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_pzcgd4ovk098.html)

## [LLMs 激活函数篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_6xm3wzzice2s.html)

- 1 介绍一下 FFN 块 计算公式？
- 2 介绍一下 GeLU 计算公式？
- 3 介绍一下 Swish 计算公式？
- 4 介绍一下 使用 GLU 线性门控单元的 FFN 块 计算公式？
- 5 介绍一下 使用 GeLU 的 GLU 块 计算公式？
- 6 介绍一下 使用 Swish 的 GLU 块 计算公式？
- 各LLMs 都使用哪种激活函数？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_6xm3wzzice2s.html)

## [LLMs 激活函数篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_6xm3wzzice2s.html)

- 1 介绍一下 FFN 块 计算公式？
- 2 介绍一下 GeLU 计算公式？
- 3 介绍一下 Swish 计算公式？
- 4 介绍一下 使用 GLU 线性门控单元的 FFN 块 计算公式？
- 5 介绍一下 使用 GeLU 的 GLU 块 计算公式？
- 6 介绍一下 使用 Swish 的 GLU 块 计算公式？
- 各LLMs 都使用哪种激活函数？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_6xm3wzzice2s.html)

## 大模型（LLMs）加速篇

### [大模型（LLMs）加速篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_w9wewc152eux.html)

1. 当前优化模型最主要技术手段有哪些？
2. 推理加速框架有哪一些？都有什么特点？

- 3 vLLM 篇

- 3.1 vLLM 的 功能有哪些？
- 3.2 vLLM 的 优点有哪些？
- 3.3 vLLM 的 缺点有哪些？
- 3.4 vLLM 离线批量推理？
- 3.5 vLLM API Server？

- 4 Text generation inference 篇

- 4.1 介绍一下 Text generation inference？
- 4.2 Text generation inference 的 功能有哪些？
- 4.3 Text generation inference 的 优点有哪些？
- 4.4 Text generation inference 的 缺点有哪些？
- 4.5 Text generation inference 的 使用docker运行web server？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_w9wewc152eux.html)

### [LLM（大语言模型）部署加速方法——PagedAttention篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_p22mjq881n3n.html)

- 一、vLLM 用于大模型并行推理加速 存在什么问题？
- 二、vLLM 如何 优化 大模型并行推理加速？
- 三、什么是 PagedAttention？
- 四、 PagedAttention 如何存储 连续的key和value？
- 五、 PagedAttention 技术细节？
- 六、 PagedAttention 如何 实现安全共享？
- 七、 PagedAttention 源码介绍？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_p22mjq881n3n.html)

### [大模型推理加速工具 —— vLLM](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_zw5h9ogvac2w.html)

- 一、引言

- 1.1 前言
- 1.2 为什么 需要 vLLM ?
- 1.3 vLLM 具有哪些特点 ?
- 1.4 vLLM 支持哪些 Huggingface 模型 ?

- 二、vLLM 性能如何？
- 三、vLLM 依赖包
- 四、vLLM 如何安装？
- 五、vLLM 如何使用？
- 六、vLLM 分布式推理与服务
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_zw5h9ogvac2w.html)

### [LLM（大语言模型）部署加速方法——Faster Transformer篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_dd2gowztxtfg.html)

- 一、为什么需要 FasterTransformer？
- 二、FasterTransformer 介绍一下？
- 三、FasterTransformer 核心是什么？
- 四、FasterTransformer 优化？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_dd2gowztxtfg.html)

### [纯Python超轻量高性能LLM推理框架 —— LightLLM](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_9a643feq2b0b.html)

- 一、引言

- 1.1 前言
- 1.2 为什么 需要 LightLLM ?
- 1.3 目前 LLM推理框架 有 哪些?

- 二、LightLLM 介绍一下？

- 2.1 什么是 LightLLM ？
- 2.2 Token Attention 介绍？
- 2.3 Efficient Router 介绍？

- 三、LightLLM 性能表现 介绍？
- 四、LightLLM 依赖包 有哪些？
- 五、LightLLM 如何安装？

- 5.1 下载 LightLLM
- 5.2 安装 LightLLM 依赖
- 5.3 安装 LightLLM

- 六、LightLLM 如何使用？

- 6.1 启动 LightLLM 服务

- 填坑笔记

- LightLLM 支持模型 LLMs 模型？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_9a643feq2b0b.html)

## [Attention 升级面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_j0nwuo0frw2x.html)

- 1 传统 Attention 存在哪些问题？
- 2 Attention 优化方向
- 3 Attention 变体有哪些？
- 4 Multi-Query Attention 篇

- 4.1 Multi-head Attention 存在什么问题？
- 4.2 介绍一下 Multi-Query Attention？
- 4.3 对比一下 Multi-head Attention 和 Multi-Query Attention？
- 4.4 Multi-Query Attention 这样做的好处是什么？
- 4.5 有 哪些模型 是 使用 Multi-Query Attention？

- 5 Grouped-query Attention

- 5.1 什么是 Grouped-query Attention？
- 5.2 有哪些大模型使用 Grouped-query Attention？

- 6 FlashAttention 介绍一下
- 7 并行 transformer block 介绍一下？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_j0nwuo0frw2x.html)

## 大模型幻觉（LLM Hallucination）面

### [大模型幻觉（LLM Hallucination）面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_schwrdmvmhr7.html)

- 一、什么是大模型幻觉？
- 二、为什么LLM会产生幻觉？
- 三、为什么需要解决LLM的幻觉问题？
- 四、幻觉一定是有害的吗？
- 五、幻觉有哪些不同类型？
- 六、如何度量幻觉？
- 七、如何缓解LLM幻觉？

- 7.1 通过使用外部知识验证主动检测和减轻幻觉
- 7.2 事实核心采样
- 7.3 SelfCheckGPT

- 八、LLMs什么时候最容易产生幻觉？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_schwrdmvmhr7.html)

### [大模型的幻觉问题篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_8mr4mlhe5q1x.html)

- 一、什么是 大模型幻觉问题？
- 二、为什么 会 出现 大模型幻觉问题？
- 三、如何 评估 大模型幻觉问题？
- 四、如何 缓解 大模型幻觉问题？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_8mr4mlhe5q1x.html)

### [大模型的幻觉问题篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_tbezgzifowzp.html)

- 一、为什么 会 出现 大模型幻觉？
- 二、如何 缓解 大模型幻觉？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_tbezgzifowzp.html)

## LLMs 对比篇

### [LLMs 对比篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_tbezgzifowzp.html)

- LLMs 训练数据 和 数据量 对比如何？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_tbezgzifowzp.html)

### [百川智能baichuan7B、13B、53B、baichuan2 总结篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ma6pw7v2g9pi.html)

- 一、baichuan-7B篇

1. 你了解baichuan-7B解构么？介绍一下？
2. baichuan-7B 如何 收集原始数据并 构建 训练数据？
3. baichuan-7B 如何 提高 训练稳定性和吞吐？

- 二、baichuan-13B篇

1. 相比于 baichuan-7B，baichuan-13B 的 特点体现在哪里？
2. 如何 对 baichuan-13B 进行推理和部署？
3. 如何 对 baichuan-13B 进行微调？

- 三、baichuan-53B篇

- 3.1 baichuan-53B 相比于 baichuan-7B 和 baichuan-13B 有哪些优势？
- 3.2 baichuan-53B 如何对 预训练数据 做处理？
- 3.3 baichuan-53B 如何进行 搜索增强？

- 四、baichuan2篇

- 4.1 baichuan2 与 其他大模型 对比

- 五、baichuan 数据构建篇

- 5.1 baichuan 进行微调时，领域数据：通用数据配比？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ma6pw7v2g9pi.html)

## 思维链 Chain-of-Thought（COT）篇

### [思维链 Chain-of-Thought（COT）篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_1cjjxf95az70.html)

- 一、什么是思维链提示？
- 二、思维链提示本质是什么？
- 三、思维链提示 与 标准的提示学习方法有什么不同?
- 四、思维链提示 为什么可以提高语言模型的复杂推理能力?它的优势在哪里?
- 五、思维链提示 适用场景 有 哪些？
- 六、思维链提示 目前还存在哪些不足点？
- 七、思维链提示 对推动语言模型复杂推理能力研究有哪些启发和影响?
- 八、思维链提示 对实现真正的通用人工智能仍面临哪些挑战?
- 九、如何通过增加模型规模来获得语言模型强大的思路链推理能力的?这与模型获得的哪些能力有关?
- 十、你认为可以在哪些其他方面应用“思路链提示”这一思路来提升语言模型的能力?
- 十一、如果需要你对 思维链提示 进行改进，你觉得你会改进哪些地方？
- 十二、思维链提示 未来研究方向？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_1cjjxf95az70.html)

### [思维链 Chain-of-Thought（COT）变体篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_sw5aljfzswiv.html)

- 思维链 Chain-of-Thought（COT）：思维链的启蒙

1. 什么是 思维链 Chain-of-Thought（COT）？
2. 思维链 Chain-of-Thought（COT）是思路是什么？
3. 思维链 Chain-of-Thought（COT）存在问题？

- 思维树 Tree of Thoughts（TOT）：一种用树结构解决复杂问题的方法

1. 为什么需要 思维树 Tree of Thoughts（TOT）？
2. 什么是 思维树 Tree of Thoughts（TOT）？
3. 思维树 Tree of Thoughts（TOT）涉及问题有哪些？

- 思维图 Graph of Thoughts（GOT）：一种把思维链过程建模层图结构的方法

1. 为什么 需要 思维图 Graph of Thoughts（GOT）？
2. 什么是 思维图 Graph of Thoughts（GOT） ？
3. 思维图 Graph of Thoughts（GOT）核心思想是什么 ？

- 思维算法 Algorithm of Thoughts（AOT）：一种用DFS/BFS示例解决问题的方法

1. 为什么 需要 思维算法 Algorithm of Thoughts（AOT）？
2. 思维算法 Algorithm of Thoughts（AOT）思路是什么？
3. 思维算法 Algorithm of Thoughts（AOT） vs 其他 COT 的 区别？

- 思维链 Chain-of-Thought（COT） 有哪些 应用场景？
- 思维链 Chain-of-Thought（COT） 有哪些 局限性？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_sw5aljfzswiv.html)

## [思维链 Chain-of-Thought（COT）变体篇](https://link.zhihu.com/?target=https%3A//file%2B.vscode-resource.vscode-cdn.net/f%3A/llms_interview_notes/llms_interview_notes_gitee/hhttps%3A/articles.zsxq.com/id_dwhonmw976n7.html)

- 一、为什么需要 Graph RAG？
- 二、什么是 Graph RAG？
- 三、Graph RAG 思路介绍？
- 四、用代码 介绍 Graph RAG ？
- 五、用 示例 介绍 Graph RAG ？
- 六、Graph RAG 排序优化方式？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_dwhonmw976n7.html)