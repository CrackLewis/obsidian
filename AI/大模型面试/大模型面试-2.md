
[src](https://zhuanlan.zhihu.com/p/659042194)

## [大模型（LLMs）基础面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_a55uo10835nv.html)

1. 目前 主流的开源模型体系 有哪些？
2. prefix Decoder 和 causal Decoder 和 Encoder-Decoder 区别是什么？
3. 大模型LLM的 训练目标 是什么？
4. 涌现能力是啥原因？
5. 为何现在的大模型大部分是Decoder only结构？
6. 简单 介绍一下 大模型【LLMs】？
7. 大模型【LLMs】后面跟的 175B、60B、540B等 指什么？
8. 大模型【LLMs】具有什么优点？
9. 大模型【LLMs】具有什么缺点？

A:

- **目前主流的开源模型体系有哪些？**
    - **Transformers**（如BERT、GPT系列、T5等）：这些是基于自注意力机制（Self-Attention）的模型，广泛用于NLP任务。
    - **BERT系列**（如RoBERTa, DistilBERT等）：用于文本理解的双向编码器模型。
    - **GPT系列**：基于解码器结构，主要用于生成任务，最近的版本包括GPT-3、GPT-4等。
    - **T5**：Encoder-Decoder架构，擅长多任务学习。
    - **Stable Diffusion**：用于图像生成的扩散模型。
    - **CLIP**：连接图像和文本的多模态模型。
    - **LLaMA系列**：Meta的开源大语言模型，注重在多个不同任务上的表现。
    - **OpenLLaMA**：由社区开发的基于Meta LLaMA架构的开源项目。
- **Prefix Decoder 和 Causal Decoder 和 Encoder-Decoder 区别是什么？**
    - **Prefix Decoder**：在生成时通过输入一个“前缀”来引导解码过程。这种结构通常在少量输入的情况下，借助已知的信息来生成输出。常用于条件生成任务。
    - **Causal Decoder**：这种解码器采用自回归的方式（前一步的输出作为下一步的输入），通常用于生成任务，如GPT模型。其特点是只能考虑到前文信息，具有因果性。
    - **Encoder-Decoder**：这种结构包含两个部分——编码器将输入信息转换为表示向量，解码器则根据这些向量生成输出。这是机器翻译、文本摘要等任务中的经典结构，如T5和BART。
- **大模型LLM的训练目标是什么？**
    - 大模型的训练目标通常是最大化语言建模的对数似然函数，即通过训练使模型能够预测下一个词（或Token）。目标是让模型学习到足够的语义和语法信息，以便在多种自然语言处理任务上具备良好的表现。具体目标可以包括：
        - **自回归语言建模**：生成下一个词。
        - **掩蔽语言建模（Masked Language Modeling, MLM）**：预测被掩蔽的词（如BERT所用）。
        - **多任务学习**：如T5和BART采用了统一框架处理多种任务。
- **涌现能力是啥原因？**
    - **涌现能力**（Emergent Abilities）指的是大规模模型在训练过程中出现的、未显式设计的复杂行为或能力。例如，大模型可能在少量的训练数据或少数几次示例上展现出超出其设计目标的表现。其原因包括：
        - 模型规模的增加导致其对数据的拟合能力和泛化能力增强。
        - 大量的训练数据和训练参数使得模型能学习到更复杂的模式。
        - 自注意力机制等架构使得模型能够更好地理解长距离依赖关系。
- **为何现在的大模型大部分是Decoder only结构？**
    - **Decoder-only模型**（如GPT系列）通常用于生成任务，这类任务的特点是自回归生成文本（每次生成一个词并将其作为输入）。这种结构的优点包括：
        - 生成能力强，适用于长文本的连贯生成。
        - 训练过程中，只需要考虑生成每个词的条件概率，不需要像Encoder-Decoder那样涉及到复杂的编码过程。
        - 更高效：在解码阶段，生成每个词的过程中都基于已经生成的部分，避免了编码-解码间的复杂数据传递。
- **简单介绍一下大模型【LLMs】？**
    - **大模型（LLMs）**是指拥有大量参数的语言模型，通常具有数十亿到数百亿的参数。这些模型通过海量的文本数据进行训练，具备较强的理解、生成能力，可以用于各类NLP任务，如文本生成、翻译、问答、摘要等。
    - 典型的LLMs包括GPT-3、GPT-4、T5、BERT等，它们的特点是使用了Transformer架构，采用自注意力机制，能够捕捉长距离依赖和复杂的语言模式。
- **大模型【LLMs】后面跟的175B、60B、540B等指什么？**
    - **175B、60B、540B等**是大模型中参数的数量，指的是模型中可训练参数的总数。例如，GPT-3有1750亿个参数，GPT-4的参数数量可能更高。参数数量是衡量模型规模的重要指标之一，通常随着参数数量的增加，模型的性能在许多任务上得到提升。
- **大模型【LLMs】具有什么优点？**
    - **强大的性能**：由于参数数量巨大，LLMs在多个NLP任务上通常表现出色，如文本生成、问答、翻译等。
    - **迁移学习能力**：经过预训练的大模型能够通过微调适应多种任务，减少了传统机器学习中的手动特征工程。
    - **处理复杂语言模式**：能够捕捉语法、语义和上下文关系，处理复杂的语言任务。
- **大模型【LLMs】具有什么缺点？**
    - **计算资源要求高**：训练和推理都需要大量的计算资源和内存，对硬件要求很高，成本昂贵。
    - **缺乏透明度**：大模型的内部工作往往不透明，很难解释其做出的决策，这对高风险应用（如医疗、法律等）是一个挑战。
    - **数据偏见问题**：模型可能会学习到训练数据中的偏见，导致不公正或不合适的结果。
    - **能耗高**：大规模模型的训练和使用消耗大量能源，对环境有影响。
    - **难以调优**：调优这样庞大的模型需要专业知识和时间，且在小规模数据集上可能表现欠佳。

## [大模型（LLMs）进阶面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_v6gltxd4qbxd.html)

1. LLMs 复读机问题
	1. 什么是 LLMs 复读机问题？
	2. 为什么会出现 LLMs 复读机问题？
	3. 如何缓解 LLMs 复读机问题？
2. llama 系列问题
	1. llama 输入句子长度理论上可以无限长吗？
3. 什么情况用Bert模型，什么情况用LLaMA、ChatGLM类大模型，咋选？
4. 各个专业领域是否需要各自的大模型来服务？
5. 如何让大模型处理更长的文本？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_v6gltxd4qbxd.html)

A:

1. *什么是 LLMs 复读机问题？*

LLMs（大型语言模型）复读机问题是指，当使用语言模型（如 GPT）进行对话或生成文本时，模型可能会出现过度重复或不断复述相同内容的情况。这个问题表现为模型反复输出类似或完全相同的句子、段落或想法，缺乏足够的多样性和创新性。这种问题通常影响对话的流畅性和互动性，可能使用户体验下降。

2. *为什么会出现 LLMs 复读机问题？*

复读机问题的出现有几个原因：
1. **训练数据问题**：模型的训练数据中可能存在大量重复的信息或者一些特定模式，模型可能在生成时倾向于回到这些熟悉的结构或表述上。
2. **生成策略问题**：LLMs在生成文本时，采用的采样方法（如贪心解码、温度调节等）可能使模型偏向生成相似的输出。例如，低温度的采样策略可能导致模型输出确定性更强的、重复性较高的文本。
3. **缺乏上下文理解**：在某些情况下，LLMs可能没有深入理解对话的上下文，导致它们反复使用相同的短语或句式来回应问题。
4. **有限的模型容量**：尽管LLMs具有强大的语言生成能力，但它们仍然是基于统计学的模型，可能在处理长时间的对话或复杂任务时失去多样性，偏向生成已知的简单响应。

3. *如何缓解 LLMs 复读机问题？*

可以通过以下几种方法缓解复读机问题：
1. **调整生成参数**：
    - **增加温度**：提高温度值（例如，从 0.7 提高到 1.0 或更高）可以让模型的输出更加多样化，避免过于确定的选择，减少重复性。
    - **使用Top-k或Top-p采样**：这些策略通过限制模型只能从概率最高的前k个词或满足一定累计概率阈值的词汇中进行选择，减少了模型输出重复内容的概率。
2. **改进训练数据和方法**：
    - **增强训练数据的多样性**：通过加入多样化的、不同来源和风格的文本数据，减少训练数据中重复内容的比例。
    - **强化学习（RLHF）**：通过强化学习和人类反馈，模型可以更好地学习如何避免无聊或重复的内容生成，增强其生成的创新性。
3. **引入上下文管理**：
    - **更精细的上下文建模**：通过提高模型理解对话上下文的能力，使其能够生成更多符合场景需求的多样化回答。
    - **对话状态跟踪**：在多轮对话中，系统可以跟踪当前对话状态，避免模型在不同对话轮次中输出相同或过度重复的内容。
4. **后处理去重**：
    - **去除重复生成内容**：在模型输出的文本生成后，进行后处理，比如去除连续的重复句子，或选择较为创新和多样化的生成结果。
5. **任务目标引导**：
    - **明确生成目标**：通过明确任务目标或问题的具体需求，引导模型输出更多相关且富有变化的答案，避免过于常规的回复。

通过这些方法，LLMs可以在提高输出质量的同时，减少重复内容的生成，使得其回答更加有趣和多样化。

4. *llama 输入句子长度理论上可以无限长吗？*

理论上，Llama（以及其他基于变压器架构的语言模型）的输入句子长度是有限制的，而不是无限长。这个限制通常由模型的架构和计算资源决定。
1. **模型架构的限制**：大多数现代变压器模型（包括Llama）通常有一个固定的最大输入长度（例如，512个token、1024个token等）。这个长度是由模型的设计和训练过程所决定的，具体取决于输入的词嵌入（embedding）的维度和模型内部的计算需求。Llama系列模型的具体最大长度可能是2048个token或更长，但它不是无限的。
2. **计算资源的限制**：变压器模型的计算复杂度通常是平方级别的，即输入长度增加时，计算和内存需求会显著增加。因此，即便模型本身能够理论上处理更长的序列，硬件资源（如内存和处理能力）也会限制输入的长度。
3. **技术演进**：随着技术进步，研究人员也在尝试解决更长序列的处理问题。例如，通过使用稀疏变压器或其他优化技术（如长序列注意力机制、记忆网络等），可以有效地扩展处理更长序列的能力。

所以，尽管Llama模型在理论上有扩展的可能，但实际应用中它的输入长度是有限的，通常在几千个token左右。

5. *什么时候用BERT模型，什么时候用LLaMA、ChatGLM类大模型，如何选择？*

- BERT模型：
	- 用途：BERT（Bidirectional Encoder Representations from Transformers）是一种基于Transformer的预训练语言模型，特别适用于处理上下文相关的语言理解任务，例如文本分类、命名实体识别（NER）、问答、情感分析等。
	- 选择场景：当任务主要是**文本理解**，并且需要从上下文中捕获深层次的语义关系时，BERT是一个不错的选择。其模型结构优化了双向注意力机制，适用于短文本或需要精确文本表示的场景。
- LLaMA、ChatGLM类大模型：
	- 用途：这些大模型（例如LLaMA、ChatGLM、GPT系列）通常是大规模预训练的生成模型，不仅擅长理解任务，还可以生成自然语言文本。因此，它们适用于对话系统、文本生成、代码生成等任务，甚至可以进行零-shot和少-shot学习。
	- 选择场景：如果任务涉及到更为复杂的交互，或需要大模型进行**文本生成、推理、对话等**更高层次的任务，LLaMA、ChatGLM等大模型更合适。尤其是在对话系统或开放式问题解答中，这类模型的能力会更加突出。

6. *各个专业领域是否需要各自的大模型来服务？*

是的，不同领域的任务可能需要专门针对该领域的模型。随着大模型技术的发展，很多领域都有了专门优化的大模型来提高特定任务的性能。

领域大模型的需求：
- 专业领域模型：例如医学、法律、金融等领域，大型预训练模型通常需要根据该领域的数据进行微调。这是因为这些领域的术语、知识结构和语言特点通常和通用语料库存在较大差异，通用大模型可能无法很好地处理。
- 跨领域模型：虽然也有一些跨领域的通用大模型（如GPT、ChatGLM等），但它们的效果通常不如针对特定领域进行训练的模型。

比如，医学领域的任务通常需要专业的医疗知识，例如疾病诊断、病历总结等，因此需要专门的医学预训练模型，如BioBERT、ClinicalBERT等，来提高准确性。

7. *如何让大模型处理更长的文本？*

大模型处理长文本的挑战主要是由于上下文窗口大小的限制。当前的Transformer架构（如GPT-3、BERT等）通常限制了单次输入的最大长度（例如，512个token、2048个token等）。为了解决这个问题，可以采取以下几种方法：
- 分段处理：将长文本拆分成多个片段，每个片段独立处理，然后将它们的输出合并。这种方法虽然简单，但可能丢失一些长距离的上下文信息。
- Sliding Window（滑动窗口）：通过在长文本上使用滑动窗口机制，逐步处理文本的不同部分，每次处理时将部分上下文信息保留。这种方法可以减少信息丢失，尤其在处理较长的对话或文档时比较有效。
- 长序列模型：一些新型的Transformer变种如Longformer、Reformer、Linformer等，专门优化了长序列的处理能力，能在保留较大上下文的情况下有效地处理更长的文本。它们采用了稀疏注意力机制或其他技术，来减少计算复杂度。
- 模型的分层处理：采用分层架构，先通过较小的上下文窗口对文本进行初步编码，再将多个局部表示聚合起来形成全局表示。这种方法通过逐步缩小范围并融合信息，能够处理更长的文本。
- Memory Augmented Networks：一些方法（如Recurrent Memory Networks、Transformer-XL）引入外部记忆模块，使得模型能够记住长时间段内的信息并跨步骤传递。

通过这些方式，可以让大模型在处理长文本时更有效，尽可能减少上下文丢失。

## [大模型（LLMs）微调面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_u62mcnga3jkd.html)

1. 如果想要在某个模型基础上做[全参数微调](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E5%85%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83&zhida_source=entity)，究竟需要多少显存？
2. 为什么SFT之后感觉LLM傻了?
3. SFT [指令微调数据](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83%E6%95%B0%E6%8D%AE&zhida_source=entity) 如何构建?
4. 领域模型Continue PreTrain 数据选取？
5. 领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力？
6. 领域模型Continue PreTrain ，如何 让模型在预训练过程中就学习到更多的知识？
7. 进行SFT操作的时候，[基座模型](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E5%9F%BA%E5%BA%A7%E6%A8%A1%E5%9E%8B&zhida_source=entity)选用Chat还是Base?
8. [领域模型微调](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E9%A2%86%E5%9F%9F%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83&zhida_source=entity) 指令&数据输入格式 要求？
9. 领域模型微调 领域评测集 构建？
10. 领域模型词表扩增是不是有必要的？
11. 如何训练自己的大模型？
12. 训练中文大模型有啥经验？
13. 指令微调的好处？
14. 预训练和微调哪个阶段注入知识的？
15. 想让模型学习某个领域或行业的知识，是应该预训练还是应该微调？
16. [多轮对话](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D&zhida_source=entity)任务如何微调模型？
17. 微调后的模型出现能力劣化，灾难性遗忘是怎么回事？
18. 微调模型需要多大显存？
19. 大模型LLM进行SFT操作的时候在学习什么？
20. 预训练和SFT操作有什么不同
21. 样本量规模增大，训练出现OOM错
22. 大模型LLM进行SFT 如何对样本进行优化？
23. 模型参数迭代实验
24. 微调大模型的一些建议

### 1-8

这些问题涉及了深度学习中的几个高级概念，尤其是大语言模型（LLM）的微调和预训练。以下是逐一回答：

1. **全参数微调显存需求：** 全参数微调的显存需求通常与模型的大小、训练数据批次大小（batch size）、以及每次训练中使用的梯度累积步数等因素相关。例如，GPT-3（175B参数）模型的显存需求可能需要上百GB的显存。一般来说，显存需求随模型规模的增大而线性增加，但如果你采用混合精度训练（如FP16），显存需求可能会有所减少。如果是BERT或GPT等基于Transformer的模型，通常在显存可用的情况下可以微调。
2. **SFT之后LLM傻了的原因：** 指令微调（SFT）过程可能使得模型过度拟合到某些特定任务或指令格式，导致其对其他类型的问题或开放式任务的表现变差。常见原因包括：
    - **任务特定化**：模型在某个特定的指令或任务上进行过度训练，丧失了原本的多样性和泛化能力。
    - **数据偏差**：如果SFT的数据没有覆盖广泛的场景或问题类型，可能导致模型产生偏见，限制了其开放问题的处理能力。
    - **过拟合问题**：如果微调时使用了不合适的超参数（如过高的学习率、过少的数据等），可能导致模型只学习到特定的模式，而无法应对其他问题。
3. **SFT指令微调数据如何构建：** SFT数据集的构建需要包含：
    - **任务明确的指令**：例如让模型在特定情境下执行某项任务（例如问题回答、摘要生成、翻译等）。
    - **多样化的输入输出样例**：包括不同的任务情境和输入格式，以帮助模型在不同场景下执行指令。
    - **领域相关数据**：如果目标是微调特定领域的模型，需要确保数据包含足够的该领域的任务样本。
4. **领域模型Continue PreTrain 数据选取：** 在领域模型的继续预训练中，应选择：
    - **高质量的领域特定数据**：包括文献、论坛、技术文章、行业报告等。
    - **多样化的文本数据**：如结构化文本、对话记录、问答数据等，以提高模型的泛化能力。
    - **知识库和外部资源**：可以通过加入领域相关的知识图谱或外部文献来补充领域信息。
5. **缓解模型遗忘通用能力的方法：** 可以使用以下策略：
    - **增加正则化**：通过增加正则化项来防止模型在微调过程中遗忘通用知识。
    - **分阶段训练**：先进行通用能力的训练，然后再进行领域微调，并确保在微调时不过度调整某些层。
    - **多任务学习**：在训练时同时进行多个任务的微调，这样模型可以学习多个领域的知识，而不至于遗忘通用能力。
6. **领域模型Continue PreTrain时如何学习更多知识：**
    - **丰富的领域数据集**：继续预训练时，使用大量且多样化的领域数据可以帮助模型更全面地掌握领域知识。
    - **知识注入**：在预训练阶段，可以结合领域专家提供的标注数据或领域知识库，使模型能直接学习到更多的背景知识。
    - **无监督学习与自监督学习**：使用无监督学习或自监督学习的方法，训练模型通过上下文关系自动发现和学习新知识。
7. **进行SFT操作时基座模型选用Chat还是Base：** 通常建议使用**Base**模型进行SFT操作，因为Base模型未经过特定任务的微调，保留了更多的通用知识。Chat模型通常已经经过对话任务的微调，可能不适合用作SFT基础模型，尤其是在非对话任务的微调中。
8. **领域模型微调的指令和数据输入格式要求：**
    - **指令格式**：输入的指令应简洁明确，最好遵循一致的格式。例如：“请为下面的文本生成摘要”或“请回答以下问题：XXX”。
    - **数据输入格式**：每个输入样本应包含清晰的上下文和任务描述，确保输入格式对模型的指令理解不会产生歧义。对于文本生成任务，应包括文本和期望的输出格式；对于问答任务，应包括问题和答案。
    - **领域特定格式**：如果是领域相关的任务（如医学、法律等），应当调整数据的输入格式以包含相关的背景信息或术语。

### 9-16

以下是对你的问题的回答：

9. **领域模型微调与领域评测集构建**： 领域模型微调（fine-tuning）指的是在一个预训练的大型语言模型基础上，针对某个特定领域的数据进行进一步训练，使其更好地理解和生成该领域的知识。领域评测集构建是指根据目标领域的特性，收集并标注相关的测试数据，用于评估模型在该领域的表现。构建评测集时，需要确保数据的代表性和多样性，确保测试数据能涵盖领域内的不同场景。
10. **领域模型词表扩增的必要性**： 领域模型词表扩增在某些情况下是有必要的，尤其是当原始模型的词表不能覆盖某个领域的专业术语、缩写或新词时。扩增词表可以提高模型在该领域任务上的表现。然而，扩增词表可能增加训练难度，且如果没有足够的领域数据进行微调，效果可能有限。
11. **如何训练自己的大模型**： 训练自己的大模型需要大量的计算资源和数据。一般步骤如下：
    1. **数据收集**：收集相关领域的大量文本数据。
    2. **数据清理与预处理**：对数据进行去噪、去重、分词等处理。
    3. **选择预训练架构**：可以基于现有的大型预训练模型（如GPT、BERT）进行训练。
    4. **大规模训练**：使用高性能的计算资源进行训练，通常需要分布式训练环境。
    5. **微调**：针对具体任务进行微调。
    训练大模型时，你需要具备大规模的计算集群和足够的训练数据。
12. **训练中文大模型的经验**： 训练中文大模型时，需要特别关注以下几个方面：
    1. **数据的语言特性**：中文文本的分词需要特别注意，尤其是对于长文本的处理。
    2. **语料的多样性**：为了避免模型在某个领域过拟合，训练数据应该涵盖多种不同领域和风格的文本。
    3. **中文词表的设计**：中文词汇的处理和词表的设计可能需要特定的处理，避免过多拆分或无法表示的字符。
    4. **计算资源**：中文大模型的训练需要强大的计算资源，尤其是在处理大规模语料时。
13. **指令微调的好处**： 指令微调（Instruction Tuning）可以让模型更好地理解和执行指令，并根据用户的要求生成更符合预期的响应。这种微调方法通过强化学习（Reinforcement Learning）或其他策略训练模型，使其不仅仅生成自然语言文本，还能够对复杂任务进行有效执行。这对于构建适用于对话系统、客服、虚拟助手等应用的模型尤其重要。
14. **预训练与微调阶段注入知识**： 知识的注入一般发生在预训练阶段，但微调阶段也可以注入特定领域的知识。预训练阶段通常通过大量的语料来学习广泛的语言模式和通用知识，而微调阶段则是针对特定任务或领域，通过更多的标注数据使模型具备更高的任务相关能力。
15. **让模型学习某个领域或行业的知识，预训练还是微调？**： 让模型学习某个领域的知识，通常更有效的方式是**微调**。在预训练阶段，模型会学习到语言的基本规则和通用知识，但特定领域的深度知识一般需要通过微调在该领域的标注数据上进行强化。预训练提供的是一个基础，而微调则能使模型掌握行业的专业术语、知识和背景。
16. **多轮对话任务如何微调模型？**： 微调多轮对话模型通常包括以下步骤：
    1. **数据准备**：收集具有多轮对话的标注数据，确保数据包含不同的对话情境。
    2. **模型选择**：选择一个预训练的语言模型，如GPT或BERT系列。
    3. **数据处理**：将多轮对话格式化为模型输入所需要的形式，通常是对话轮次的前后文数据。
    4. **微调训练**：在多轮对话数据上进行训练，使模型能够理解上下文关系，生成合理的对话回应。
    5. **评估与调优**：使用评测集验证模型的对话生成能力，并进行必要的参数调整。

微调过程中，强化学习（RLHF）等方法也可以进一步提升模型的对话质量，尤其是在生成更自然、流畅的对话时。

如果你有其他具体问题或想要深入了解某个部分，随时告诉我！

### 17-24

以下是针对你的问题的解答：

17. **微调后的模型出现能力劣化，灾难性遗忘是怎么回事？** 灾难性遗忘（Catastrophic Forgetting）是指模型在微调过程中，由于过度优化针对新任务的性能，而遗忘了它在预训练阶段学习到的通用知识。通常，这是由于新任务的训练数据与原有任务的数据分布差异较大，导致模型在学习新任务时会丧失之前的能力。解决灾难性遗忘的方法包括：1) 使用**知识蒸馏**，2) **正则化**方法防止参数的过度调整，3) 使用**经验回放**方法来重用旧任务的知识。
18. **微调模型需要多大显存？** 微调模型的显存需求与模型的大小、输入数据的长度、批量大小（batch size）等因素密切相关。一般来说，**大模型（如GPT-3、GPT-4）在微调时**，需要显存在**16GB到80GB**之间，具体取决于模型的参数量。如果显存不足，可以通过以下方法优化：1) 降低批量大小，2) 使用梯度累积（gradient accumulation），3) 分布式训练。
19. **大模型LLM进行SFT操作时在学习什么？** SFT（Supervised Fine-Tuning）操作通常是在大模型上进行有监督微调，通过特定任务的数据进行训练。在SFT过程中，模型学习的是如何更好地**执行特定任务**，例如：文本分类、命名实体识别、翻译、摘要生成等。其核心目标是让模型在预训练的基础上，在目标任务上获得更好的表现。
20. **预训练和SFT操作有什么不同？** 预训练是模型学习通用语言模式和基础知识的阶段，通常是在大量无标签的数据上进行自监督学习。预训练的目的是让模型具备通用的语言理解和生成能力。 而**SFT（有监督微调）**是基于特定任务或领域进行的微调，通过标注数据让模型专注于优化某个具体任务或应用。SFT的目标是让模型能够在目标任务上表现得更好。
21. **样本量规模增大，训练出现OOM错误**： OOM（Out of Memory）错误通常发生在GPU显存不足的情况下。如果在训练过程中样本量增加，模型所需的内存也会增加，导致OOM错误。解决这个问题可以采取以下措施：
	1. **减少批量大小**（batch size）；
	2. **使用混合精度训练**（FP16）；
	3. **模型并行化**，即将模型分布到多个GPU上；
	4. **梯度累积**，即通过多次前向传播来累积梯度，减少显存压力。
22. **大模型LLM进行SFT时如何对样本进行优化？** 在进行SFT时，优化样本数据通常包括（SWAP）：
	1. **数据选择**（Select）：选取与任务最相关且高质量的数据。
	2. **样本权重**（Weighten）：对样本进行加权，帮助模型更多关注那些对任务有重要贡献的样本。
	3. **数据增强**（Augment）：通过数据增强（如同义词替换、文本重排序等）增加样本的多样性。
	4. **样本剪裁**（Prune）：在某些情况下，对那些不利于模型训练的噪声样本进行筛选或过滤。
23. **模型参数迭代实验**： 模型参数迭代实验是指通过调整模型的超参数（如学习率、批量大小、优化器选择等）来优化模型的表现。实验通常包括以下几个步骤：
	1. **选择超参数范围**：根据任务选择可能的超参数值范围。
	2. **训练与验证**：使用不同的超参数组合进行多次训练，并通过验证集来评估性能。
	3. **性能评估**：根据模型的表现（例如准确率、F1分数等）选择最佳的超参数组合。
	4. **迭代优化**：通过不断调整，找到最优的参数配置。
24. **微调大模型的一些建议**： 微调大模型时，有几个关键建议可以帮助提高效率和效果：
	1. **选择合适的预训练模型**：根据任务的需求选择一个与任务相似的预训练模型作为基础。
	2. **少量数据微调**：对于特定任务或小样本任务，可以采用少量数据微调（例如，通过迁移学习的方式）。
	3. **合适的学习率**：微调时使用较小的学习率，避免对已有的预训练权重进行过大的更新。
	4. **正则化**：使用Dropout、L2正则化等方法避免过拟合。
	5. **保存中间模型**：训练过程中定期保存模型，以防止训练不稳定或早期停止。
	6. **使用验证集**：通过验证集来调整超参数和监控模型的泛化能力。
    

微调大模型时，合理调整超参数、选择合适的数据以及采用高效的训练方法，能够在保持良好性能的同时避免过拟合。

## [大模型（LLMs）训练经验帖](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_06n25d9wjs0e.html)

- [分布式训练框架](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E6%A1%86%E6%9E%B6&zhida_source=entity)选择？
- LLMs 训练时 有哪些有用的建议？
- 模型大小如何选择？
- 加速卡如何选择？

在大模型训练时，以下是针对你提出的几个问题的详细建议：

1. **分布式训练框架如何选择？**

选择合适的分布式训练框架取决于多个因素，如硬件环境、可扩展性需求、易用性等。以下是几种常见的框架：
- **Horovod**：
    - 优势：基于MPI (Message Passing Interface) 的分布式训练框架，广泛应用于大规模深度学习训练。支持TensorFlow、Keras、PyTorch等框架。
    - 特点：高效的分布式梯度同步机制，能够在多个GPU甚至多个节点间实现并行训练。
- **DeepSpeed**：
    - 优势：微软开源的训练优化库，提供模型并行、数据并行、混合精度训练、梯度累积等功能，专为处理大型模型（如GPT、T5等）设计。
    - 特点：支持零冗余优化（ZeRO），显著提高了内存利用率，适合大规模模型的训练。
- **TensorFlow Distributed (TF-Distribution)**：
    - 优势：TensorFlow提供的原生分布式训练支持，简化了分布式训练的过程，适用于大规模训练任务。
    - 特点：使用`tf.distribute.Strategy` API，方便进行数据并行训练，并支持模型并行训练。
- **PyTorch Distributed**：
    - 优势：PyTorch原生支持分布式训练，结合`torch.nn.parallel.DistributedDataParallel`（DDP）可以有效进行数据并行训练。
    - 特点：易于使用和调试，广泛应用于学术研究和工业界，具有较强的灵活性和扩展性。

**选择建议**：如果你需要在多个节点上进行大规模分布式训练，Horovod和DeepSpeed是非常好的选择。如果你的项目主要依赖PyTorch或TensorFlow，则可以考虑使用PyTorch Distributed或TensorFlow Distributed。

2. **LLMs训练时的有用建议**
- **预训练与微调**：
    - 预训练大语言模型时，首先要有大量的文本数据（通常是无标签数据）。接着可以进行微调（fine-tuning），使模型适应特定任务或领域的需求。
- **模型架构选择**：
    - 选择适当的架构非常重要。Transformer架构是目前大多数LLM的基础，像GPT、BERT等都使用Transformer。
- **数据预处理**：
    - 高质量、充分多样化的数据集对LLM的训练至关重要。要保证数据的多样性和代表性，同时进行适当的文本清洗和去噪。
- **批量大小与梯度累积**：
    - 对于LLM训练，通常使用较大的批量大小，但受硬件限制，可能需要使用梯度累积技术来模拟更大的批量，从而提高训练效率。
- **混合精度训练**：
    - 使用混合精度训练（FP16 + FP32）可以显著加速训练过程，减少显存占用。
- **动态学习率调整**：
    - 使用如`lr_scheduler`等动态学习率策略来避免模型训练过程中出现过拟合或收敛困难的问题。

3. **模型大小如何选择？**

- **硬件限制**：你的硬件环境（如GPU显存、内存等）会直接影响可训练的模型大小。如果硬件资源较为紧张，可以选择使用模型压缩、剪枝、量化等技术来优化模型大小。
- **任务需求**：任务的复杂性和准确性要求也会影响模型大小。对于一些简单的任务，小模型（如DistilBERT等）可能已经足够，但对于复杂的生成任务，可能需要更大的模型。
- **训练数据量**：通常，大模型在大数据集上训练时能够展现更强的性能。如果数据集较小，使用过大的模型可能会出现过拟合，反而影响效果。
- **训练成本**：更大的模型需要更多的计算资源、存储空间和训练时间。需要综合考虑项目的预算和时间限制来选择合适的模型大小。

4. **加速卡如何选择？**
- **NVIDIA A100**：
    - 适合大规模训练，提供强大的性能和显存，支持大多数深度学习框架。
    - 特别适用于Transformer架构和大规模的自然语言处理任务。
- **NVIDIA H100**：
    - 基于最新的Hopper架构，适用于超大规模AI模型的训练，拥有更高的性能和内存带宽，适合下一代的大规模训练任务。
- **AMD MI250X**：
    - 如果你的框架支持AMD的硬件，可以考虑使用AMD的MI250X加速卡。AMD在深度学习领域逐渐崭露头角，尤其是在一些多卡训练环境中。
- **TPU（Tensor Processing Units）**：
    - 对于使用Google Cloud的用户，TPU是一个非常好的选择，尤其是在训练大型Transformer模型时，能够提供极高的性能。

**选择建议**：如果预算充足，NVIDIA A100或H100是当前大规模训练的最佳选择。对于云端计算，TPU也提供了优秀的性能，特别适合Google Cloud环境下的训练。

总体来说，选择适合的分布式训练框架、加速卡和模型大小需要综合考虑硬件资源、训练任务的复杂性、预算和时间要求等因素。希望这些建议对你有所帮助！

## 大模型（LLMs）langchain 面

### [大模型（LLMs）langchain 面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ve2dgaiqrjzv.html)

1. 什么是 LangChain?
2. LangChain 包含哪些 核心概念？
- 2.1 LangChain 中 Components and Chains 是什么？
- 2.2 LangChain 中 Prompt Templates and Values 是什么？
- 2.3 LangChain 中 Example Selectors 是什么？
- 2.4 LangChain 中 Output Parsers 是什么？
- 2.5 LangChain 中 Indexes and Retrievers 是什么？
- 2.6 LangChain 中 Chat Message History 是什么？
- 2.7 LangChain 中 Agents and Toolkits 是什么？
3. 什么是 LangChain Agent?
4. 如何使用 LangChain ?
5. LangChain 支持哪些功能?
6. 什么是 LangChain model?
7. LangChain 包含哪些特点?
8. LangChain 如何使用?
- 8.1 LangChain 如何调用 LLMs 生成回复？
- 8.2 LangChain 如何修改 提示模板？
- 8.3 LangChain 如何链接多个组件处理一个特定的下游任务？
- 8.4 LangChain 如何Embedding & vector store？
9. LangChain 存在哪些问题及方法方案？
- LangChain 低效的令牌使用问题
- LangChain 文档的问题
- LangChain 太多概念容易混淆，过多的“辅助”函数问题
- LangChain 行为不一致并且隐藏细节问题
- LangChain 缺乏标准的可互操作数据类型问题
10. LangChain 替代方案？

*Answer:*

**1. 什么是 LangChain?**

LangChain 是一个开源框架，旨在帮助开发者构建和部署基于语言模型（LLM）的应用程序。它提供了对不同语言模型、数据源、外部 API 以及各种链式操作的统一接口，使得开发者可以更加方便地创建复杂的语言处理工作流。LangChain 主要应用于 NLP 任务（如文本生成、搜索、问答等）和一些自动化任务（如 agent-driven workflows）。这个框架非常适合与大型语言模型（如 GPT）以及其他 AI 工具结合使用。

**2. LangChain 包含哪些核心概念?**

LangChain 的核心概念涉及到多个模块和组件，这些组件有助于开发者实现更灵活和功能丰富的语言模型应用。以下是 LangChain 中的一些核心概念：

2.1 **Components and Chains**
- **Components** 是构成 LangChain 应用程序的基本单元。它们可以是不同类型的组件，如 LLM（语言模型）、Prompts（提示）、转换器、索引器、Retriever 等。每个组件都执行一个特定的操作，并能在应用程序中彼此交互。
- **Chains** 是将多个组件组合在一起形成一个工作流。通过链式操作，LangChain 允许你将多个处理步骤串联，形成一个完整的处理过程。例如，一个典型的链可能会包括数据检索、文本生成、输出解析等多个步骤。
2.2 **Prompt Templates and Values**
- **Prompt Templates** 是用于构建动态生成提示（prompts）的工具。它们允许开发者使用占位符（placeholders）来表示输入变量，然后在实际运行时替换为特定的值。Prompt Templates 可以帮助开发者更方便地管理和生成动态提示，以便更好地引导语言模型生成所需的结果。
- **Prompt Values** 是实际传递给 Prompt Template 的具体数据或参数。它们可以是用户输入的数据、从数据库中获取的值，或者计算得到的结果。
2.3 **Example Selectors**
- **Example Selectors** 是一种用于选择示例数据的机制。在许多任务中，示例数据可以用来帮助模型理解问题的上下文并生成更准确的输出。LangChain 提供了 Example Selectors，用于从预定义的示例集合中选择合适的示例来供语言模型使用。
2.4 **Output Parsers**
- **Output Parsers** 是用来解析语言模型输出的工具。它们可以对语言模型的原始输出进行后处理，转化为更适合应用的格式。例如，如果语言模型生成的是一段文本，而应用程序需要结构化的数据，Output Parsers 可以将文本转换为表格、JSON 或其他数据格式。
2.5 **Indexes and Retrievers**
- **Indexes** 是用来存储和管理大量信息的结构。在 LangChain 中，索引可以是文本数据、文档或其他形式的数据。索引通常结合检索算法（retrieval algorithms）使用，允许快速查找和获取相关信息。
- **Retrievers** 是用于从索引中检索信息的工具。它们能够根据用户的查询或特定的搜索条件，从大型数据集或文档库中找到相关的内容。
2.6 **Chat Message History**
- **Chat Message History** 是一种机制，用于跟踪和存储与用户之间的对话历史。在基于聊天的应用中，记录消息历史对于维护上下文和生成连贯的对话至关重要。LangChain 提供了管理和操作消息历史的工具，以便开发者创建更智能、具有上下文理解能力的聊天系统。
2.7 **Agents and Toolkits**
- **Agents** 是一种基于语言模型的自适应系统，它们能够理解任务、获取信息并执行动作。Agents 通常由多个工具和组件构成，可以根据任务需求进行决策。它们能够调用其他工具、检索信息、生成输出等。
- **Toolkits** 是一组与 Agents 结合使用的工具，可以提供给 Agents 执行任务时使用的外部资源。Toolkits 可以包括诸如搜索引擎、API、数据库查询等工具，使 Agents 能够执行复杂的任务。

这些核心概念的组合，使得 LangChain 成为一个功能强大且灵活的框架，适合构建各种基于语言模型的应用程序和自动化工作流。

3. **什么是 LangChain Agent?**

LangChain Agent 是 LangChain 中的一个重要概念，它允许构建智能的代理程序，这些代理能够根据输入动态地选择、调用不同的工具或执行不同的操作来解决问题。LangChain Agent 基于一系列预定义的动作，它能够在应用程序中自动选择最合适的工具（如外部API、数据库查询、计算等）进行任务处理。这些代理可以用来执行复杂的任务、处理多步骤推理、与其他系统交互等。

4. 如何使用 LangChain?

- **安装 LangChain**：首先，你需要安装 LangChain 库，通常使用 pip 安装：
    `pip install langchain`
- **设置和配置模型**：你可以通过 LangChain 轻松地集成不同的语言模型，例如 OpenAI 的 GPT、Google PaLM、Hugging Face 模型等。
- **构建管道或代理**：你可以根据任务需求，使用 LangChain 中的工具构建处理链。例如，创建一个链式处理流程，将用户输入传递给不同的工具来完成多步推理。
- **调用并执行任务**：构建好链或代理后，可以调用并执行任务。

5. **LangChain 支持哪些功能?**

LangChain 提供了多种功能来支持语言模型驱动的应用程序，包括：

1. **LLM（语言模型）支持**：支持与多种语言模型的集成，如 OpenAI、Hugging Face、Google等。
2. **链（Chains）**：链是 LangChain 中的核心概念，用于将多个组件（如语言模型、数据源、工具等）串联起来，形成一个工作流。支持单步骤和多步骤链。
3. **代理（Agents）**：允许动态选择工具，执行多步骤推理，或与外部系统交互。
4. **记忆（Memory）**：LangChain 支持在对话中保持记忆，以便跟踪上下文和用户状态。
5. **工具（Tools）**：LangChain 支持与各种外部工具和 API 集成，例如数据库、搜索引擎、Web API、文件处理等。
6. **Prompt 模板**：可以通过模板化的方式构建和管理复杂的提示（prompt），以提高任务的准确性和一致性。
7. **自定义管道**：可以构建和定制数据流管道，灵活处理不同的业务需求。


6. **什么是 LangChain model?**

LangChain 模型是指与 LangChain 框架集成的语言模型，它们用于处理自然语言任务。这些模型可以是任何已知的预训练语言模型，如 OpenAI GPT 系列、Hugging Face 的模型、Anthropic 的 Claude 等。LangChain 提供了一个统一的接口，允许开发者轻松地配置和操作这些模型。

7. LangChain 包含哪些特点?

LangChain 的一些显著特点包括：
1. **模块化和扩展性**：LangChain 的架构是高度模块化的，支持扩展、定制不同的组件，如工具、代理、链、提示模板等。
2. **易于集成外部工具**：支持与多种外部系统的集成，包括数据库、Web API、文件存储、搜索引擎等。
3. **支持多步推理**：通过链和代理，LangChain 可以实现复杂的多步骤推理任务，并在每个步骤中动态选择操作。
4. **记忆和上下文管理**：LangChain 支持记忆功能，允许系统在多个交互中保持上下文，以便实现更加连贯的对话和任务处理。
5. **开箱即用**：提供了多种开箱即用的模板和功能，帮助开发者快速启动项目。
6. **灵活的提示设计**：提供强大的提示模板支持，使得处理自然语言输入和输出变得更加方便和灵活。

8、9、10：WIP

### WIP：基于LLM+向量库的文档对话 经验面

- 一、基于LLM+向量库的文档对话 基础面

- 1.1 为什么 大模型 需要 外挂(向量)知识库？
- 1.2. 基于LLM+向量库的文档对话 思路是怎么样？
- 1.3. 基于LLM+向量库的文档对话 核心技术是什么？
- 1.4. 基于LLM+向量库的文档对话 prompt 模板 如何构建？

- 二、基于LLM+向量库的文档对话 存在哪些痛点？
- 三、基于LLM+向量库的文档对话 工程示例面

### WIP：LLM文档对话 —— pdf解析关键问题

- 一、为什么需要进行pdf解析？
- 二、为什么需要 对 pdf 进行解析？
- 三、pdf解析 有哪些方法，对应的区别是什么？
- 四、pdf解析 存在哪些问题？
- 五、如何 长文档（书籍）中关键信息？
- 六、为什么要提取标题甚至是多级标题？
- 七、如何提取 文章标题？
- 八、如何区分单栏还是双栏pdf？如何重新排序？
- 九、如何提取表格和图片中的数据？
- 十、基于AI的文档解析有什么优缺点？

### WIP：基于LLM+向量库的文档对话 经验面

- 一、基于LLM+向量库的文档对话 基础面

- 1.1 为什么 大模型 需要 外挂(向量)知识库？
- 1.2. 基于LLM+向量库的文档对话 思路是怎么样？
- 1.3. 基于LLM+向量库的文档对话 核心技术是什么？
- 1.4. 基于LLM+向量库的文档对话 prompt 模板 如何构建？

- 二、基于LLM+向量库的文档对话 存在哪些痛点？
- 三、基于LLM+向量库的文档对话 工程示例面
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_m9t1w8pokjpf.html)

## [大模型（LLMs）参数高效微调(PEFT) 面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ahk2br3igwx9.html)

### [大模型（LLMs）参数高效微调(PEFT) 面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ipkod91a939n.html)

- 微调方法是啥？如何微调？
- 为什么需要 PEFT？
- 介绍一下 PEFT？
- PEFT 有什么优点？
- 微调方法批处理大小模式GPU显存速度？
- Peft 和 [全量微调](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E5%85%A8%E9%87%8F%E5%BE%AE%E8%B0%83&zhida_source=entity)区别？
- 多种不同的高效微调方法对比
- 当前[高效微调技术](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF&zhida_source=entity)存在的一些问题
- 高效微调技术[最佳实践](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5&zhida_source=entity)
- PEFT 存在问题？
- 能不能总结一下各种参数高效微调方法？

### [配器微调（Adapter-tuning）篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_h5q2fzq8wvt8.html)

- 一、为什么 需要 [适配器微调](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E9%80%82%E9%85%8D%E5%99%A8%E5%BE%AE%E8%B0%83&zhida_source=entity)（Adapter-tuning）？
- 二、适配器微调（Adapter-tuning）思路？
- 三、 适配器微调（Adapter-tuning）特点是什么？
- 四、AdapterFusion 思路 是什么？
- 五、AdapterDrop 思路 是什么？
- 六、AdapterDrop 特点 是什么？
- 七、MAM Adapter 思路 是什么？
- 八、MAM Adapter 特点 是什么？

### [提示学习（Prompting）](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_662wpbw47gtj.html)

- 一、为什么需要 提示学习（Prompting）？
- 二、什么是 提示学习（Prompting）？
- 三、提示学习（Prompting） 有什么优点？
- 四、提示学习（Prompting）有哪些方法，能不能稍微介绍一下它们间？

- 4.1 [前缀微调](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E5%89%8D%E7%BC%80%E5%BE%AE%E8%B0%83&zhida_source=entity)（Prefix-tining）篇

- 4.1.1 为什么需要 前缀微调（Prefix-tining）？
- 4.1.2 前缀微调（Prefix-tining）思路是什么？
- 4.1.3 前缀微调（[Prefix-tining](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=4&q=Prefix-tining&zhida_source=entity)）的优点是什么？
- 4.1.4 前缀微调（Prefix-tining）的缺点是什么？

- 4.2 指示微调（Prompt-tuning）篇

- 4.2.1 为什么需要 指示微调（Prompt-tuning）？
- 4.2.2 指示微调（[Prompt-tuning](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=3&q=Prompt-tuning&zhida_source=entity)）思路是什么？
- 4.2.3 指示微调（Prompt-tuning）优点是什么？
- 4.2.4 指示微调（Prompt-tuning）缺点是什么？
- 4.2.5 指示微调（Prompt-tuning）与 [Prefix-tuning](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=Prefix-tuning&zhida_source=entity) 区别 是什么？
- 4.2.6 指示微调（Prompt-tuning）与 [fine-tuning](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=fine-tuning&zhida_source=entity) 区别 是什么？

- 4.3 P-tuning 篇

- 4.3.1 为什么需要 [P-tuning](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=2&q=P-tuning&zhida_source=entity)？
- 4.3.2 P-tuning 思路是什么？
- 4.3.3 P-tuning 优点是什么？
- 4.3.4 P-tuning 缺点是什么？

- 4.4 P-tuning v2 篇

- 4.4.1 为什么需要 P-tuning v2？
- 4.4.2 P-tuning v2 思路是什么？
- 4.4.3 P-tuning v2 优点是什么？
- 4.4.4 P-tuning v2 缺点是什么？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_662wpbw47gtj.html)

### [LoRA 系列篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ham28l44907e.html)

- 一、LoRA篇

- 1.1 什么是 LoRA？
- 1.2 LoRA 的思路是什么？
- 1.3 LoRA 的特点是什么？

- 二、QLoRA篇

- 2.1 QLoRA 的思路是怎么样的？
- 2.2 QLoRA 的特点是什么？

- 三、AdaLoRA篇

- 3.1 AdaLoRA 的思路是怎么样的？

- 四、LoRA权重是否可以合入原模型？
- 五、ChatGLM-6B LoRA后的权重多大？
- 六、LoRA 微调优点是什么？
- 七、LoRA微调方法为啥能加速训练？
- 八、如何在已有LoRA模型上继续训练？
- 九、LoRA 缺点是什么？
- 十、LoRA这种微调方法和全参数比起来有什么劣势吗？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ham28l44907e.html)

## [大模型（LLMs）推理面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_64vc5vvwpobv.html)

### [大模型（LLMs）推理面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_topiwiusclkr.html)

1. 为什么大模型推理时显存涨的那么多还一直占着？
2. 大模型在gpu和cpu上推理速度如何？
3. 推理速度上，int8和fp16比起来怎么样？
4. 大模型有推理能力吗？
5. 大模型生成时的参数怎么设置？
6. 有哪些省内存的大语言模型训练/微调/推理方法？

- 6.1 如何 估算模型所需的RAM？
- 6.2 Fp16-mixed precision
- 6.3 Int8-bitsandbytes
- 6.4 LoRA
- 6.5 Gradient Checkpointing
- 6.6 Torch FSDP+CPU offload

1. 如何让大模型输出合规化
2. 应用模式变更

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_topiwiusclkr.html)

## 大模型（LLMs）预训练面

### [大模型（LLMs）增量预训练篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_lj47ancwcmv2.html)

1. 为什么要增量预训练？
2. 进行 增量预训练 需要做哪些准备工作？
3. 增量预训练 所用 训练框架？
4. 增量预训练 训练流程 是怎么样？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_lj47ancwcmv2.html)

## [大模型（LLMs）评测面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_j9wcj62eovgc.html)

1. 大模型怎么评测？
2. 大模型的honest原则是如何实现的？模型如何判断回答的知识是训练过的已知的知识，怎么训练这种能力？
3. 如何衡量大模型水平？
4. 大模型评估方法 有哪些？
5. 大模型评估工具 有哪些？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_j9wcj62eovgc.html)

## [大模型（LLMs）强化学习面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_zqs7mjw6c8k7.html)

1. 简单介绍强化学习？
2. 简单介绍一下 RLHF？
3. 奖励模型需要和基础模型一致吗？
4. RLHF 在实践过程中存在哪些不足？
5. 如何解决 人工产生的偏好数据集成本较高，很难量产问题？
6. 如何解决三个阶段的训练（SFT->RM->PPO）过程较长，更新迭代较慢问题
7. 如何解决 PPO 的训练过程同时存在4个模型（2训练，2推理），对计算资源的要求较高 问题？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_zqs7mjw6c8k7.html)

## [大模型（LLMs）软硬件配置面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_m5q8zk3wo84k.html)

1. 建议的软件环境是什么？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_m5q8zk3wo84k.html)

## [大模型（LLMs）训练集面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_jwvpaujrojtt.html)

1. SFT（有监督微调）的数据集格式？
2. RM（奖励模型）的数据格式？
3. PPO（强化学习）的数据格式？
4. 找数据集哪里找？
5. 微调需要多少条数据？
6. 有哪些大模型的训练集？
7. 进行领域大模型预训练应用哪些[数据集](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=4&q=%E6%95%B0%E6%8D%AE%E9%9B%86&zhida_source=entity)比较好？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_jwvpaujrojtt.html)

## [大模型（LLMs）显存问题面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_jhiocx89p3su.html)

1. 大模型大概有多大，模型文件有多大?
2. 能否用4 * v100 32G训练vicuna 65b？
3. 如果就是想要试试65b模型，但是显存不多怎么办？
4. nB模型推理需要多少显存？
5. nB模型训练需要多少显存？
6. 如何 估算模型所需的RAM？
7. 如何评估你的显卡利用率?
8. 测试你的显卡利用率 实现细节篇

1. 如何查看多机训练时的网速？
2. 如何查看服务器上的多卡之间的NVLINK topo？
3. 如何查看服务器上显卡的具体型号?
4. 如何查看训练时的flops？（也就是每秒的计算量）
5. 如何查看对deepspeed的环境配置是否正确？
6. tf32格式有多长？
7. 哪里看各类显卡算力比较？
8. （torch profiler）如何查看自己的训练中通信开销？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_jhiocx89p3su.html)

## 大模型（LLMs）分布式训练面

### [大模型（LLMs）分布式训练面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ah2ibj3z22c7.html)

1. 理论篇

- 1.1 训练 大语言模型 存在问题？
- 1.2 什么是 [点对点通信](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E7%82%B9%E5%AF%B9%E7%82%B9%E9%80%9A%E4%BF%A1&zhida_source=entity)？
- 1.3 什么是 集体通信？
- 1.4 什么是 [数据并行](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C&zhida_source=entity)？
- 1.5 数据并行 如何 提升效率？
- 1.6 什么是 流水线并行？
- 1.7 什么是 张量并行 (intra-layer)？
- 1.8 数据并行 vs 张量并行 vs 流水线并行?
- 1.9 什么是 3D并行？
- 1.10 想要训练1个LLM，如果只想用1张显卡，那么对显卡的要求是什么？
- 1.11 如果有N张显存足够大的显卡，怎么加速训练？
- 1.12 如果显卡的显存不够装下一个完整的模型呢？
- 1.13 PP推理时，是一个串行的过程，1个GPU计算，其他空闲，有没有其他方式？
- 1.14 3种[并行方式](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E5%B9%B6%E8%A1%8C%E6%96%B9%E5%BC%8F&zhida_source=entity)可以叠加吗？
- 1.15 Colossal-AI 有1D/2D/2.5D/3D，是什么情况？
- 1.16 除了3D并行有没有其他方式大规模训练？
- 1.17 有了ZeRO系列，为什么还需要3D并行？
- 1.18 平民适不适合玩3D并行？
- 1.19 平民适不适合直接上多机多卡的ZeRO3（万兆网）？
- 1.20 分布式并行及显存[优化技术](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF&zhida_source=entity)并行技术有哪一些，都有什么特点？
- 1.21 显存优化技术有哪一些，都有什么特点？
- 1.22 常见的分布式训练框架哪一些，都有什么特点？

1. 实践篇

- 2.1 假如有超多的8卡A100节点（DGX A100），如何应用3D并行策略？
- 2.2 如果想构这样一个大规模并行训练系统，训练框架如何选？
- 2.3 训练框架如何选？

1. 并行化策略选择篇

- 3.1 如何选择一款分布式训练框架？
- 3.2 如何选择一款分布式训练框架？
- 3.3 单GPU
- 3.4 单节点多卡
- 3.5 多节点多卡

1. 问题篇

- 4.1 推理速度验证
- 4.2 并行化训练加速
- 4.3 deepspeed 训练过程，报找不主机
- 4.4 为什么 多机训练效率不如单机？
- 4.5 多机训练不通，DeepSPeed配置问题
  
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ah2ibj3z22c7.html)

### [图解分布式训练（一） —— 流水线并行（Pipeline Parallelism）面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ah2ibj3z22c7.html)

- 为什么需要流水线并行（Pipeline Parallelism）？
- 一、流水线并行（Pipeline Parallelism） 优化目标是什么？
- 二、图解 流水线并行（Pipeline Parallelism）模型并行 必要性？
- 三、流水线并行（Pipeline Parallelism） 图解？
- 四、流水线并行（Pipeline Parallelism）优缺点？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_wre1eni0oq7d.html)

### [图解分布式训练（二） —— nn.DataParallel面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ah2ibj3z22c7.html)

- 为什么需要nn.DataParallel？
- 一、pytorch中的GPU操作默认是什么样？
- 二、介绍一下 nn.DataParallel 函数？
- 三、nn.DataParallel 函数 处理逻辑 介绍一下？
- 四、nn.DataParallel 函数 常见问题及解答 有哪些？

- 4.1 多GPU计算减少了程序运行的时间？
- 4.2 如何保存和加载多GPU训练模型呢？
- 4.3 为什么第一块卡的显存会占用的更多一些？
- 4.4 直接使用nn.DataParallel的时候，训练采用多卡训练，会出现一个warning？
- 4.5 device_ids 0 被占用问题

- 五、nn.DataParallel 函数 参数更新方式 ？
- 六、nn.DataParallel 函数 优点 介绍一下？
- 七、nn.DataParallel 函数 缺点 介绍一下？
- 八、nn.DataParallel 函数 实战？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_wre1eni0oq7d.html)

### [图解分布式训练（三） —— nn.parallel.DistributedDataParallel](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_i4s3ia057rmh.html)

- 为什么需要 nn.parallel.DistributedDataParallel ？
- 一、什么是 DistributedDataParallel 核心 —— Ring-AllReduce？
- 二、nn.parallel.DistributedDataParallel 函数 介绍一下？
- 三、nn.parallel.DistributedDataParallel 函数 如何多卡加速训练？
- 四、nn.parallel.DistributedDataParallel 实现流程介绍一下？
- 五、nn.parallel.DistributedDataParallel 参数更新介绍一下？
- 六、nn.DataParallel(以下简称DP) vs DistributedDataParallel(以下简称DDP)介绍一下？
- 七、DistributedDataParallel(以下简称DDP) 优点有哪些？
- 八、DistributedDataParallel(以下简称DDP) 缺点有哪些？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_i4s3ia057rmh.html)

### [图解分布式训练（四） —— torch.multiprocessing 详细解析](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_gu9smpbn510e.html)

- 一、torch.multiprocessing 函数介绍一下？
- 二、torch.multiprocessing 函数如何使用？
- 三、介绍一下 共享CUDA张量？
- 四、介绍一下 共享策略？
- 五、torch.multiprocessing 函数使用
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_gu9smpbn510e.html)

### [图解分布式训练（五） —— AMP混合精度训练 详细解析](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_0slrgoti6gvb.html)

- 为什么需要 AMP混合精度训练？
- 一、什么是自动混合精度训练(AMP)
- 二、为什么需要自动混合精度？
- 三、混合精度训练的优点是什么？
- 四、混合精度训练的缺点是什么？
- 五、混合精度训练的关键技术是什么？
- 六、介绍一下 混合精度训练 动态损失缩放？
- 七、如何在PyTorch中使用自动混合精度？
- 八、如何使用 AMP混合精度训练 ？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_0slrgoti6gvb.html)

### [图解分布式训练（六） —— Pytorch的 DeepSpeed 详细解析](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_2v6wv29ce8nn.html)

- 一、为什么需要 Deepspeed？
- 二、DeepSpeed 基本概念 介绍一下？
- 三、DeepSpeed 通信策略 介绍一下？
- 四、DeepSpeed 如何使用？
- 五、DeepSpeed 代码实现？
- 七、训练精度 介绍一下？
- 八、获取模型参数 介绍一下？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_2v6wv29ce8nn.html)

### [图解分布式训练（七）—— accelerate 分布式训练 详细解析](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_o5wkeionnqr7.html)

- 一、为什么需要 accelerate 分布式训练？
- 二、什么是 accelerate 分布式训练?
- 三、accelerate 分布式训练 原理讲解？
- 四、accelerate 分布式训练 如何实践？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_o5wkeionnqr7.html)

### [图解分布式训练（八）—— ZeRO 学习](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_600z63vou4nj.html)

- 一、什么是 3D 并行？
- 二、3D 并行 策略有哪些？
- 三、为什么需要 ZeRO？
- 四、ZeRO 的 核心思想是什么？
- 五、ZeRO 显存如何分配？
- 六、ZeRO 优化策略是怎么样？
- 七、ZeRO Offload后的计算流程是怎么样？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_600z63vou4nj.html)

## [大模型（LLMs）agent 面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_9dfwi0ooio2z.html)

1. 如何给LLM注入领域知识？
2. 如果想要快速体验各种模型，该怎么办？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_mzfogrjhkp17.html)

## [Token及模型参数准备篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_9oplu4014qx5.html)

1. 预训练数据 Token 重复 是否影响 模型性能？
2. SFT需要训练Token数？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_9oplu4014qx5.html)

## [LLMs 位置编码篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_bmn80nar12c7.html)

- 1 什么是位置编码？
- 2 什么是绝对位置编码？
- 3 什么是相对位置编码？
- 4 旋转位置编码 RoPE篇

- 4.1 旋转位置编码 RoPE 思路是什么？
- 4.2 推导一下 旋转位置编码 RoPE ？
- 4.3 旋转位置编码 RoPE 有什么优点？
- 4.4 旋转位置编码 RoPE 被哪些 LLMs 应用？

- 5 长度外推问题篇

- 5.1 什么是 长度外推问题？
- 5.2 长度外推问题 的 解决方法 有哪些？

- 6 ALiBi (Attention with Linear Biases)篇

- 6.1 ALiBi (Attention with Linear Biases) 思路是什么？
- 6.2 ALiBi (Attention with Linear Biases) 的偏置矩阵是什么？有什么作用？
- 6.3 ALiBi (Attention with Linear Biases) 有什么优点？
- 6.4 ALiBi (Attention with Linear Biases) 被哪些 LLMs 应用？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_bmn80nar12c7.html)

## LLMs Tokenizer 篇

### [LLMs Tokenizer 篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_c1wrizv0im1a.html)

- Byte-Pair Encoding(BPE)篇

- 1 Byte-Pair Encoding(BPE) 如何构建词典？

- WordPiece 篇

- 1 WordPiece 与 BPE 异同点是什么？

- SentencePiece 篇

- 简单介绍一下 SentencePiece 思路？

- 对比篇

- 1 举例 介绍一下 不同 大模型LLMs 的分词方式？
- 2 介绍一下 不同 大模型LLMs 的分词方式 的区别？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_c1wrizv0im1a.html)

### [怎么让英文大语言模型支持中文？（一） —— 构建中文tokenization](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_w0d2q29sueq7.html)

- 一、为什么需要 构建中文tokenization？
- 二、如何对 原始数据预处理？
- 三、如何构建中文的词库？
- 四、如何使用transformers库加载sentencepiece模型？
- 五、如何合并英文词表和中文词表？
- 六、怎么使用修改后的词表？
- 总结一下 构建中文tokenization？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_w0d2q29sueq7.html)

### [怎么让英文大语言模型支持中文？（二） —— 继续预训练篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_jprkwhrvf3tw.html)

- 一、为什么需要进行继续预训练？
- 二、如何对 继续预训练 数据预处理？
- 三、如何 构建模型？
- 四、如何 使用模型？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_jprkwhrvf3tw.html)

### [怎么让英文大语言模型支持中文？（三） —— 对预训练模型进行指令微调](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_p2wj7zadwxwb.html)

- 一、为什么需要对预训练模型进行指令微调？
- 二、对预训练模型进行指令微调 数据 如何处理？
- 三、对预训练模型进行指令微调 tokenization 如何构建？
- 四、对预训练模型进行指令微调 模型 如何构建？
- 五、是否可以结合 其他库 使用？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_p2wj7zadwxwb.html)

## [Layer normalization 篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_pzcgd4ovk098.html)

- Layer normalization-方法篇

- Layer Norm 篇

- Layer Norm 的计算公式写一下？

- RMS Norm 篇 （[均方根](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E5%9D%87%E6%96%B9%E6%A0%B9&zhida_source=entity) Norm）

- RMS Norm 的计算公式写一下？
- RMS Norm 相比于 Layer Norm 有什么特点？

- Deep Norm 篇

- Deep Norm 思路？
- 写一下 Deep Norm 代码实现？

- Deep Norm 有什么优点？

- Layer normalization-位置篇

- 1 LN 在 LLMs 中的不同位置 有什么区别么？如果有，能介绍一下区别么？

- Layer normalization 对比篇

- LLMs 各模型分别用了 哪种 Layer normalization？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_pzcgd4ovk098.html)

## [LLMs 激活函数篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_6xm3wzzice2s.html)

- 1 介绍一下 FFN 块 计算公式？
- 2 介绍一下 GeLU 计算公式？
- 3 介绍一下 Swish 计算公式？
- 4 介绍一下 使用 GLU 线性门控单元的 FFN 块 计算公式？
- 5 介绍一下 使用 GeLU 的 GLU 块 计算公式？
- 6 介绍一下 使用 Swish 的 GLU 块 计算公式？
- 各LLMs 都使用哪种激活函数？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_6xm3wzzice2s.html)

## [LLMs 激活函数篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_6xm3wzzice2s.html)

- 1 介绍一下 FFN 块 计算公式？
- 2 介绍一下 GeLU 计算公式？
- 3 介绍一下 Swish 计算公式？
- 4 介绍一下 使用 GLU 线性门控单元的 FFN 块 计算公式？
- 5 介绍一下 使用 GeLU 的 GLU 块 计算公式？
- 6 介绍一下 使用 Swish 的 GLU 块 计算公式？
- 各LLMs 都使用哪种激活函数？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_6xm3wzzice2s.html)

## 大模型（LLMs）加速篇

### [大模型（LLMs）加速篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_w9wewc152eux.html)

1. 当前优化模型最主要技术手段有哪些？
2. 推理加速框架有哪一些？都有什么特点？

- 3 vLLM 篇

- 3.1 vLLM 的 功能有哪些？
- 3.2 vLLM 的 优点有哪些？
- 3.3 vLLM 的 缺点有哪些？
- 3.4 vLLM 离线批量推理？
- 3.5 vLLM API Server？

- 4 Text generation inference 篇

- 4.1 介绍一下 Text generation inference？
- 4.2 Text generation inference 的 功能有哪些？
- 4.3 Text generation inference 的 优点有哪些？
- 4.4 Text generation inference 的 缺点有哪些？
- 4.5 Text generation inference 的 使用docker运行web server？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_w9wewc152eux.html)

### [LLM（大语言模型）部署加速方法——PagedAttention篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_p22mjq881n3n.html)

- 一、vLLM 用于大模型并行推理加速 存在什么问题？
- 二、vLLM 如何 优化 大模型并行推理加速？
- 三、什么是 PagedAttention？
- 四、 PagedAttention 如何存储 连续的key和value？
- 五、 PagedAttention 技术细节？
- 六、 PagedAttention 如何 实现安全共享？
- 七、 PagedAttention 源码介绍？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_p22mjq881n3n.html)

### [大模型推理加速工具 —— vLLM](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_zw5h9ogvac2w.html)

- 一、引言

- 1.1 前言
- 1.2 为什么 需要 vLLM ?
- 1.3 vLLM 具有哪些特点 ?
- 1.4 vLLM 支持哪些 Huggingface 模型 ?

- 二、vLLM 性能如何？
- 三、vLLM 依赖包
- 四、vLLM 如何安装？
- 五、vLLM 如何使用？
- 六、vLLM 分布式推理与服务
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_zw5h9ogvac2w.html)

### [LLM（大语言模型）部署加速方法——Faster Transformer篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_dd2gowztxtfg.html)

- 一、为什么需要 FasterTransformer？
- 二、FasterTransformer 介绍一下？
- 三、FasterTransformer 核心是什么？
- 四、FasterTransformer 优化？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_dd2gowztxtfg.html)

### [纯Python超轻量高性能LLM推理框架 —— LightLLM](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_9a643feq2b0b.html)

- 一、引言

- 1.1 前言
- 1.2 为什么 需要 LightLLM ?
- 1.3 目前 LLM推理框架 有 哪些?

- 二、LightLLM 介绍一下？

- 2.1 什么是 LightLLM ？
- 2.2 Token Attention 介绍？
- 2.3 Efficient Router 介绍？

- 三、LightLLM 性能表现 介绍？
- 四、LightLLM 依赖包 有哪些？
- 五、LightLLM 如何安装？

- 5.1 下载 LightLLM
- 5.2 安装 LightLLM 依赖
- 5.3 安装 LightLLM

- 六、LightLLM 如何使用？

- 6.1 启动 LightLLM 服务

- 填坑笔记

- LightLLM 支持模型 LLMs 模型？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_9a643feq2b0b.html)

## [Attention 升级面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_j0nwuo0frw2x.html)

- 1 传统 Attention 存在哪些问题？
- 2 Attention 优化方向
- 3 Attention 变体有哪些？
- 4 Multi-Query Attention 篇

- 4.1 Multi-head Attention 存在什么问题？
- 4.2 介绍一下 Multi-Query Attention？
- 4.3 对比一下 Multi-head Attention 和 Multi-Query Attention？
- 4.4 Multi-Query Attention 这样做的好处是什么？
- 4.5 有 哪些模型 是 使用 Multi-Query Attention？

- 5 Grouped-query Attention

- 5.1 什么是 Grouped-query Attention？
- 5.2 有哪些大模型使用 Grouped-query Attention？

- 6 FlashAttention 介绍一下
- 7 并行 transformer block 介绍一下？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_j0nwuo0frw2x.html)

## 大模型幻觉（LLM Hallucination）面

### [大模型幻觉（LLM Hallucination）面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_schwrdmvmhr7.html)

- 一、什么是大模型幻觉？
- 二、为什么LLM会产生幻觉？
- 三、为什么需要解决LLM的幻觉问题？
- 四、幻觉一定是有害的吗？
- 五、幻觉有哪些不同类型？
- 六、如何度量幻觉？
- 七、如何缓解LLM幻觉？

- 7.1 通过使用外部知识验证主动检测和减轻幻觉
- 7.2 事实核心采样
- 7.3 SelfCheckGPT

- 八、LLMs什么时候最容易产生幻觉？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_schwrdmvmhr7.html)

### [大模型的幻觉问题篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_8mr4mlhe5q1x.html)

- 一、什么是 大模型幻觉问题？
- 二、为什么 会 出现 大模型幻觉问题？
- 三、如何 评估 大模型幻觉问题？
- 四、如何 缓解 大模型幻觉问题？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_8mr4mlhe5q1x.html)

### [大模型的幻觉问题篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_tbezgzifowzp.html)

- 一、为什么 会 出现 大模型幻觉？
- 二、如何 缓解 大模型幻觉？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_tbezgzifowzp.html)

## LLMs 对比篇

### [LLMs 对比篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_tbezgzifowzp.html)

- LLMs 训练数据 和 数据量 对比如何？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_tbezgzifowzp.html)

### [百川智能baichuan7B、13B、53B、baichuan2 总结篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ma6pw7v2g9pi.html)

- 一、baichuan-7B篇

1. 你了解baichuan-7B解构么？介绍一下？
2. baichuan-7B 如何 收集原始数据并 构建 训练数据？
3. baichuan-7B 如何 提高 训练稳定性和吞吐？

- 二、baichuan-13B篇

1. 相比于 baichuan-7B，baichuan-13B 的 特点体现在哪里？
2. 如何 对 baichuan-13B 进行推理和部署？
3. 如何 对 baichuan-13B 进行微调？

- 三、baichuan-53B篇

- 3.1 baichuan-53B 相比于 baichuan-7B 和 baichuan-13B 有哪些优势？
- 3.2 baichuan-53B 如何对 预训练数据 做处理？
- 3.3 baichuan-53B 如何进行 搜索增强？

- 四、baichuan2篇

- 4.1 baichuan2 与 其他大模型 对比

- 五、baichuan 数据构建篇

- 5.1 baichuan 进行微调时，领域数据：通用数据配比？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ma6pw7v2g9pi.html)

## 思维链 Chain-of-Thought（COT）篇

### [思维链 Chain-of-Thought（COT）篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_1cjjxf95az70.html)

- 一、什么是思维链提示？
- 二、思维链提示本质是什么？
- 三、思维链提示 与 标准的提示学习方法有什么不同?
- 四、思维链提示 为什么可以提高语言模型的复杂推理能力?它的优势在哪里?
- 五、思维链提示 适用场景 有 哪些？
- 六、思维链提示 目前还存在哪些不足点？
- 七、思维链提示 对推动语言模型复杂推理能力研究有哪些启发和影响?
- 八、思维链提示 对实现真正的通用人工智能仍面临哪些挑战?
- 九、如何通过增加模型规模来获得语言模型强大的思路链推理能力的?这与模型获得的哪些能力有关?
- 十、你认为可以在哪些其他方面应用“思路链提示”这一思路来提升语言模型的能力?
- 十一、如果需要你对 思维链提示 进行改进，你觉得你会改进哪些地方？
- 十二、思维链提示 未来研究方向？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_1cjjxf95az70.html)

### [思维链 Chain-of-Thought（COT）变体篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_sw5aljfzswiv.html)

- 思维链 Chain-of-Thought（COT）：思维链的启蒙

1. 什么是 思维链 Chain-of-Thought（COT）？
2. 思维链 Chain-of-Thought（COT）是思路是什么？
3. 思维链 Chain-of-Thought（COT）存在问题？

- 思维树 Tree of Thoughts（TOT）：一种用树结构解决复杂问题的方法

1. 为什么需要 思维树 Tree of Thoughts（TOT）？
2. 什么是 思维树 Tree of Thoughts（TOT）？
3. 思维树 Tree of Thoughts（TOT）涉及问题有哪些？

- 思维图 Graph of Thoughts（GOT）：一种把思维链过程建模层图结构的方法

1. 为什么 需要 思维图 Graph of Thoughts（GOT）？
2. 什么是 思维图 Graph of Thoughts（GOT） ？
3. 思维图 Graph of Thoughts（GOT）核心思想是什么 ？

- 思维算法 Algorithm of Thoughts（AOT）：一种用DFS/BFS示例解决问题的方法

1. 为什么 需要 思维算法 Algorithm of Thoughts（AOT）？
2. 思维算法 Algorithm of Thoughts（AOT）思路是什么？
3. 思维算法 Algorithm of Thoughts（AOT） vs 其他 COT 的 区别？

- 思维链 Chain-of-Thought（COT） 有哪些 应用场景？
- 思维链 Chain-of-Thought（COT） 有哪些 局限性？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_sw5aljfzswiv.html)

## [思维链 Chain-of-Thought（COT）变体篇](https://link.zhihu.com/?target=https%3A//file%2B.vscode-resource.vscode-cdn.net/f%3A/llms_interview_notes/llms_interview_notes_gitee/hhttps%3A/articles.zsxq.com/id_dwhonmw976n7.html)

- 一、为什么需要 Graph RAG？
- 二、什么是 Graph RAG？
- 三、Graph RAG 思路介绍？
- 四、用代码 介绍 Graph RAG ？
- 五、用 示例 介绍 Graph RAG ？
- 六、Graph RAG 排序优化方式？

## 豆包面经-1

src: [link](https://blog.csdn.net/Python_cocola/article/details/141143099)

1. *介绍和大模型相关项目*

~~我不是，我没有，别骂了~~

WIP

2. *介绍rope和位置外推*

RoPE（Rotary Position Embedding）简介

RoPE（Rotary Position Embedding）是一种用于Transformer模型的**位置编码策略**。它通过旋转操作将相对位置信息集成到self-attention机制中，从而提升Transformer架构的性能。RoPE广泛应用于大型模型，如LLaMA和ChatGLM等。RoPE的一个显著特点是其外推性，即在训练时和预测时输入长度不一致的情况下，RoPE能够较好地维持模型的泛化能力。

位置外推简介

位置外推是指在不需要对模型进行额外训练的情况下，使模型能够处理更长的序列。这种技术对于大模型尤其重要，因为它们通常在较小的上下文长度中进行训练，而在推理时可能需要处理更长的文本。位置外推技术包括基于RoPE的位置插值、NTK-aware、动态NTK等方法。这些方法通过不同的缩放策略来优化模型的外推能力。

RoPE的位置外推能力通过调整旋转角度来实现，这种方法允许模型在预训练长度之外取得更好的效果。

3. *sft阶段有什么重要的技术*

在大模型的指令微调阶段，有几种重要技术：
1. **指令微调（Instruction Tuning）**：这是一种针对大型预训练语言模型的微调技术，其核心目的是增强模型理解和执行特定指令的能力，使模型能够根据用户提供的自然语言指令准确执行任务。
2. **混合微调与压缩技术（Hybrid Fine-Tuning and Compression）**：未来的趋势是将微调和压缩技术相结合，在微调的同时进行模型压缩，例如结合微调与蒸馏或剪枝。
3. **强化微调（Reinforcement Fine-Tuning）**：与传统的微调相比，强化微调可以让开发者使用经过微调的模型进行更复杂的推理和任务执行。
4. **参数高效微调（PEFT, Parameter-Efficient Fine Tuning）**：这种方法只对部分的参数进行训练，主要有Prompt Tuning、Prefix Tuning、LoRA、QLoRA等方法，旨在降低计算和存储成本。
5. **高效指令微调技术**：为了降低计算成本，研究者们提出了多种高效微调方法，如LoRA、QLoRA、LOMO等，可以在较小的计算资源下实现指令微调。
6. **多阶段指令数据微调**：这种策略首先使用大规模NLP任务指令数据对模型进行微调，然后再使用相对多样的日常对话指令和合成指令进一步微调，以避免能力遗忘问题。

4. *为什么需要RLHF，能解决什么SFT解决不了的问题*

**RLHF**（Reinforcement Learning from Human Feedback）是大模型训练中的一种重要技术，特别是在生成式模型（如GPT类模型）的训练过程中，它可以解决**SFT**（Supervised Fine-Tuning）无法解决的一些问题。两者的核心区别在于训练目标和反馈来源：

**SFT的局限性：**
- **数据依赖：** SFT依赖于人工标注的数据集，这些数据集通常是静态的、有限的且可能无法覆盖所有可能的用户需求和场景。SFT的优化目标通常是基于传统的监督学习方法，通过已标注的训练样本调整模型参数。
- **不可捕捉长尾需求：** 由于人工标注的样本有限，SFT训练的模型可能无法应对很多未知场景或用户个性化需求，尤其是一些长尾问题和复杂的语境。

**RLHF的优势：**
- **动态反馈：** RLHF通过强化学习和人类反馈相结合的方式，可以动态调整模型的行为，而不仅仅依赖于静态的标注数据。这使得模型可以通过互动学习逐步改善其输出，尤其是在面对用户的实时反馈时。
- **优化行为而非仅仅是预测：** 在RLHF中，模型不仅仅是根据输入生成输出，而是学习如何优化行为，以最大化用户的满意度或任务的成功率。通过这种方法，模型的目标可以更加灵活和人性化，而不仅仅是预测“正确的答案”。
- **处理复杂多样的任务：** RLHF可以处理更多复杂的、含糊不清的任务，因为它允许模型通过逐步的尝试和错误来学习，而不需要预先定义所有可能的情况。这种方法尤其适合于那些没有明确标准答案、而是需要与用户进行互动和优化体验的场景。
- **反馈的个性化：** RLHF可以根据用户反馈不断调整模型，逐步适应特定用户或用户群体的需求，而SFT则难以做到这种层面的个性化优化。

**RLHF能解决SFT无法解决的问题：**
- **任务自适应：** RLHF能通过人类反馈不断调整模型，使其能够应对更多动态、复杂或开放性的问题，而SFT只能处理有限的、预先定义好的问题。
- **优化策略：** RLHF不仅仅依赖于标注数据来进行误差反向传播，而是通过奖励机制来指导模型行为。这个机制可以使得模型在训练过程中主动探索并发现更符合需求的策略。
- **长尾问题：** SFT面临的一个问题是，它通常依赖于标注数据的覆盖度，不能很好地应对长尾需求。而RLHF能够通过实际的用户反馈不断弥补这一不足，尤其在面对复杂、个性化的任务时，RLHF能更好地收集反馈并逐步改进模型。

5. PPO和DPO区别

**PPO**（Proximal Policy Optimization）和**DPO**（Direct Preference Optimization）都是强化学习中的算法，它们在模型优化的方式和目标上有所不同，特别是在与**RLHF**（Reinforcement Learning from Human Feedback）结合使用时。下面是它们的主要区别：

**基本概念与背景：**
- **PPO（Proximal Policy Optimization）：** PPO是一种强化学习算法，属于策略优化方法。它的目标是通过在每次更新时限制策略的变动幅度，来避免策略变化过大导致训练不稳定。PPO通过引入一个“剪切”目标（clipped objective）来限制新旧策略之间的差距，确保在优化过程中不会做出过于激进的调整。
- **DPO（Direct Preference Optimization）：** DPO是针对人类反馈优化的一种方法，专门设计用于优化模型生成的输出，使其更符合用户的偏好。DPO直接优化用户反馈或偏好，而不是通过奖励信号或策略值函数来优化。DPO的一个重要特点是它不依赖于强化学习中的奖励函数，而是直接利用来自用户的比较偏好（即哪种输出更符合用户需求）来调整模型。

**优化方式：**
- **PPO：** PPO通过强化学习的策略梯度方法来优化策略。它通过最大化某个目标函数，进而改进策略，这个目标函数包括了奖励信号和概率比的剪切约束。PPO在每次优化时会对策略进行一定程度的限制，确保每次更新不偏离原始策略太远，从而保持训练的稳定性。
- **DPO：** DPO直接优化人类偏好，通常不依赖于传统的奖励函数，而是通过用户的反馈来优化模型输出。例如，在对话生成任务中，DPO会根据用户对不同生成文本的偏好来进行优化。DPO方法利用的是人类对模型行为的直接偏好（例如，哪个回答更合适或更有帮助），而不是间接的奖励信号。

**适用场景：**
- **PPO：** PPO广泛应用于强化学习任务，尤其是在需要直接优化行为策略的场景中。它适用于各种任务，从机器人控制到游戏AI等领域。在RLHF中，PPO通常用于模型通过与环境或用户互动来学习和优化其行为。
- **DPO：** DPO特别适用于**人类反馈优化**的场景。例如，在自然语言处理任务（如对话系统）中，DPO通过优化用户偏好来调整模型的输出，使其更加符合用户的需求。DPO也非常适合用于那些没有明确奖励信号的任务，而是依赖于用户偏好的情境。

**优化目标：**
- **PPO：** PPO的优化目标是最大化期望奖励函数，通常是根据策略在环境中的表现来评估奖励。PPO在更新过程中会通过剪切目标函数来限制每次策略变化的幅度，以避免过度调整。
- **DPO：** DPO的优化目标是最大化用户的偏好反馈。它直接基于用户的反馈来调整模型行为，使得模型生成的输出更加符合用户的期望，而不依赖于传统的奖励信号。

**模型更新与稳定性：**
- **PPO：** PPO引入了“重要性采样”技术和策略“剪切”，使得在优化过程中策略变化不会过于剧烈。这一方法强调训练的稳定性，因此适用于需要多次迭代和较大策略更新的任务。
- **DPO：** DPO通常会直接基于用户偏好进行优化，因此它的更新方式与PPO有所不同。DPO的核心是利用用户比较偏好信息来直接引导优化过程，通常不会依赖于策略的精细调整，而是根据每次用户反馈来更新模型。

**计算复杂度：**
- **PPO：** PPO在训练过程中涉及到样本的采集和更新策略的多个步骤，因此计算复杂度相对较高。它需要进行多轮的采样和梯度更新，尤其在复杂的环境下，这种计算复杂度可能会很大。
- **DPO：** DPO通常不需要像PPO那样进行大量的环境交互，而是基于用户提供的偏好信息来进行优化。因此，DPO在计算上可能更加高效，尤其是在没有环境交互时，只需要处理偏好数据。

总结：
- **PPO**更偏向于传统的强化学习方法，依赖于环境反馈和奖励信号来优化策略，通常用于优化行为策略。
- **DPO**则是一个直接优化模型输出以符合用户偏好的方法，特别适用于没有明确奖励信号而需要根据人类反馈调整模型行为的任务。

6. 介绍一下对你做的大模型领域的看法

??

7. 介绍你的研究领域的评估方法，有没有什么问题

??

8. 说几个你最近觉得有意思的论文

??

### 