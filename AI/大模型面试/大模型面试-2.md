
[src](https://zhuanlan.zhihu.com/p/659042194)

## LLM基础面

1. 目前 主流的开源模型体系 有哪些？
2. prefix Decoder 和 causal Decoder 和 Encoder-Decoder 区别是什么？
3. 大模型LLM的 训练目标 是什么？
4. 涌现能力是啥原因？
5. 为何现在的大模型大部分是Decoder only结构？
6. 简单 介绍一下 大模型【LLMs】？
7. 大模型【LLMs】后面跟的 175B、60B、540B等 指什么？
8. 大模型【LLMs】具有什么优点？
9. 大模型【LLMs】具有什么缺点？

A:

- **目前主流的开源模型体系有哪些？**
    - **Transformers**（如BERT、GPT系列、T5等）：这些是基于自注意力机制（Self-Attention）的模型，广泛用于NLP任务。
    - **BERT系列**（如RoBERTa, DistilBERT等）：用于文本理解的双向编码器模型。
    - **GPT系列**：基于解码器结构，主要用于生成任务，最近的版本包括GPT-3、GPT-4等。
    - **T5**：Encoder-Decoder架构，擅长多任务学习。
    - **Stable Diffusion**：用于图像生成的扩散模型。
    - **CLIP**：连接图像和文本的多模态模型。
    - **LLaMA系列**：Meta的开源大语言模型，注重在多个不同任务上的表现。
    - **OpenLLaMA**：由社区开发的基于Meta LLaMA架构的开源项目。
- **Prefix Decoder 和 Causal Decoder 和 Encoder-Decoder 区别是什么？**
    - **Prefix Decoder**：在生成时通过输入一个“前缀”来引导解码过程。这种结构通常在少量输入的情况下，借助已知的信息来生成输出。常用于条件生成任务。
    - **Causal Decoder**：这种解码器采用自回归的方式（前一步的输出作为下一步的输入），通常用于生成任务，如GPT模型。其特点是只能考虑到前文信息，具有因果性。
    - **Encoder-Decoder**：这种结构包含两个部分——编码器将输入信息转换为表示向量，解码器则根据这些向量生成输出。这是机器翻译、文本摘要等任务中的经典结构，如T5和BART。
- **大模型LLM的训练目标是什么？**
    - 大模型的训练目标通常是最大化语言建模的对数似然函数，即通过训练使模型能够预测下一个词（或Token）。目标是让模型学习到足够的语义和语法信息，以便在多种自然语言处理任务上具备良好的表现。具体目标可以包括：
        - **自回归语言建模**：生成下一个词。
        - **掩蔽语言建模（Masked Language Modeling, MLM）**：预测被掩蔽的词（如BERT所用）。
        - **多任务学习**：如T5和BART采用了统一框架处理多种任务。
- **涌现能力是啥原因？**
    - **涌现能力**（Emergent Abilities）指的是大规模模型在训练过程中出现的、未显式设计的复杂行为或能力。例如，大模型可能在少量的训练数据或少数几次示例上展现出超出其设计目标的表现。其原因包括：
        - 模型规模的增加导致其对数据的拟合能力和泛化能力增强。
        - 大量的训练数据和训练参数使得模型能学习到更复杂的模式。
        - 自注意力机制等架构使得模型能够更好地理解长距离依赖关系。
- **为何现在的大模型大部分是Decoder only结构？**
    - **Decoder-only模型**（如GPT系列）通常用于生成任务，这类任务的特点是自回归生成文本（每次生成一个词并将其作为输入）。这种结构的优点包括：
        - 生成能力强，适用于长文本的连贯生成。
        - 训练过程中，只需要考虑生成每个词的条件概率，不需要像Encoder-Decoder那样涉及到复杂的编码过程。
        - 更高效：在解码阶段，生成每个词的过程中都基于已经生成的部分，避免了编码-解码间的复杂数据传递。
- **简单介绍一下大模型【LLMs】？**
    - **大模型（LLMs）**是指拥有大量参数的语言模型，通常具有数十亿到数百亿的参数。这些模型通过海量的文本数据进行训练，具备较强的理解、生成能力，可以用于各类NLP任务，如文本生成、翻译、问答、摘要等。
    - 典型的LLMs包括GPT-3、GPT-4、T5、BERT等，它们的特点是使用了Transformer架构，采用自注意力机制，能够捕捉长距离依赖和复杂的语言模式。
- **大模型【LLMs】后面跟的175B、60B、540B等指什么？**
    - **175B、60B、540B等**是大模型中参数的数量，指的是模型中可训练参数的总数。例如，GPT-3有1750亿个参数，GPT-4的参数数量可能更高。参数数量是衡量模型规模的重要指标之一，通常随着参数数量的增加，模型的性能在许多任务上得到提升。
- **大模型【LLMs】具有什么优点？**
    - **强大的性能**：由于参数数量巨大，LLMs在多个NLP任务上通常表现出色，如文本生成、问答、翻译等。
    - **迁移学习能力**：经过预训练的大模型能够通过微调适应多种任务，减少了传统机器学习中的手动特征工程。
    - **处理复杂语言模式**：能够捕捉语法、语义和上下文关系，处理复杂的语言任务。
- **大模型【LLMs】具有什么缺点？**
    - **计算资源要求高**：训练和推理都需要大量的计算资源和内存，对硬件要求很高，成本昂贵。
    - **缺乏透明度**：大模型的内部工作往往不透明，很难解释其做出的决策，这对高风险应用（如医疗、法律等）是一个挑战。
    - **数据偏见问题**：模型可能会学习到训练数据中的偏见，导致不公正或不合适的结果。
    - **能耗高**：大规模模型的训练和使用消耗大量能源，对环境有影响。
    - **难以调优**：调优这样庞大的模型需要专业知识和时间，且在小规模数据集上可能表现欠佳。

## LLM进阶面

1. LLMs 复读机问题
	1. 什么是 LLMs 复读机问题？
	2. 为什么会出现 LLMs 复读机问题？
	3. 如何缓解 LLMs 复读机问题？
2. llama 系列问题
	1. llama 输入句子长度理论上可以无限长吗？
3. 什么情况用Bert模型，什么情况用LLaMA、ChatGLM类大模型，咋选？
4. 各个专业领域是否需要各自的大模型来服务？
5. 如何让大模型处理更长的文本？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_v6gltxd4qbxd.html)

A:

1. *什么是 LLMs 复读机问题？*

LLMs（大型语言模型）复读机问题是指，当使用语言模型（如 GPT）进行对话或生成文本时，模型可能会出现过度重复或不断复述相同内容的情况。这个问题表现为模型反复输出类似或完全相同的句子、段落或想法，缺乏足够的多样性和创新性。这种问题通常影响对话的流畅性和互动性，可能使用户体验下降。

2. *为什么会出现 LLMs 复读机问题？*

复读机问题的出现有几个原因：
1. **训练数据问题**：模型的训练数据中可能存在大量重复的信息或者一些特定模式，模型可能在生成时倾向于回到这些熟悉的结构或表述上。
2. **生成策略问题**：LLMs在生成文本时，采用的采样方法（如贪心解码、温度调节等）可能使模型偏向生成相似的输出。例如，低温度的采样策略可能导致模型输出确定性更强的、重复性较高的文本。
3. **缺乏上下文理解**：在某些情况下，LLMs可能没有深入理解对话的上下文，导致它们反复使用相同的短语或句式来回应问题。
4. **有限的模型容量**：尽管LLMs具有强大的语言生成能力，但它们仍然是基于统计学的模型，可能在处理长时间的对话或复杂任务时失去多样性，偏向生成已知的简单响应。

3. *如何缓解 LLMs 复读机问题？*

可以通过以下几种方法缓解复读机问题：
1. **调整生成参数**：
    - **增加温度**：提高温度值（例如，从 0.7 提高到 1.0 或更高）可以让模型的输出更加多样化，避免过于确定的选择，减少重复性。
    - **使用Top-k或Top-p采样**：这些策略通过限制模型只能从概率最高的前k个词或满足一定累计概率阈值的词汇中进行选择，减少了模型输出重复内容的概率。
2. **改进训练数据和方法**：
    - **增强训练数据的多样性**：通过加入多样化的、不同来源和风格的文本数据，减少训练数据中重复内容的比例。
    - **强化学习（RLHF）**：通过强化学习和人类反馈，模型可以更好地学习如何避免无聊或重复的内容生成，增强其生成的创新性。
3. **引入上下文管理**：
    - **更精细的上下文建模**：通过提高模型理解对话上下文的能力，使其能够生成更多符合场景需求的多样化回答。
    - **对话状态跟踪**：在多轮对话中，系统可以跟踪当前对话状态，避免模型在不同对话轮次中输出相同或过度重复的内容。
4. **后处理去重**：
    - **去除重复生成内容**：在模型输出的文本生成后，进行后处理，比如去除连续的重复句子，或选择较为创新和多样化的生成结果。
5. **任务目标引导**：
    - **明确生成目标**：通过明确任务目标或问题的具体需求，引导模型输出更多相关且富有变化的答案，避免过于常规的回复。

通过这些方法，LLMs可以在提高输出质量的同时，减少重复内容的生成，使得其回答更加有趣和多样化。

4. *llama 输入句子长度理论上可以无限长吗？*

理论上，Llama（以及其他基于变压器架构的语言模型）的输入句子长度是有限制的，而不是无限长。这个限制通常由模型的架构和计算资源决定。
1. **模型架构的限制**：大多数现代变压器模型（包括Llama）通常有一个固定的最大输入长度（例如，512个token、1024个token等）。这个长度是由模型的设计和训练过程所决定的，具体取决于输入的词嵌入（embedding）的维度和模型内部的计算需求。Llama系列模型的具体最大长度可能是2048个token或更长，但它不是无限的。
2. **计算资源的限制**：变压器模型的计算复杂度通常是平方级别的，即输入长度增加时，计算和内存需求会显著增加。因此，即便模型本身能够理论上处理更长的序列，硬件资源（如内存和处理能力）也会限制输入的长度。
3. **技术演进**：随着技术进步，研究人员也在尝试解决更长序列的处理问题。例如，通过使用稀疏变压器或其他优化技术（如长序列注意力机制、记忆网络等），可以有效地扩展处理更长序列的能力。

所以，尽管Llama模型在理论上有扩展的可能，但实际应用中它的输入长度是有限的，通常在几千个token左右。

5. *什么时候用BERT模型，什么时候用LLaMA、ChatGLM类大模型，如何选择？*

- BERT模型：
	- 用途：BERT（Bidirectional Encoder Representations from Transformers）是一种基于Transformer的预训练语言模型，特别适用于处理上下文相关的语言理解任务，例如文本分类、命名实体识别（NER）、问答、情感分析等。
	- 选择场景：当任务主要是**文本理解**，并且需要从上下文中捕获深层次的语义关系时，BERT是一个不错的选择。其模型结构优化了双向注意力机制，适用于短文本或需要精确文本表示的场景。
- LLaMA、ChatGLM类大模型：
	- 用途：这些大模型（例如LLaMA、ChatGLM、GPT系列）通常是大规模预训练的生成模型，不仅擅长理解任务，还可以生成自然语言文本。因此，它们适用于对话系统、文本生成、代码生成等任务，甚至可以进行零-shot和少-shot学习。
	- 选择场景：如果任务涉及到更为复杂的交互，或需要大模型进行**文本生成、推理、对话等**更高层次的任务，LLaMA、ChatGLM等大模型更合适。尤其是在对话系统或开放式问题解答中，这类模型的能力会更加突出。

6. *各个专业领域是否需要各自的大模型来服务？*

是的，不同领域的任务可能需要专门针对该领域的模型。随着大模型技术的发展，很多领域都有了专门优化的大模型来提高特定任务的性能。

领域大模型的需求：
- 专业领域模型：例如医学、法律、金融等领域，大型预训练模型通常需要根据该领域的数据进行微调。这是因为这些领域的术语、知识结构和语言特点通常和通用语料库存在较大差异，通用大模型可能无法很好地处理。
- 跨领域模型：虽然也有一些跨领域的通用大模型（如GPT、ChatGLM等），但它们的效果通常不如针对特定领域进行训练的模型。

比如，医学领域的任务通常需要专业的医疗知识，例如疾病诊断、病历总结等，因此需要专门的医学预训练模型，如BioBERT、ClinicalBERT等，来提高准确性。

7. *如何让大模型处理更长的文本？*（段滑长层）

大模型处理长文本的挑战主要是由于上下文窗口大小的限制。当前的Transformer架构（如GPT-3、BERT等）通常限制了单次输入的最大长度（例如，512个token、2048个token等）。为了解决这个问题，可以采取以下几种方法：
- 分段处理：将长文本拆分成多个片段，每个片段独立处理，然后将它们的输出合并。这种方法虽然简单，但可能丢失一些长距离的上下文信息。
- Sliding Window（滑动窗口）：通过在长文本上使用滑动窗口机制，逐步处理文本的不同部分，每次处理时将部分上下文信息保留。这种方法可以减少信息丢失，尤其在处理较长的对话或文档时比较有效。
- 长序列模型：一些新型的Transformer变种如Longformer、Reformer、Linformer等，专门优化了长序列的处理能力，能在保留较大上下文的情况下有效地处理更长的文本。它们采用了稀疏注意力机制或其他技术，来减少计算复杂度。
- 模型的分层处理：采用分层架构，先通过较小的上下文窗口对文本进行初步编码，再将多个局部表示聚合起来形成全局表示。这种方法通过逐步缩小范围并融合信息，能够处理更长的文本。
- Memory Augmented Networks：一些方法（如Recurrent Memory Networks、Transformer-XL）引入外部记忆模块，使得模型能够记住长时间段内的信息并跨步骤传递。

通过这些方式，可以让大模型在处理长文本时更有效，尽可能减少上下文丢失。

## LLM微调面

1. 如果想要在某个模型基础上做[全参数微调](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E5%85%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83&zhida_source=entity)，究竟需要多少显存？
2. 为什么SFT之后感觉LLM傻了?
3. SFT [指令微调数据](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83%E6%95%B0%E6%8D%AE&zhida_source=entity) 如何构建?
4. 领域模型Continue PreTrain 数据选取？
5. 领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力？
6. 领域模型Continue PreTrain ，如何 让模型在预训练过程中就学习到更多的知识？
7. 进行SFT操作的时候，[基座模型](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E5%9F%BA%E5%BA%A7%E6%A8%A1%E5%9E%8B&zhida_source=entity)选用Chat还是Base?
8. [领域模型微调](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E9%A2%86%E5%9F%9F%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83&zhida_source=entity) 指令&数据输入格式 要求？
9. 领域模型微调 领域评测集 构建？
10. 领域模型词表扩增是不是有必要的？
11. 如何训练自己的大模型？
12. 训练中文大模型有啥经验？
13. 指令微调的好处？
14. 预训练和微调哪个阶段注入知识的？
15. 想让模型学习某个领域或行业的知识，是应该预训练还是应该微调？
16. [多轮对话](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D&zhida_source=entity)任务如何微调模型？
17. 微调后的模型出现能力劣化，灾难性遗忘是怎么回事？
18. 微调模型需要多大显存？
19. 大模型LLM进行SFT操作的时候在学习什么？
20. 预训练和SFT操作有什么不同
21. 样本量规模增大，训练出现OOM错
22. 大模型LLM进行SFT 如何对样本进行优化？
23. 模型参数迭代实验
24. 微调大模型的一些建议

### 1-8

这些问题涉及了深度学习中的几个高级概念，尤其是大语言模型（LLM）的微调和预训练。以下是逐一回答：

1. **全参数微调显存需求：** 全参数微调的显存需求通常与模型的大小、训练数据批次大小（batch size）、以及每次训练中使用的梯度累积步数等因素相关。例如，GPT-3（175B参数）模型的显存需求可能需要上百GB的显存。一般来说，显存需求随模型规模的增大而线性增加，但如果你采用混合精度训练（如FP16），显存需求可能会有所减少。如果是BERT或GPT等基于Transformer的模型，通常在显存可用的情况下可以微调。
2. **SFT之后LLM傻了的原因：** 指令微调（SFT）过程可能使得模型过度拟合到某些特定任务或指令格式，导致其对其他类型的问题或开放式任务的表现变差。常见原因包括：
    - **任务特定化**：模型在某个特定的指令或任务上进行过度训练，丧失了原本的多样性和泛化能力。
    - **数据偏差**：如果SFT的数据没有覆盖广泛的场景或问题类型，可能导致模型产生偏见，限制了其开放问题的处理能力。
    - **过拟合问题**：如果微调时使用了不合适的超参数（如过高的学习率、过少的数据等），可能导致模型只学习到特定的模式，而无法应对其他问题。
3. **SFT指令微调数据如何构建：** SFT数据集的构建需要包含：
    - **任务明确的指令**：例如让模型在特定情境下执行某项任务（例如问题回答、摘要生成、翻译等）。
    - **多样化的输入输出样例**：包括不同的任务情境和输入格式，以帮助模型在不同场景下执行指令。
    - **领域相关数据**：如果目标是微调特定领域的模型，需要确保数据包含足够的该领域的任务样本。
4. **领域模型Continue PreTrain 数据选取：** 在领域模型的继续预训练中，应选择：
    - **高质量的领域特定数据**：包括文献、论坛、技术文章、行业报告等。
    - **多样化的文本数据**：如结构化文本、对话记录、问答数据等，以提高模型的泛化能力。
    - **知识库和外部资源**：可以通过加入领域相关的知识图谱或外部文献来补充领域信息。
5. **缓解模型遗忘通用能力的方法：** 可以使用以下策略：
    - **增加正则化**：通过增加正则化项来防止模型在微调过程中遗忘通用知识。
    - **分阶段训练**：先进行通用能力的训练，然后再进行领域微调，并确保在微调时不过度调整某些层。
    - **多任务学习**：在训练时同时进行多个任务的微调，这样模型可以学习多个领域的知识，而不至于遗忘通用能力。
6. **领域模型Continue PreTrain时如何学习更多知识：**
    - **丰富的领域数据集**：继续预训练时，使用大量且多样化的领域数据可以帮助模型更全面地掌握领域知识。
    - **知识注入**：在预训练阶段，可以结合领域专家提供的标注数据或领域知识库，使模型能直接学习到更多的背景知识。
    - **无监督学习与自监督学习**：使用无监督学习或自监督学习的方法，训练模型通过上下文关系自动发现和学习新知识。
7. **进行SFT操作时基座模型选用Chat还是Base：** 通常建议使用**Base**模型进行SFT操作，因为Base模型未经过特定任务的微调，保留了更多的通用知识。Chat模型通常已经经过对话任务的微调，可能不适合用作SFT基础模型，尤其是在非对话任务的微调中。
8. **领域模型微调的指令和数据输入格式要求：**
    - **指令格式**：输入的指令应简洁明确，最好遵循一致的格式。例如：“请为下面的文本生成摘要”或“请回答以下问题：XXX”。
    - **数据输入格式**：每个输入样本应包含清晰的上下文和任务描述，确保输入格式对模型的指令理解不会产生歧义。对于文本生成任务，应包括文本和期望的输出格式；对于问答任务，应包括问题和答案。
    - **领域特定格式**：如果是领域相关的任务（如医学、法律等），应当调整数据的输入格式以包含相关的背景信息或术语。

### 9-16

以下是对你的问题的回答：

9. **领域模型微调与领域评测集构建**： 领域模型微调（fine-tuning）指的是在一个预训练的大型语言模型基础上，针对某个特定领域的数据进行进一步训练，使其更好地理解和生成该领域的知识。领域评测集构建是指根据目标领域的特性，收集并标注相关的测试数据，用于评估模型在该领域的表现。构建评测集时，需要确保数据的代表性和多样性，确保测试数据能涵盖领域内的不同场景。
10. **领域模型词表扩增的必要性**： 领域模型词表扩增在某些情况下是有必要的，尤其是当原始模型的词表不能覆盖某个领域的专业术语、缩写或新词时。扩增词表可以提高模型在该领域任务上的表现。然而，扩增词表可能增加训练难度，且如果没有足够的领域数据进行微调，效果可能有限。
11. **如何训练自己的大模型**： 训练自己的大模型需要大量的计算资源和数据。一般步骤如下：
    1. **数据收集**：收集相关领域的大量文本数据。
    2. **数据清理与预处理**：对数据进行去噪、去重、分词等处理。
    3. **选择预训练架构**：可以基于现有的大型预训练模型（如GPT、BERT）进行训练。
    4. **大规模训练**：使用高性能的计算资源进行训练，通常需要分布式训练环境。
    5. **微调**：针对具体任务进行微调。
    训练大模型时，你需要具备大规模的计算集群和足够的训练数据。
12. **训练中文大模型的经验**： 训练中文大模型时，需要特别关注以下几个方面：
    1. **数据的语言特性**：中文文本的分词需要特别注意，尤其是对于长文本的处理。
    2. **语料的多样性**：为了避免模型在某个领域过拟合，训练数据应该涵盖多种不同领域和风格的文本。
    3. **中文词表的设计**：中文词汇的处理和词表的设计可能需要特定的处理，避免过多拆分或无法表示的字符。
    4. **计算资源**：中文大模型的训练需要强大的计算资源，尤其是在处理大规模语料时。
13. **指令微调的好处**： 指令微调（Instruction Tuning）可以让模型更好地理解和执行指令，并根据用户的要求生成更符合预期的响应。这种微调方法通过强化学习（Reinforcement Learning）或其他策略训练模型，使其不仅仅生成自然语言文本，还能够对复杂任务进行有效执行。这对于构建适用于对话系统、客服、虚拟助手等应用的模型尤其重要。
14. **预训练与微调阶段注入知识**： 知识的注入一般发生在预训练阶段，但微调阶段也可以注入特定领域的知识。预训练阶段通常通过大量的语料来学习广泛的语言模式和通用知识，而微调阶段则是针对特定任务或领域，通过更多的标注数据使模型具备更高的任务相关能力。
15. **让模型学习某个领域或行业的知识，预训练还是微调？**： 让模型学习某个领域的知识，通常更有效的方式是**微调**。在预训练阶段，模型会学习到语言的基本规则和通用知识，但特定领域的深度知识一般需要通过微调在该领域的标注数据上进行强化。预训练提供的是一个基础，而微调则能使模型掌握行业的专业术语、知识和背景。
16. **多轮对话任务如何微调模型？**： 微调多轮对话模型通常包括以下步骤：
    1. **数据准备**：收集具有多轮对话的标注数据，确保数据包含不同的对话情境。
    2. **模型选择**：选择一个预训练的语言模型，如GPT或BERT系列。
    3. **数据处理**：将多轮对话格式化为模型输入所需要的形式，通常是对话轮次的前后文数据。
    4. **微调训练**：在多轮对话数据上进行训练，使模型能够理解上下文关系，生成合理的对话回应。
    5. **评估与调优**：使用评测集验证模型的对话生成能力，并进行必要的参数调整。

微调过程中，强化学习（RLHF）等方法也可以进一步提升模型的对话质量，尤其是在生成更自然、流畅的对话时。

如果你有其他具体问题或想要深入了解某个部分，随时告诉我！

### 17-24

以下是针对你的问题的解答：

17. **微调后的模型出现能力劣化，灾难性遗忘是怎么回事？** 灾难性遗忘（Catastrophic Forgetting）是指模型在微调过程中，由于过度优化针对新任务的性能，而遗忘了它在预训练阶段学习到的通用知识。通常，这是由于新任务的训练数据与原有任务的数据分布差异较大，导致模型在学习新任务时会丧失之前的能力。解决灾难性遗忘的方法包括：1) 使用**知识蒸馏**，2) **正则化**方法防止参数的过度调整，3) 使用**经验回放**方法来重用旧任务的知识。
18. **微调模型需要多大显存？** 微调模型的显存需求与模型的大小、输入数据的长度、批量大小（batch size）等因素密切相关。一般来说，**大模型（如GPT-3、GPT-4）在微调时**，需要显存在**16GB到80GB**之间，具体取决于模型的参数量。如果显存不足，可以通过以下方法优化：1) 降低批量大小，2) 使用梯度累积（gradient accumulation），3) 分布式训练。
19. **大模型LLM进行SFT操作时在学习什么？** SFT（Supervised Fine-Tuning）操作通常是在大模型上进行有监督微调，通过特定任务的数据进行训练。在SFT过程中，模型学习的是如何更好地**执行特定任务**，例如：文本分类、命名实体识别、翻译、摘要生成等。其核心目标是让模型在预训练的基础上，在目标任务上获得更好的表现。
20. **预训练和SFT操作有什么不同？** 预训练是模型学习通用语言模式和基础知识的阶段，通常是在大量无标签的数据上进行自监督学习。预训练的目的是让模型具备通用的语言理解和生成能力。 而**SFT（有监督微调）**是基于特定任务或领域进行的微调，通过标注数据让模型专注于优化某个具体任务或应用。SFT的目标是让模型能够在目标任务上表现得更好。
21. **样本量规模增大，训练出现OOM错误**： OOM（Out of Memory）错误通常发生在GPU显存不足的情况下。如果在训练过程中样本量增加，模型所需的内存也会增加，导致OOM错误。解决这个问题可以采取以下措施：
	1. **减少批量大小**（batch size）；
	2. **使用混合精度训练**（FP16）；
	3. **模型并行化**，即将模型分布到多个GPU上；
	4. **梯度累积**，即通过多次前向传播来累积梯度，减少显存压力。
22. **大模型LLM进行SFT时如何对样本进行优化？** 在进行SFT时，优化样本数据通常包括（SWAP）：
	1. **数据选择**（Select）：选取与任务最相关且高质量的数据。
	2. **样本权重**（Weighten）：对样本进行加权，帮助模型更多关注那些对任务有重要贡献的样本。
	3. **数据增强**（Augment）：通过数据增强（如同义词替换、文本重排序等）增加样本的多样性。
	4. **样本剪裁**（Prune）：在某些情况下，对那些不利于模型训练的噪声样本进行筛选或过滤。
23. **模型参数迭代实验**： 模型参数迭代实验是指通过调整模型的超参数（如学习率、批量大小、优化器选择等）来优化模型的表现。实验通常包括以下几个步骤：
	1. **选择超参数范围**：根据任务选择可能的超参数值范围。
	2. **训练与验证**：使用不同的超参数组合进行多次训练，并通过验证集来评估性能。
	3. **性能评估**：根据模型的表现（例如准确率、F1分数等）选择最佳的超参数组合。
	4. **迭代优化**：通过不断调整，找到最优的参数配置。
24. **微调大模型的一些建议**： 微调大模型时，有几个关键建议可以帮助提高效率和效果：
	1. **选择合适的预训练模型**：根据任务的需求选择一个与任务相似的预训练模型作为基础。
	2. **少量数据微调**：对于特定任务或小样本任务，可以采用少量数据微调（例如，通过迁移学习的方式）。
	3. **合适的学习率**：微调时使用较小的学习率，避免对已有的预训练权重进行过大的更新。
	4. **正则化**：使用Dropout、L2正则化等方法避免过拟合。
	5. **保存中间模型**：训练过程中定期保存模型，以防止训练不稳定或早期停止。
	6. **使用验证集**：通过验证集来调整超参数和监控模型的泛化能力。
    

微调大模型时，合理调整超参数、选择合适的数据以及采用高效的训练方法，能够在保持良好性能的同时避免过拟合。

## 大模型（LLMs）训练经验帖

- [分布式训练框架](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E6%A1%86%E6%9E%B6&zhida_source=entity)选择？
- LLMs 训练时 有哪些有用的建议？
- 模型大小如何选择？
- 加速卡如何选择？

在大模型训练时，以下是针对你提出的几个问题的详细建议：

1. **分布式训练框架如何选择？**

选择合适的分布式训练框架取决于多个因素，如硬件环境、可扩展性需求、易用性等。以下是几种常见的框架：
- **Horovod**：
    - 优势：基于MPI (Message Passing Interface) 的分布式训练框架，广泛应用于大规模深度学习训练。支持TensorFlow、Keras、PyTorch等框架。
    - 特点：高效的分布式梯度同步机制，能够在多个GPU甚至多个节点间实现并行训练。
- **DeepSpeed**：
    - 优势：微软开源的训练优化库，提供模型并行、数据并行、混合精度训练、梯度累积等功能，专为处理大型模型（如GPT、T5等）设计。
    - 特点：支持零冗余优化（ZeRO），显著提高了内存利用率，适合大规模模型的训练。
- **TensorFlow Distributed (TF-Distribution)**：
    - 优势：TensorFlow提供的原生分布式训练支持，简化了分布式训练的过程，适用于大规模训练任务。
    - 特点：使用`tf.distribute.Strategy` API，方便进行数据并行训练，并支持模型并行训练。
- **PyTorch Distributed**：
    - 优势：PyTorch原生支持分布式训练，结合`torch.nn.parallel.DistributedDataParallel`（DDP）可以有效进行数据并行训练。
    - 特点：易于使用和调试，广泛应用于学术研究和工业界，具有较强的灵活性和扩展性。

**选择建议**：如果你需要在多个节点上进行大规模分布式训练，Horovod和DeepSpeed是非常好的选择。如果你的项目主要依赖PyTorch或TensorFlow，则可以考虑使用PyTorch Distributed或TensorFlow Distributed。

2. **LLMs训练时的有用建议**
- **预训练与微调**：
    - 预训练大语言模型时，首先要有大量的文本数据（通常是无标签数据）。接着可以进行微调（fine-tuning），使模型适应特定任务或领域的需求。
- **模型架构选择**：
    - 选择适当的架构非常重要。Transformer架构是目前大多数LLM的基础，像GPT、BERT等都使用Transformer。
- **数据预处理**：
    - 高质量、充分多样化的数据集对LLM的训练至关重要。要保证数据的多样性和代表性，同时进行适当的文本清洗和去噪。
- **批量大小与梯度累积**：
    - 对于LLM训练，通常使用较大的批量大小，但受硬件限制，可能需要使用梯度累积技术来模拟更大的批量，从而提高训练效率。
- **混合精度训练**：
    - 使用混合精度训练（FP16 + FP32）可以显著加速训练过程，减少显存占用。
- **动态学习率调整**：
    - 使用如`lr_scheduler`等动态学习率策略来避免模型训练过程中出现过拟合或收敛困难的问题。

3. **模型大小如何选择？**

- **硬件限制**：你的硬件环境（如GPU显存、内存等）会直接影响可训练的模型大小。如果硬件资源较为紧张，可以选择使用模型压缩、剪枝、量化等技术来优化模型大小。
- **任务需求**：任务的复杂性和准确性要求也会影响模型大小。对于一些简单的任务，小模型（如DistilBERT等）可能已经足够，但对于复杂的生成任务，可能需要更大的模型。
- **训练数据量**：通常，大模型在大数据集上训练时能够展现更强的性能。如果数据集较小，使用过大的模型可能会出现过拟合，反而影响效果。
- **训练成本**：更大的模型需要更多的计算资源、存储空间和训练时间。需要综合考虑项目的预算和时间限制来选择合适的模型大小。

4. **加速卡如何选择？**
- **NVIDIA A100**：
    - 适合大规模训练，提供强大的性能和显存，支持大多数深度学习框架。
    - 特别适用于Transformer架构和大规模的自然语言处理任务。
- **NVIDIA H100**：
    - 基于最新的Hopper架构，适用于超大规模AI模型的训练，拥有更高的性能和内存带宽，适合下一代的大规模训练任务。
- **AMD MI250X**：
    - 如果你的框架支持AMD的硬件，可以考虑使用AMD的MI250X加速卡。AMD在深度学习领域逐渐崭露头角，尤其是在一些多卡训练环境中。
- **TPU（Tensor Processing Units）**：
    - 对于使用Google Cloud的用户，TPU是一个非常好的选择，尤其是在训练大型Transformer模型时，能够提供极高的性能。

**选择建议**：如果预算充足，NVIDIA A100或H100是当前大规模训练的最佳选择。对于云端计算，TPU也提供了优秀的性能，特别适合Google Cloud环境下的训练。

总体来说，选择适合的分布式训练框架、加速卡和模型大小需要综合考虑硬件资源、训练任务的复杂性、预算和时间要求等因素。希望这些建议对你有所帮助！

## 大模型（LLMs）langchain 面

### [大模型（LLMs）langchain 面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ve2dgaiqrjzv.html)

1. 什么是 LangChain?
2. LangChain 包含哪些 核心概念？
- 2.1 LangChain 中 Components and Chains 是什么？
- 2.2 LangChain 中 Prompt Templates and Values 是什么？
- 2.3 LangChain 中 Example Selectors 是什么？
- 2.4 LangChain 中 Output Parsers 是什么？
- 2.5 LangChain 中 Indexes and Retrievers 是什么？
- 2.6 LangChain 中 Chat Message History 是什么？
- 2.7 LangChain 中 Agents and Toolkits 是什么？
3. 什么是 LangChain Agent?
4. 如何使用 LangChain ?
5. LangChain 支持哪些功能?
6. 什么是 LangChain model?
7. LangChain 包含哪些特点?
8. LangChain 如何使用?
- 8.1 LangChain 如何调用 LLMs 生成回复？
- 8.2 LangChain 如何修改 提示模板？
- 8.3 LangChain 如何链接多个组件处理一个特定的下游任务？
- 8.4 LangChain 如何Embedding & vector store？
9. LangChain 存在哪些问题及方法方案？
- LangChain 低效的令牌使用问题
- LangChain 文档的问题
- LangChain 太多概念容易混淆，过多的“辅助”函数问题
- LangChain 行为不一致并且隐藏细节问题
- LangChain 缺乏标准的可互操作数据类型问题
10. LangChain 替代方案？

*Answer:*

**1. 什么是 LangChain?**

LangChain 是一个开源框架，旨在帮助开发者构建和部署基于语言模型（LLM）的应用程序。它提供了对不同语言模型、数据源、外部 API 以及各种链式操作的统一接口，使得开发者可以更加方便地创建复杂的语言处理工作流。LangChain 主要应用于 NLP 任务（如文本生成、搜索、问答等）和一些自动化任务（如 agent-driven workflows）。这个框架非常适合与大型语言模型（如 GPT）以及其他 AI 工具结合使用。

**2. LangChain 包含哪些核心概念?**

LangChain 的核心概念涉及到多个模块和组件，这些组件有助于开发者实现更灵活和功能丰富的语言模型应用。以下是 LangChain 中的一些核心概念：

2.1 **Components and Chains**
- **Components** 是构成 LangChain 应用程序的基本单元。它们可以是不同类型的组件，如 LLM（语言模型）、Prompts（提示）、转换器、索引器、Retriever 等。每个组件都执行一个特定的操作，并能在应用程序中彼此交互。
- **Chains** 是将多个组件组合在一起形成一个工作流。通过链式操作，LangChain 允许你将多个处理步骤串联，形成一个完整的处理过程。例如，一个典型的链可能会包括数据检索、文本生成、输出解析等多个步骤。
2.2 **Prompt Templates and Values**
- **Prompt Templates** 是用于构建动态生成提示（prompts）的工具。它们允许开发者使用占位符（placeholders）来表示输入变量，然后在实际运行时替换为特定的值。Prompt Templates 可以帮助开发者更方便地管理和生成动态提示，以便更好地引导语言模型生成所需的结果。
- **Prompt Values** 是实际传递给 Prompt Template 的具体数据或参数。它们可以是用户输入的数据、从数据库中获取的值，或者计算得到的结果。
2.3 **Example Selectors**
- **Example Selectors** 是一种用于选择示例数据的机制。在许多任务中，示例数据可以用来帮助模型理解问题的上下文并生成更准确的输出。LangChain 提供了 Example Selectors，用于从预定义的示例集合中选择合适的示例来供语言模型使用。
2.4 **Output Parsers**
- **Output Parsers** 是用来解析语言模型输出的工具。它们可以对语言模型的原始输出进行后处理，转化为更适合应用的格式。例如，如果语言模型生成的是一段文本，而应用程序需要结构化的数据，Output Parsers 可以将文本转换为表格、JSON 或其他数据格式。
2.5 **Indexes and Retrievers**
- **Indexes** 是用来存储和管理大量信息的结构。在 LangChain 中，索引可以是文本数据、文档或其他形式的数据。索引通常结合检索算法（retrieval algorithms）使用，允许快速查找和获取相关信息。
- **Retrievers** 是用于从索引中检索信息的工具。它们能够根据用户的查询或特定的搜索条件，从大型数据集或文档库中找到相关的内容。
2.6 **Chat Message History**
- **Chat Message History** 是一种机制，用于跟踪和存储与用户之间的对话历史。在基于聊天的应用中，记录消息历史对于维护上下文和生成连贯的对话至关重要。LangChain 提供了管理和操作消息历史的工具，以便开发者创建更智能、具有上下文理解能力的聊天系统。
2.7 **Agents and Toolkits**
- **Agents** 是一种基于语言模型的自适应系统，它们能够理解任务、获取信息并执行动作。Agents 通常由多个工具和组件构成，可以根据任务需求进行决策。它们能够调用其他工具、检索信息、生成输出等。
- **Toolkits** 是一组与 Agents 结合使用的工具，可以提供给 Agents 执行任务时使用的外部资源。Toolkits 可以包括诸如搜索引擎、API、数据库查询等工具，使 Agents 能够执行复杂的任务。

这些核心概念的组合，使得 LangChain 成为一个功能强大且灵活的框架，适合构建各种基于语言模型的应用程序和自动化工作流。

3. **什么是 LangChain Agent?**

LangChain Agent 是 LangChain 中的一个重要概念，它允许构建智能的代理程序，这些代理能够根据输入动态地选择、调用不同的工具或执行不同的操作来解决问题。LangChain Agent 基于一系列预定义的动作，它能够在应用程序中自动选择最合适的工具（如外部API、数据库查询、计算等）进行任务处理。这些代理可以用来执行复杂的任务、处理多步骤推理、与其他系统交互等。

4. 如何使用 LangChain?

- **安装 LangChain**：首先，你需要安装 LangChain 库，通常使用 pip 安装：
    `pip install langchain`
- **设置和配置模型**：你可以通过 LangChain 轻松地集成不同的语言模型，例如 OpenAI 的 GPT、Google PaLM、Hugging Face 模型等。
- **构建管道或代理**：你可以根据任务需求，使用 LangChain 中的工具构建处理链。例如，创建一个链式处理流程，将用户输入传递给不同的工具来完成多步推理。
- **调用并执行任务**：构建好链或代理后，可以调用并执行任务。

5. **LangChain 支持哪些功能?**

LangChain 提供了多种功能来支持语言模型驱动的应用程序，包括：

1. **LLM（语言模型）支持**：支持与多种语言模型的集成，如 OpenAI、Hugging Face、Google等。
2. **链（Chains）**：链是 LangChain 中的核心概念，用于将多个组件（如语言模型、数据源、工具等）串联起来，形成一个工作流。支持单步骤和多步骤链。
3. **代理（Agents）**：允许动态选择工具，执行多步骤推理，或与外部系统交互。
4. **记忆（Memory）**：LangChain 支持在对话中保持记忆，以便跟踪上下文和用户状态。
5. **工具（Tools）**：LangChain 支持与各种外部工具和 API 集成，例如数据库、搜索引擎、Web API、文件处理等。
6. **Prompt 模板**：可以通过模板化的方式构建和管理复杂的提示（prompt），以提高任务的准确性和一致性。
7. **自定义管道**：可以构建和定制数据流管道，灵活处理不同的业务需求。


6. **什么是 LangChain model?**

LangChain 模型是指与 LangChain 框架集成的语言模型，它们用于处理自然语言任务。这些模型可以是任何已知的预训练语言模型，如 OpenAI GPT 系列、Hugging Face 的模型、Anthropic 的 Claude 等。LangChain 提供了一个统一的接口，允许开发者轻松地配置和操作这些模型。

7. LangChain 包含哪些特点?

LangChain 的一些显著特点包括：
1. **模块化和扩展性**：LangChain 的架构是高度模块化的，支持扩展、定制不同的组件，如工具、代理、链、提示模板等。
2. **易于集成外部工具**：支持与多种外部系统的集成，包括数据库、Web API、文件存储、搜索引擎等。
3. **支持多步推理**：通过链和代理，LangChain 可以实现复杂的多步骤推理任务，并在每个步骤中动态选择操作。
4. **记忆和上下文管理**：LangChain 支持记忆功能，允许系统在多个交互中保持上下文，以便实现更加连贯的对话和任务处理。
5. **开箱即用**：提供了多种开箱即用的模板和功能，帮助开发者快速启动项目。
6. **灵活的提示设计**：提供强大的提示模板支持，使得处理自然语言输入和输出变得更加方便和灵活。

8、9、10：WIP

### WIP：基于LLM+向量库的文档对话 经验面

- 一、基于LLM+向量库的文档对话 基础面

- 1.1 为什么 大模型 需要 外挂(向量)知识库？
- 1.2. 基于LLM+向量库的文档对话 思路是怎么样？
- 1.3. 基于LLM+向量库的文档对话 核心技术是什么？
- 1.4. 基于LLM+向量库的文档对话 prompt 模板 如何构建？

- 二、基于LLM+向量库的文档对话 存在哪些痛点？
- 三、基于LLM+向量库的文档对话 工程示例面

### WIP：LLM文档对话 —— pdf解析关键问题

- 一、为什么需要进行pdf解析？
- 二、为什么需要 对 pdf 进行解析？
- 三、pdf解析 有哪些方法，对应的区别是什么？
- 四、pdf解析 存在哪些问题？
- 五、如何 长文档（书籍）中关键信息？
- 六、为什么要提取标题甚至是多级标题？
- 七、如何提取 文章标题？
- 八、如何区分单栏还是双栏pdf？如何重新排序？
- 九、如何提取表格和图片中的数据？
- 十、基于AI的文档解析有什么优缺点？

### WIP：基于LLM+向量库的文档对话 经验面

- 一、基于LLM+向量库的文档对话 基础面

- 1.1 为什么 大模型 需要 外挂(向量)知识库？
- 1.2. 基于LLM+向量库的文档对话 思路是怎么样？
- 1.3. 基于LLM+向量库的文档对话 核心技术是什么？
- 1.4. 基于LLM+向量库的文档对话 prompt 模板 如何构建？

- 二、基于LLM+向量库的文档对话 存在哪些痛点？
- 三、基于LLM+向量库的文档对话 工程示例面
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_m9t1w8pokjpf.html)

## [大模型（LLMs）参数高效微调(PEFT) 面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ahk2br3igwx9.html)

### 大模型（LLMs）参数高效微调(PEFT) 面

- 微调方法是啥？如何微调？
- 为什么需要 PEFT？
- 介绍一下 PEFT？
- PEFT 有什么优点？
- 微调方法批处理大小模式GPU显存速度？
- Peft 和全量微调区别？
- 多种不同的高效微调方法对比
- 当前高效微调技术存在的一些问题
- 高效微调技术最佳实践
- PEFT 存在问题？
- 能不能总结一下各种参数高效微调方法？

*Answer*:

- *微调方法是啥？如何微调？*
微调（Fine-tuning）是一种迁移学习的技术，用于在一个已经预训练好的模型基础上，通过进一步训练来适应特定的任务或数据集。微调可以在具有相似特征的任务之间共享知识，从而加快训练速度并提高模型性能。

- *为什么需要 PEFT？*
PEFT（Parameter-Efficient Fine-Tuning）是一种在保持预训练模型大部分参数不变的情况下，通过仅调整少量额外参数来适应新任务的技术。这种方法可以在提高模型效果的同时，大大缩短模型训练时间和计算成本。

- *介绍一下 PEFT？*
PEFT是一种参数高效微调技术，旨在通过最小化微调参数的数量和计算复杂度，来提高预训练模型在新任务上的性能，从而缓解大型预训练模型的训练成本。

- *PEFT 有什么优点？*
PEFT技术可以在提高模型效果的同时，大大缩短模型训练时间和计算成本，让更多人能够参与到深度学习研究中来。除此之外，PEFT可以缓解全量微调带来灾难性遗忘的问题。

- *微调方法批处理大小模式GPU显存速度？*
不同微调方法的批处理大小、模式、GPU显存和速度如下：
- LoRA (r=8) 16 FP16 28GB 8ex/s
- LoRA (r=8) 8 FP16 24GB 8ex/s
- LoRA (r=8) 4 FP16 20GB 8ex/s
- LoRA (r=8) 4 INT8 10GB 8ex/s
- LoRA (r=8) 4 INT4 8GB 8ex/s
- P-Tuning (p=16) 4 FP16 20GB 8ex/s
- P-Tuning (p=16) 4 INT8 16GB 8ex/s
- P-Tuning (p=16) 4 INT4 12GB 8ex/s
- Freeze (l=3) 4 FP16 24GB 8ex/s
- Freeze (l=3) 4 INT8 12GB 8ex/s。

- *Peft 和全量微调区别？*
PEFT和全量微调（Full Fine-Tuning）是两种不同的微调方法。全量微调调整所有参数，而PEFT通过增加新的参数层、选择性微调部分参数或重新参数化来实现高效的微调。

- *多种不同的高效微调方法对比*
高效微调技术如P-Tuning v2、LoRA等都是综合评估很不错的高效微调技术。如果显存资源有限可以考虑QLoRA；如果只是解决一些简单任务场景，可以考虑P-Tuning、Prompt Tuning也行。

- *当前高效微调技术存在的一些问题*
高效微调技术存在的问题包括需要选择合适的预训练模型、冻结部分层、适当调整学习率、使用数据增强、使用早停策略以及结合其他高效微调技术。

- *高效微调技术最佳实践*
高效微调技术的最佳实践包括选择合适的预训练模型、冻结部分层、适当调整学习率、使用数据增强、使用早停策略以及结合其他高效微调技术。

- *PEFT 存在问题？*
PEFT存在的问题包括可能的效果损失，尽管一般效果能打平。

- *能不能总结一下各种参数高效微调方法？*
参数高效微调方法包括：
- **加性微调**：通过在预训练模型的特定位置添加可学习的模块或参数，以最小化适配下游任务时模型的可训练的参数量。
- **选择性微调**：在微调过程中只更新模型中的一部分参数，而保持其余参数固定。
- **重参数化微调**：通过构建预训练模型参数的（低秩的）表示形式用于训练。
- **混合微调**：结合了各类PEFT方法的优势，并通过分析不同方法的相似性以构建一个统一的PEFT架构。


### 适配器微调（Adapter-tuning）篇

- 一、为什么 需要 [适配器微调](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E9%80%82%E9%85%8D%E5%99%A8%E5%BE%AE%E8%B0%83&zhida_source=entity)（Adapter-tuning）？
- 二、适配器微调（Adapter-tuning）思路？
- 三、 适配器微调（Adapter-tuning）特点是什么？
- 四、AdapterFusion 思路 是什么？
- 五、AdapterDrop 思路 是什么？
- 六、AdapterDrop 特点 是什么？
- 七、MAM Adapter 思路 是什么？
- 八、MAM Adapter 特点 是什么？

Answer:

一、*为什么需要适配器微调（Adapter-tuning）？* 适配器微调（Adapter-tuning）是一种用于微调预训练模型的方法，相比于传统的微调方法具有一些优势和应用场景。需要适配器微调的情况包括：

1. 保留预训练模型的知识：适配器微调可以只微调模型的适配器层，而不改变预训练模型的参数，从而保留预训练模型的知识。
2. 减少微调的计算量和时间：适配器微调只需要微调适配器层的参数，而不需要重新训练整个模型，这样可以显著减少微调的计算量和时间。
3. 提高模型的可解释性和可复用性：适配器微调可以使模型更具可解释性和可复用性，通过在适配器层中添加任务特定的适配器，可以更好地理解模型在不同任务上的表现。
4. 避免灾难性遗忘：适配器微调通过只微调适配器层，可以避免对预训练模型的其他部分进行大幅度的更新，从而减少灾难性遗忘的风险。

二、*适配器微调（Adapter-tuning）思路？* 适配器微调的思路是在预训练模型中添加适配器层，并只微调适配器层的参数，从而保留预训练模型的知识、减少计算量和时间，并提高模型的可解释性和可复用性。具体步骤包括：

1. 预训练模型选择：选择一个适合任务的预训练模型，例如BERT、GPT等。
2. 适配器层添加：在选择的预训练模型中，为目标任务添加适配器层。适配器层是一个小型的任务特定层，通常由一个或多个全连接层组成。
3. 冻结其他层：在适配器微调中，通常会冻结预训练模型的其他层，只微调适配器层的参数。

三、*适配器微调（Adapter-tuning）特点是什么？* 适配器微调具有以下特点：

1. 保留预训练模型的知识：适配器微调只微调适配器层的参数，而不改变预训练模型的其他参数。
2. 减少微调的计算量和时间：适配器微调只需要微调适配器层的参数，而不需要重新训练整个模型。
3. 提高模型的可解释性和可复用性：适配器微调在预训练模型中添加了适配器层，这些适配器层可以理解为任务特定的模块。
4. 避免灾难性遗忘：适配器微调只微调适配器层的参数，不对预训练模型的其他部分进行大幅度的更新，可以减少灾难性遗忘的风险。

四、*AdapterFusion 思路是什么？* AdapterFusion是一种用于多任务学习的方法，其思路可以概括如下：

1. 预训练模型选择：选择一个适合多任务学习的预训练模型，例如BERT、GPT等。
2. 适配器层添加：在选择的预训练模型中，为每个任务添加适配器层。适配器层是一个小型的任务特定层，通常由一个或多个全连接层组成。
3. 适配器融合：在AdapterFusion中，适配器融合是关键步骤。适配器融合通过将不同任务的适配器层的输出进行融合，得到一个综合的表示。

五、*AdapterDrop 思路是什么？* AdapterDrop的思路是通过适配器层的随机丢弃机制，实现动态的适配器选择和微调。这种方法可以增加模型的鲁棒性和泛化能力，使得模型能够适应不同任务的变化和不确定性。

六、*AdapterDrop 特点是什么？* AdapterDrop的特点包括：

1. 通过从较低的Transformer层删除可变数量的Adapter来提升推理速度。
2. 当对多个任务执行推理时，动态地减少了运行时的计算开销，并在很大程度上保持了任务性能。

七、*MAM Adapter 思路是什么？* MAM Adapter是一种在Adapter、Prefix Tuning和LoRA之间建立联系的统一方法。最终的模型MAM Adapter是用于FFN的并行Adapter和软提示的组合。

八、*MAM Adapter 特点是什么？* MAM Adapter的特点包括：

1. 整体上来说，最终的模型MAM Adapter效果会优于单个高效微调方法。

### [提示学习（Prompting）](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_662wpbw47gtj.html)

- 一、为什么需要 提示学习（Prompting）？
- 二、什么是 提示学习（Prompting）？
- 三、提示学习（Prompting） 有什么优点？
- 四、提示学习（Prompting）有哪些方法，能不能稍微介绍一下它们间？

- 4.1 [前缀微调](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E5%89%8D%E7%BC%80%E5%BE%AE%E8%B0%83&zhida_source=entity)（Prefix-tining）篇

- 4.1.1 为什么需要 前缀微调（Prefix-tining）？
- 4.1.2 前缀微调（Prefix-tining）思路是什么？
- 4.1.3 前缀微调（[Prefix-tining](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=4&q=Prefix-tining&zhida_source=entity)）的优点是什么？
- 4.1.4 前缀微调（Prefix-tining）的缺点是什么？

- 4.2 指示微调（Prompt-tuning）篇

- 4.2.1 为什么需要 指示微调（Prompt-tuning）？
- 4.2.2 指示微调（[Prompt-tuning](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=3&q=Prompt-tuning&zhida_source=entity)）思路是什么？
- 4.2.3 指示微调（Prompt-tuning）优点是什么？
- 4.2.4 指示微调（Prompt-tuning）缺点是什么？
- 4.2.5 指示微调（Prompt-tuning）与 [Prefix-tuning](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=Prefix-tuning&zhida_source=entity) 区别 是什么？
- 4.2.6 指示微调（Prompt-tuning）与 [fine-tuning](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=fine-tuning&zhida_source=entity) 区别 是什么？

- 4.3 P-tuning 篇

- 4.3.1 为什么需要 [P-tuning](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=2&q=P-tuning&zhida_source=entity)？
- 4.3.2 P-tuning 思路是什么？
- 4.3.3 P-tuning 优点是什么？
- 4.3.4 P-tuning 缺点是什么？

- 4.4 P-tuning v2 篇

- 4.4.1 为什么需要 P-tuning v2？
- 4.4.2 P-tuning v2 思路是什么？
- 4.4.3 P-tuning v2 优点是什么？
- 4.4.4 P-tuning v2 缺点是什么？

A:

一、*为什么需要提示学习（Prompting）？*

提示学习的出现主要是为了解决传统预训练模型加微调（PLM+Finetuning）形式的缺陷。这些缺陷包括：

1. **零样本/少样本学习能力差**：预训练模型在预训练阶段采用的任务形式（如自回归、自编码）与下游任务形式存在较大差别，导致模型在微调时需要较多数据来适应新的形式，容易丧失预训练模型上已经学习到的能力。
2. **模型专用性导致成本高昂**：预训练模型参数数量极大，为了一个特定任务去fine-tuning一个模型，然后去部署，会造成部署资源的极大浪费。

因此，提示学习被提出作为一种新的范式，旨在利用预训练模型学习到的知识，并做到部分通用性，减少finetuning的部分。

二、*什么是提示学习（Prompting）？*

提示学习（Prompting）是一种基于预训练模型（如BERT、GPT等）的上下文学习算法。它通过将预训练模型与提示符相结合，使模型能够利用先验知识进行分类或生成任务。在提示学习中，不同的任务可以通过不同的提示符来引导模型进行学习。这种方法允许模型在不更新任何参数的情况下，在零样本、少样本场景下实现不错的效果。

三、*提示学习（Prompting）有什么优点？*

提示学习（Prompting）具有以下几个优点：

1. **少样本学习**：提示学习可以在少样本甚至无样本的情况下实现良好的分类或生成效果，这使得它特别适合于在资源有限的情况下进行模型训练。
2. **利用先验知识**：提示学习可以利用预训练模型的先验知识，从而避免从头开始训练模型。这使得它能够更快地进行训练，并提高模型的泛化能力。
3. **灵活性**：提示学习具有很高的灵活性，可以轻松地适应不同的任务和数据集。只需要根据任务特点设计合适的提示符，就可以将预训练模型应用于各种不同的场景。
4. **简化训练过程**：通过使用提示，可以简化训练过程，减少模型的训练时间和计算资源的消耗。
5. **提供可解释性**：提示作为人工设计的输入，可以提供对模型生成输出的解释和理解。通过分析和调整提示，可以更好地理解模型在生成过程中的决策和行为，从而提高模型的可解释性。

四、提示学习（Prompting）有哪些方法？

提示学习的方法主要包括前缀微调（Prefix-tuning）、指示微调（Prompt-tuning）、P-tuning及其改进版本P-tuning v2。这些方法各有特点和应用场景。

4.1 前缀微调（Prefix-tuning）

4.1.1 为什么需要前缀微调（Prefix-tuning）？

前缀微调旨在通过在输入序列前添加一系列可训练的向量（称为前缀）来调整预训练模型，以适应特定任务。这种方法避免了对整个模型参数的微调，从而减少了计算成本。

4.1.2 前缀微调（Prefix-tuning）思路是什么？

在前缀微调中，模型输入被修改为包含一个可训练的前缀，这个前缀与原始输入一起通过模型。前缀的作用是引导模型的注意力机制，使其更专注于任务相关的特征。

4.1.3 前缀微调（Prefix-tuning）的优点是什么？

- **计算效率高**：只需训练前缀向量，而不是整个模型参数。
- **灵活性**：适用于多种任务，只需调整前缀即可。
- **减少过拟合风险**：由于只训练少量参数，减少了过拟合的可能性。

4.1.4 前缀微调（Prefix-tuning）的缺点是什么？

- **表现可能受限**：由于只调整前缀，可能无法充分利用模型的全部能力。
- **调试复杂性**：设计和调试前缀可能需要较多的实验和调整。

4.2 指示微调（Prompt-tuning）

4.2.1 为什么需要指示微调（Prompt-tuning）？

指示微调通过调整提示词本身来适应不同的任务，而不是调整模型参数。这种方法利用了提示词的灵活性和表达能力。

4.2.2 指示微调（Prompt-tuning）思路是什么？

在指示微调中，提示词被视为模型输入的一部分，并通过训练进行调整，以更好地引导模型完成任务。

4.2.3 指示微调（Prompt-tuning）优点是什么？

- **任务适应性强**：通过调整提示词，可以灵活适应不同的任务。
- **减少参数更新**：不需要更新模型的全部参数，只需调整提示词。

4.2.4 指示微调（Prompt-tuning）缺点是什么？

- **提示设计复杂**：设计有效的提示词可能需要较多的人工努力和实验。
- **泛化能力受限**：提示词的设计可能限制了模型的泛化能力。

4.2.5 指示微调（Prompt-tuning）与Prefix-tuning区别是什么？

- **调整对象不同**：指示微调调整提示词，而Prefix-tuning调整输入前缀。
- **灵活性和适应性**：指示微调更侧重于提示词的设计，而Prefix-tuning通过前缀引导模型注意力。

4.2.6 指示微调（Prompt-tuning）与fine-tuning区别是什么？

- **参数更新范围**：指示微调只调整提示词，而fine-tuning更新整个模型参数。
- **计算成本**：指示微调通常计算成本更低，因为只调整提示词。

4.3 P-tuning

4.3.1 为什么需要P-tuning？

P-tuning旨在通过引入可训练的提示向量来增强模型的任务适应性，同时保持模型参数不变。

4.3.2 P-tuning思路是什么？

P-tuning通过将提示向量嵌入到模型输入中，并通过训练来调整这些向量，以引导模型完成特定任务。

4.3.3 P-tuning优点是什么？

- **参数效率**：不需要更新模型参数，只需训练提示向量。
- **任务灵活性**：通过调整提示向量，可以灵活适应不同的任务。

4.3.4 P-tuning缺点是什么？

- **表现可能受限**：由于只调整提示向量，可能无法充分利用模型的全部能力。
- **调试复杂性**：设计和调试提示向量可能需要较多的实验和调整。

4.4 P-tuning v2

4.4.1 为什么需要P-tuning v2？

P-tuning v2是对P-tuning的改进，旨在进一步提高模型的任务适应性和表现。

4.4.2 P-tuning v2思路是什么？

P-tuning v2通过引入更复杂的提示结构和训练策略，以更好地引导模型完成任务。

4.4.3 P-tuning v2优点是什么？

- **增强的任务适应性**：通过更复杂的提示结构，提高了模型的任务适应性。
- **改进的表现**：相比P-tuning，P-tuning v2通常能取得更好的任务表现。

4.4.4 P-tuning v2缺点是什么？

- **复杂性增加**：更复杂的提示结构可能导致调试和训练的复杂性增加。
- **计算成本**：虽然参数效率高，但更复杂的提示结构可能增加计算成本。

### [LoRA 系列篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ham28l44907e.html)

- 一、LoRA篇

- 1.1 什么是 LoRA？
- 1.2 LoRA 的思路是什么？
- 1.3 LoRA 的特点是什么？

- 二、QLoRA篇

- 2.1 QLoRA 的思路是怎么样的？
- 2.2 QLoRA 的特点是什么？

- 三、AdaLoRA篇

- 3.1 AdaLoRA 的思路是怎么样的？

- 四、LoRA权重是否可以合入原模型？
- 五、ChatGLM-6B LoRA后的权重多大？
- 六、LoRA 微调优点是什么？
- 七、LoRA微调方法为啥能加速训练？
- 八、如何在已有LoRA模型上继续训练？
- 九、LoRA 缺点是什么？
- 十、LoRA这种微调方法和全参数比起来有什么劣势吗？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ham28l44907e.html)

## [大模型（LLMs）推理面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_64vc5vvwpobv.html)

### [大模型（LLMs）推理面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_topiwiusclkr.html)

1. 为什么大模型推理时显存涨的那么多还一直占着？
2. 大模型在gpu和cpu上推理速度如何？
3. 推理速度上，int8和fp16比起来怎么样？
4. 大模型有推理能力吗？
5. 大模型生成时的参数怎么设置？
6. 有哪些省内存的大语言模型训练/微调/推理方法？

- 6.1 如何 估算模型所需的RAM？
- 6.2 Fp16-mixed precision
- 6.3 Int8-bitsandbytes
- 6.4 LoRA
- 6.5 Gradient Checkpointing
- 6.6 Torch FSDP+CPU offload

7. 如何让大模型输出合规化
8. 应用模式变更

A：

1. **为什么大模型推理时显存涨的那么多还一直占着？** 大模型推理时显存占用多是因为模型参数量巨大，这些参数需要存储在显存中以供推理使用。显存占用与模型参数量成正比，因此大模型会占用相当大的显存空间。此外，显存占用与batch size成正比，模型输出不需要存储相应的动量信息。
2. **大模型在gpu和cpu上推理速度如何？** 一般情况下，GPU在进行深度学习推理任务时具有更高的计算性能，因此大语言模型在GPU上的推理速度通常会比在CPU上更快。
3. **推理速度上，int8和fp16比起来怎么样？** 在CPU上，INT8略快；在GPU上，FP16最快。INT8量化可以显著加速模型的推理速度，特别是当使用硬件加速时，INT8的计算性能可以远超FP32/FP16的推理。
4. **大模型有推理能力吗？** 是的，大模型具有强大的推理能力，这在其智能水平的重要体现中得到了验证。近年来，大模型在处理复杂任务时展现出强大的推理能力。
5. **大模型生成时的参数怎么设置？** 在使用大型语言模型进行文本生成时，调整生成参数（如temperature、top_k等）是优化生成结果质量和多样性的重要手段。温度设置用于控制模型生成内容的随机性，温度高时，生成的内容会更加多样。

6. **有哪些省内存的大语言模型训练/微调/推理方法？**
    - **FP16 Mixed Precision**：使用16位浮点数（FP16）进行混合精度训练，这种技术可以在保证训练速度的同时减少内存占用，提高模型训练效率。
    - **INT8 Quantization**：通过将模型权重量化为INT8，进一步减少数据的表示精度，以换取更小的存储需求和更快的计算速度。
    - **LoRA**：Low-Rank Adaptation of Large Language Models，通过仅训练低秩矩阵，然后将这些参数注入到原始模型中，从而实现对模型的微调。这种方法减少了计算需求，使得训练资源比直接训练原始模型要小得多。
    - **Gradient Checkpointing**：这种方法通过在训练过程中保存和重新计算梯度来减少内存占用。
    - **Torch FSDP + CPU Offloading**：使用Torch的FSDP（Fully Sharded Data Parallel）功能，结合CPU卸载技术，以减少GPU显存的占用。

6.1 **如何估算模型所需的RAM？**
- 可以使用HuggingFace的Accelerate推出的Model Memory Calculator工具来估算模型运行所需的显存大小。此外，根据模型参数量估算显存的方法包括考虑模型权重、优化器部分以及梯度的显存占用。

6.2 **FP16-Mixed Precision**
- FP16混合精度训练结合了半精度（FP16）和单精度（FP32）计算，以提高效率并减少内存占用。

6.3 **INT8-Bitsandbytes**
- bitsandbytes库支持8位推理与HuggingFace转换器集成，提供内存节省的优化。它专注于8位优化器，如LLM.int8()，用于矩阵乘法和量子化功能。

6.4 **LoRA**
- LoRA是一种低秩适应技术，用于微调大型语言模型。它通过仅训练低秩矩阵，然后将这些参数注入到原始模型中，从而实现对模型的微调。这种方法减少了计算需求，使得训练资源比直接训练原始模型要小得多。

6.5 **Gradient Checkpointing**
- Gradient Checkpointing通过在训练过程中保存和重新计算梯度来减少内存占用。

6.6 **Torch FSDP + CPU Offloading**
- 使用Torch的FSDP（Fully Sharded Data Parallel）功能，结合CPU卸载技术，以减少GPU显存的占用。

7. **如何让大模型输出合规化**
    要让大模型输出合规化，可以采取以下方法：
    - **数据清理和预处理**：在进行模型训练之前，对输入数据进行清理和预处理，以确保数据符合合规要求。这可能包括去除敏感信息。
    - **后处理函数**：使用后处理函数来确保输出格式符合要求。
8. **应用模式变更**
    - **深度理解行业需求和挑战**：分析目标行业的痛点、需求和发展趋势，明确哪些环节可以通过AI大模型来改善或革新。
    - **数据收集和分析**：收集行业内相关的大量、多样且高质量的数据，包括结构化和非结构化的数据资源。对数据进行清洗、预处理和标注，确保其符合训练AI大模型的标准和要求。
    - **定制化模型开发**：根据行业特性和应用场景设计和调整模型架构，可能需要对基础大模型进行微调或者迁移学习。
    - **模型训练与验证**：利用准备好的数据集对模型进行训练，评估模型在特定任务上的性能表现。
    - **模型集成与应用**：将训练好的模型集成到实际的产品或服务中，比如嵌入到智能客服系统、自动化决策平台、图像识别系统等。
    - **模型部署与运维**：设计适合生产环境的模型部署方案，考虑实时响应、安全性、隐私保护等因素。
    - **模型持续优化和迭代**：针对应用中发现的新问题和变化的需求，不断优化模型参数和算法策略。
    - **合规与安全性考量**：在整个过程中严格遵守法律法规，特别是对于数据安全、隐私保护及行业特有的合规要求。

## 大模型（LLMs）预训练面

### [大模型（LLMs）增量预训练篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_lj47ancwcmv2.html)

1. 为什么要增量预训练？
2. 进行 增量预训练 需要做哪些准备工作？
3. 增量预训练 所用 训练框架？
4. 增量预训练 训练流程 是怎么样？

A:

1. **为什么要增量预训练？**
    - 增量预训练的目的是为了让大模型具备领域知识。通过增量预训练，模型可以在已有预训练模型的基础上，利用新收集或新标注的数据进行再次训练，以优化模型对新数据的适应能力。这种方法避免了从头开始训练的耗时和资源消耗，同时能够保持模型对旧数据的记忆。
2. **进行增量预训练需要做哪些准备工作？**
    - **模型底座选型**：选择合适的底座模型，如LLaMA等。
    - **数据收集**：收集与当前任务或领域相关的新数据，确保数据质量和多样性。
    - **数据清洗**：对新数据进行清洗、标注和格式化，使其与模型输入格式一致。
3. **增量预训练所用训练框架？**
    - 对于超大规模训练，选用3D并行，Megatron-Deepspeed拥有多个成功案例。对于少量节点训练，选用张量并行，但张量并行只有在nvlink环境下才会起正向作用。如果资源特别少，显存不够，可以使用LoRA进行增量预训练。
4. **增量预训练训练流程是怎么样？**
    - **数据准备**：收集与当前任务或领域相关的新数据，并进行标注。
    - **模型选择与加载**：选择适合当前任务的预训练模型，并加载其权重。
    - **数据预处理**：对新数据进行清洗、标注和格式化，使其与模型输入格式一致。
    - **增量训练**：设置训练参数，根据新数据的特点和模型规模，调整学习率、批量大小等训练参数。在预训练模型的基础上，使用新数据进行训练，注意监控训练过程中的损失函数和评价指标，确保模型性能逐步提升。为防止模型过拟合新数据而忘记旧数据，可采用正则化方法，如L2正则化、dropout等。
    - **模型评估**：在验证集上评估增量训练后的模型性能，确保模型在新数据和旧数据上都能保持良好的表现。
    - **部署与迭代**：将训练好的模型部署到实际应用中，并根据反馈和新的数据持续进行增量训练，形成闭环迭代。

## [大模型（LLMs）评测面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_j9wcj62eovgc.html)

1. 大模型怎么评测？
2. 大模型的honest原则是如何实现的？模型如何判断回答的知识是训练过的已知的知识，怎么训练这种能力？
3. 如何衡量大模型水平？
4. 大模型评估方法 有哪些？
5. 大模型评估工具 有哪些？

A:

1. **大模型怎么评测？** 大模型的评测方法主要包括自动基准测试、使用人类作为评判者，以及使用模型作为评判者。自动基准测试通常涉及一组样本输入给模型，并通过标准答案对比输出结果。评估指标用于计算模型得分，以测试模型的泛化能力。此外，评测方法还可以分为人工评测和自动评测，自动评测技术相比人工评测来讲，具有效率高、一致性好、可复现等优点。
2. **大模型的honest原则是如何实现的？模型如何判断回答的知识是训练过的已知的知识，怎么训练这种能力？** 大模型的"honest"原则可以通过构造特定的训练样本实现，例如在微调时构造知识问答类训练集，让模型在不知道答案时不回答，以加强honest原则。此外，阅读理解题训练中，模型只回答读过的内容，不回答未读过的内容，避免胡说八道。
3. **如何衡量大模型水平？** 衡量大模型水平可以从多个维度提出具有代表性的问题，如理解能力、推理能力等。理解能力测试需要模型准确回答深入理解文本的问题。
4. **大模型评估方法有哪些？** 大模型评估方法包括自动基准测试、人类评判和模型评判。自动基准测试通过样本输入和标准答案对比来评估模型。评估指标包括准确性、稳定性、推理能力和泛化能力等。
5. **大模型评估工具有哪些？** 大模型评估工具包括OpenCompass（司南）这款开源工具，它提供了全面的能力维度测试，支持多种大模型和API接口。此外，还有llmuses框架，它预置了多个常用的测试基准数据集，并实现了常用评估指标。

## 大模型（LLMs）强化学习面

1. 简单介绍强化学习？
2. 简单介绍一下 RLHF？
3. 奖励模型需要和基础模型一致吗？
4. RLHF 在实践过程中存在哪些不足？
5. 如何解决 人工产生的偏好数据集成本较高，很难量产问题？
6. 如何解决三个阶段的训练（SFT->RM->PPO）过程较长，更新迭代较慢问题
7. 如何解决 PPO 的训练过程同时存在4个模型（2训练，2推理），对计算资源的要求较高 问题？

A:

1. **简单介绍强化学习？** 强化学习（Reinforcement Learning, RL）是机器学习的一个重要分支，它使得智能体通过与环境的互动来学习如何选择最优动作，以最大化累积奖励。
2. **简单介绍一下 RLHF？** RLHF（Reinforcement Learning from Human Feedback）是一种基于人类反馈进行强化学习的方法。它利用人类的直接反馈来训练“奖励模型”，然后利用该模型通过强化学习来优化人工智能模型的性能。
3. **奖励模型需要和基础模型一致吗？** 奖励模型和基础模型在训练过程中可以是一致的，也可以是不同的。这取决于任务需求和优化目标。
4. **RLHF 在实践过程中存在哪些不足？** RLHF 在实践过程中存在的不足包括：人工产生的偏好数据集成本较高，难以量产；三个阶段的训练（SFT->RM->PPO）过程较长，更新迭代较慢；PPO 的训练过程同时存在4个模型（2训练，2推理），对计算资源的要求较高。
5. **如何解决 人工产生的偏好数据集成本较高，很难量产问题？** 可以通过使用AI模型来替换人工标注数据形成偏好，或者指导模型训练。这种方法的核心在于通过AI模型监督其他AI模型，即在SFT阶段，从初始模型中采样，然后生成自我批评和修正，然后根据修正后的反应微调原始模型。
6. **如何解决三个阶段的训练（SFT->RM->PPO）过程较长，更新迭代较慢问题？** 可以通过优化训练流程和使用更高效的算法来减少训练时间。例如，使用AI模型来生成偏好数据，减少对人工标注的依赖，从而加快训练过程。
7. **如何解决 PPO 的训练过程同时存在4个模型（2训练，2推理），对计算资源的要求较高 问题？** 可以通过减少训练模型的数量或优化模型结构来降低计算资源的需求。例如，使用更高效的算法或减少模型的复杂度来降低资源消耗。

## WIP：LLM软硬件配置面

1. 建议的软件环境是什么？

WIP

## LLM训练集面

1. SFT（有监督微调）的数据集格式？
2. RM（奖励模型）的数据格式？
3. PPO（强化学习）的数据格式？
4. 找数据集哪里找？
5. 微调需要多少条数据？
6. 有哪些大模型的训练集？
7. 进行领域大模型预训练应用哪些[数据集](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=4&q=%E6%95%B0%E6%8D%AE%E9%9B%86&zhida_source=entity)比较好？

A:

1. **SFT（有监督微调）的数据集格式？**
    - SFT的数据集格式包括输入数据和标签数据。输入数据是一个文本序列，可以是字符串或tokenized的文本序列。标签数据是与输入数据对应的标签或类别，可以是单个类别或多个类别的集合。数据集通常划分为训练集、验证集和测试集。数据集可以以文本文件（如CSV、JSON等）或数据库的形式存储。
2. **RM（奖励模型）的数据格式？**
    - RM的数据集格式包括输入数据和奖励数据。输入数据是一个文本序列，可以是字符串或tokenized的文本序列。奖励数据是与输入数据对应的奖励或评分，可以是一个实数值或离散的标签。数据集可以以文本文件（如CSV、JSON等）或数据库的形式存储。
3. **PPO（强化学习）的数据格式？**
    - PPO的数据格式涉及从特定数据集划分出来的prompt数据，作为ppo训练时的环境交互数据。
4. **找数据集哪里找？**
    - 可以在Google Dataset、SelectDataset、HuggingFace、Kaggle等平台找到数据集。
5. **微调需要多少条数据？**
    - 微调所需数据量取决于具体任务和模型复杂度，但通常使用1000条数据作为微调数据与验证数据。
6. **有哪些大模型的训练集？**
    - 大模型的训练集包括CEval的中文选择题、GSM8K的小学数学题目，以及GLUE、OpenHermes等用于语言理解和对话能力测试的数据集。
7. **进行领域大模型预训练应用哪些数据集比较好？**
    - 领域大模型预训练可以应用Skypile-150B等大型中文语言模型预训练数据集，这些数据集取自公开可用的中文互联网网页数据，并经过严格的过滤和广泛的重复数据删除。

## 大模型（LLMs）显存问题面

### 问题篇

1. 大模型大概有多大，模型文件有多大?
2. 能否用4 * v100 32G训练vicuna 65b？
3. 如果就是想要试试65b模型，但是显存不多怎么办？
4. nB模型推理需要多少显存？
5. nB模型训练需要多少显存？
6. 如何 估算模型所需的RAM？
7. 如何评估你的显卡利用率?
8. 测试你的显卡利用率 

A:

1. **大模型大概有多大，模型文件有多大?** 大模型的大小通常与其参数量有关。例如，一个13B的模型参数量为130亿。大模型通常以半精度存储，因此一个Xb的模型文件大约是2X GB多一些，例如13b的模型文件大小大约是27GB左右。
2. **能否用4 * v100 32G训练vicuna 65b？** 不能。首先，llama 65b的权重需要5* v100 32G才能完整加载到GPU。其次，vicuna使用flash-attention加速训练，暂不支持v100，需要turing架构之后的显卡。
3. **如果就是想要试试65b模型，但是显存不多怎么办？** 如果显存不足，可以尝试使用LoRA等技术来降低显存需求。最少大概50g显存，可以在llama-65b-int4（gptq）模型基础上LoRA。
4. **nB模型推理需要多少显存？** 推理所需显存与模型参数量有关。一个粗略的计算方法是，每10亿个参数，占用4G显存。如果使用半精度的FP16/BF16来加载，所需显存可以降为一半。
5. **nB模型训练需要多少显存？** 训练时的显存需求通常比推理时高。一般来说，推理模型需要的显存约等于模型文件大小，全参训练需要的显存约为推理所需显存的三倍到四倍。
6. **如何估算模型所需的RAM？** 估算模型所需的RAM需要考虑模型参数量、精度（如fp32、fp16、int8）以及batch_size等因素。例如，fp32精度下，一个参数需要4个字节，而fp16精度下，一个参数需要2个字节。
7. **如何评估你的显卡利用率?** 评估显卡利用率的方法包括：1）flops比值法，通过实测的flops与显卡理论上的峰值flops计算利用率；2）throughout估计法，根据实际吞吐量与论文中的吞吐量计算利用率；3）torch profiler分析法，利用torch profiler记录各个函数的时间并分析GPU利用率。
8. **测试你的显卡利用率** 可以使用nvidia-smi命令或DeepSpeed Flops Profiler等工具来测试显卡利用率。nvidia-smi命令可以显示GPU的温度、功耗和利用率等实时指标。

### 实现细节篇

1. 如何查看多机训练时的网速？
2. 如何查看服务器上的多卡之间的NVLINK topo？
3. 如何查看服务器上显卡的具体型号?
4. 如何查看训练时的flops？（也就是每秒的计算量）
5. 如何查看对deepspeed的环境配置是否正确？
6. tf32格式有多长？
7. 哪里看各类显卡算力比较？
8. （torch profiler）如何查看自己的训练中通信开销？

A:

1. **如何查看多机训练时的网速？** 可以使用`iftop`命令来监控多机训练时的网速。`iftop`可以监控发送流量、接收流量、总流量，以及运行`iftop`到目前时间的总流量、流量峰值和过去2s、10s、40s的平均流量。
2. **如何查看服务器上的多卡之间的NVLINK topo？** 可以使用`nvidia-smi topo --matrix`命令来查看服务器上多卡之间的NVLINK拓扑。
3. **如何查看服务器上显卡的具体型号?** 可以通过运行`cd /usr/local/cuda/samples/1_Utilities/deviceQuery`，然后执行`make`和`./deviceQuery`来查看服务器上显卡的具体型号。
4. **如何查看训练时的flops？（也就是每秒的计算量）** 如果基于DeepSpeed训练，可以通过配置文件来测试flops。配置文件示例如下：
    ```text
    {
      "flops_profiler": {
        "enabled": true,
        "profile_step": 1,
        "module_depth": -1,
        "top_modules": 1,
        "detailed": true,
        "output_file": null
      }
    }
    ```
    这将启用flops分析器并输出相关数据。
5. **如何查看对deepspeed的环境配置是否正确？** 可以使用DeepSpeed环境报告（通过`ds_report`或`python -m deepspeed.env_report`）来验证安装并查看机器兼容哪些操作。此报告在调试环境中非常有用。
6. **tf32格式有多长？** TF32（TensorFloat32）是一种截短的Float32数据格式，总长度为19位（1位符号位，8位指数位，10位尾数位）。
7. **哪里看各类显卡算力比较？** 可以查看专门的算力参数详解及国内外算力整理文章，例如在CSDN博客上查看不同显卡的详细数据。
8. **（torch profiler）如何查看自己的训练中通信开销？** 使用PyTorch Profiler可以记录CPU操作时间、CUDA内核计时和内存消耗历史。通过将训练嵌入到分析器上下文中，并启动tensorboard查看分析轨迹，可以分析训练中的通信开销。

## WIP：LLM分布式训练面

### [大模型（LLMs）分布式训练面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ah2ibj3z22c7.html)

1. 理论篇

- 1.1 训练 大语言模型 存在问题？
- 1.2 什么是 [点对点通信](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E7%82%B9%E5%AF%B9%E7%82%B9%E9%80%9A%E4%BF%A1&zhida_source=entity)？
- 1.3 什么是 集体通信？
- 1.4 什么是 [数据并行](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C&zhida_source=entity)？
- 1.5 数据并行 如何 提升效率？
- 1.6 什么是 流水线并行？
- 1.7 什么是 张量并行 (intra-layer)？
- 1.8 数据并行 vs 张量并行 vs 流水线并行?
- 1.9 什么是 3D并行？
- 1.10 想要训练1个LLM，如果只想用1张显卡，那么对显卡的要求是什么？
- 1.11 如果有N张显存足够大的显卡，怎么加速训练？
- 1.12 如果显卡的显存不够装下一个完整的模型呢？
- 1.13 PP推理时，是一个串行的过程，1个GPU计算，其他空闲，有没有其他方式？
- 1.14 3种[并行方式](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E5%B9%B6%E8%A1%8C%E6%96%B9%E5%BC%8F&zhida_source=entity)可以叠加吗？
- 1.15 Colossal-AI 有1D/2D/2.5D/3D，是什么情况？
- 1.16 除了3D并行有没有其他方式大规模训练？
- 1.17 有了ZeRO系列，为什么还需要3D并行？
- 1.18 平民适不适合玩3D并行？
- 1.19 平民适不适合直接上多机多卡的ZeRO3（万兆网）？
- 1.20 分布式并行及显存[优化技术](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF&zhida_source=entity)并行技术有哪一些，都有什么特点？
- 1.21 显存优化技术有哪一些，都有什么特点？
- 1.22 常见的分布式训练框架哪一些，都有什么特点？

1. 实践篇

- 2.1 假如有超多的8卡A100节点（DGX A100），如何应用3D并行策略？
- 2.2 如果想构这样一个大规模并行训练系统，训练框架如何选？
- 2.3 训练框架如何选？

1. 并行化策略选择篇

- 3.1 如何选择一款分布式训练框架？
- 3.2 如何选择一款分布式训练框架？
- 3.3 单GPU
- 3.4 单节点多卡
- 3.5 多节点多卡

1. 问题篇

- 4.1 推理速度验证
- 4.2 并行化训练加速
- 4.3 deepspeed 训练过程，报找不主机
- 4.4 为什么 多机训练效率不如单机？
- 4.5 多机训练不通，DeepSPeed配置问题
  
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ah2ibj3z22c7.html)

### [图解分布式训练（一） —— 流水线并行（Pipeline Parallelism）面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ah2ibj3z22c7.html)

- 为什么需要流水线并行（Pipeline Parallelism）？
- 一、流水线并行（Pipeline Parallelism） 优化目标是什么？
- 二、图解 流水线并行（Pipeline Parallelism）模型并行 必要性？
- 三、流水线并行（Pipeline Parallelism） 图解？
- 四、流水线并行（Pipeline Parallelism）优缺点？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_wre1eni0oq7d.html)

### [图解分布式训练（二） —— nn.DataParallel面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ah2ibj3z22c7.html)

- 为什么需要nn.DataParallel？
- 一、pytorch中的GPU操作默认是什么样？
- 二、介绍一下 nn.DataParallel 函数？
- 三、nn.DataParallel 函数 处理逻辑 介绍一下？
- 四、nn.DataParallel 函数 常见问题及解答 有哪些？

- 4.1 多GPU计算减少了程序运行的时间？
- 4.2 如何保存和加载多GPU训练模型呢？
- 4.3 为什么第一块卡的显存会占用的更多一些？
- 4.4 直接使用nn.DataParallel的时候，训练采用多卡训练，会出现一个warning？
- 4.5 device_ids 0 被占用问题

- 五、nn.DataParallel 函数 参数更新方式 ？
- 六、nn.DataParallel 函数 优点 介绍一下？
- 七、nn.DataParallel 函数 缺点 介绍一下？
- 八、nn.DataParallel 函数 实战？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_wre1eni0oq7d.html)

### [图解分布式训练（三） —— nn.parallel.DistributedDataParallel](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_i4s3ia057rmh.html)

- 为什么需要 nn.parallel.DistributedDataParallel ？
- 一、什么是 DistributedDataParallel 核心 —— Ring-AllReduce？
- 二、nn.parallel.DistributedDataParallel 函数 介绍一下？
- 三、nn.parallel.DistributedDataParallel 函数 如何多卡加速训练？
- 四、nn.parallel.DistributedDataParallel 实现流程介绍一下？
- 五、nn.parallel.DistributedDataParallel 参数更新介绍一下？
- 六、nn.DataParallel(以下简称DP) vs DistributedDataParallel(以下简称DDP)介绍一下？
- 七、DistributedDataParallel(以下简称DDP) 优点有哪些？
- 八、DistributedDataParallel(以下简称DDP) 缺点有哪些？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_i4s3ia057rmh.html)

### [图解分布式训练（四） —— torch.multiprocessing 详细解析](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_gu9smpbn510e.html)

- 一、torch.multiprocessing 函数介绍一下？
- 二、torch.multiprocessing 函数如何使用？
- 三、介绍一下 共享CUDA张量？
- 四、介绍一下 共享策略？
- 五、torch.multiprocessing 函数使用
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_gu9smpbn510e.html)

### [图解分布式训练（五） —— AMP混合精度训练 详细解析](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_0slrgoti6gvb.html)

- 为什么需要 AMP混合精度训练？
- 一、什么是自动混合精度训练(AMP)
- 二、为什么需要自动混合精度？
- 三、混合精度训练的优点是什么？
- 四、混合精度训练的缺点是什么？
- 五、混合精度训练的关键技术是什么？
- 六、介绍一下 混合精度训练 动态损失缩放？
- 七、如何在PyTorch中使用自动混合精度？
- 八、如何使用 AMP混合精度训练 ？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_0slrgoti6gvb.html)

### [图解分布式训练（六） —— Pytorch的 DeepSpeed 详细解析](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_2v6wv29ce8nn.html)

- 一、为什么需要 Deepspeed？
- 二、DeepSpeed 基本概念 介绍一下？
- 三、DeepSpeed 通信策略 介绍一下？
- 四、DeepSpeed 如何使用？
- 五、DeepSpeed 代码实现？
- 七、训练精度 介绍一下？
- 八、获取模型参数 介绍一下？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_2v6wv29ce8nn.html)

### [图解分布式训练（七）—— accelerate 分布式训练 详细解析](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_o5wkeionnqr7.html)

- 一、为什么需要 accelerate 分布式训练？
- 二、什么是 accelerate 分布式训练?
- 三、accelerate 分布式训练 原理讲解？
- 四、accelerate 分布式训练 如何实践？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_o5wkeionnqr7.html)

### [图解分布式训练（八）—— ZeRO 学习](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_600z63vou4nj.html)

- 一、什么是 3D 并行？
- 二、3D 并行 策略有哪些？
- 三、为什么需要 ZeRO？
- 四、ZeRO 的 核心思想是什么？
- 五、ZeRO 显存如何分配？
- 六、ZeRO 优化策略是怎么样？
- 七、ZeRO Offload后的计算流程是怎么样？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_600z63vou4nj.html)

## [大模型（LLMs）agent 面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_9dfwi0ooio2z.html)

1. 如何给LLM注入领域知识？
2. 如果想要快速体验各种模型，该怎么办？

A：

1. 如何给LLM注入领域知识？
- **长文本微调**：通过在预训练的语言模型基础上，使用包含长文本示例的数据集对模型进行进一步训练，以适应处理长文本输入的任务。
- **知识库结合**：将外部知识库中的信息与大型语言模型相结合。
- **数据增强**：在训练过程中，通过添加领域相关的数据来增强模型的训练数据。
- **微调（SFT）与检索（RAG）**：这两种方法可以用来给LLM注入知识。微调通过在特定领域数据上继续训练模型来注入知识，而检索方法通过构建知识库并在回答问题时检索相关信息来增强模型的知识。
2. 如果想要快速体验各种模型，该怎么办？
- **ChatDev IDE**：这是一个用于构建智能体的集成开发环境，支持游戏模式和聊天模式，允许用户在聊天模式中选择不同的大模型进行体验。
- **XEduLLM**：通过创建大模型对话页面，用户可以快速体验不同的大模型。基础代码简单，可以通过浏览器访问Web服务或通过代码调用大模型服务。
- **Dify平台**：使用Dify平台结合大模型和Agent技术，可以快速构建和体验智能小助手。

## [Token及模型参数准备篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_9oplu4014qx5.html)

1. 预训练数据 Token 重复 是否影响 模型性能？
2. SFT需要训练Token数？

A:

1. 影响，会导致性能下降，产生过拟合风险。可通过增大数据集规模，减少重复模式，采用正则化技术等方法解决。
2. ？？

## [LLMs 位置编码篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_bmn80nar12c7.html)

- 1 什么是位置编码？
- 2 什么是绝对位置编码？
- 3 什么是相对位置编码？
- 4 旋转位置编码 RoPE篇

- 4.1 旋转位置编码 RoPE 思路是什么？
- 4.2 推导一下 旋转位置编码 RoPE ？
- 4.3 旋转位置编码 RoPE 有什么优点？
- 4.4 旋转位置编码 RoPE 被哪些 LLMs 应用？

- 5 长度外推问题篇

- 5.1 什么是 长度外推问题？
- 5.2 长度外推问题 的 解决方法 有哪些？

- 6 ALiBi (Attention with Linear Biases)篇

- 6.1 ALiBi (Attention with Linear Biases) 思路是什么？
- 6.2 ALiBi (Attention with Linear Biases) 的偏置矩阵是什么？有什么作用？
- 6.3 ALiBi (Attention with Linear Biases) 有什么优点？
- 6.4 ALiBi (Attention with Linear Biases) 被哪些 LLMs 应用？

*Answer:*

1. 什么是位置编码？

位置编码是一种用于表示序列中每个token的位置信息的向量，通常使用周期函数（如sin，cos）来生成。它在Transformer模型中非常重要，用于在自注意力机制中引入位置信息。

2. 什么是绝对位置编码？

绝对位置编码（Absolute Positional Encoding）是最常见的一种位置编码方法，其思想是在每个输入序列的元素上添加一个位置向量，以表示该元素在序列中的具体位置。这个位置向量通常通过固定的函数生成，与输入数据无关，通常使用的是正弦和余弦函数。

3. 什么是相对位置编码？

相对位置编码（Relative Positional Encoding）是一种根据位置之间的相对关系来编码序列的方法。相对位置编码考虑了序列中不同位置之间的相对距离和关系，并使用可学习的参数来对这些关系进行建模。相对于绝对位置编码，相对位置编码更关注序列中位置之间的相对顺序和距离，它可以更好地处理长序列中的位置信息。

4. 旋转位置编码 RoPE篇

4.1 旋转位置编码 RoPE 思路是什么？

旋转位置编码（RoPE）是一种新型的位置编码方法，专为Transformer架构设计。它通过引入旋转矩阵，将位置信息直接嵌入到词向量中，以增强模型对位置信息的敏感性。

4.2 推导一下 旋转位置编码 RoPE？

RoPE的推导涉及到将位置信息通过旋转矩阵嵌入到词向量中。具体来说，RoPE利用正弦和余弦函数的周期性特性，通过旋转操作将位置信息编码到词向量中，从而使得模型能够更好地捕捉序列中的相对位置信息。

4.3 旋转位置编码 RoPE 有什么优点？

RoPE的优点在于它能够有效地将相对位置信息集成到self-attention中，并提升Transformer架构的性能。RoPE在处理长序列和复杂依赖关系时表现优异，能够捕捉到更为细致的序列信息。

4.4 旋转位置编码 RoPE 被哪些 LLMs 应用？

RoPE被应用于多种大语言模型中，如Meta的LLaMA和清华的ChatGLM，这证明了RoPE的优势。

5. 长度外推问题篇

5.1 什么是 长度外推问题？

长度外推问题是Transformer模型及其衍生的大语言模型（LLMs）在处理超过训练时预设上下文长度限制的序列时遇到的挑战。由于模型在训练时受到上下文长度的限制，因此在推理时处理更长序列的能力受限。

5.2 长度外推问题 的 解决方法 有哪些？

解决长度外推问题的方法主要包括位置插值和随机化位置编码。位置插值方法通过在推理时对位置编码进行缩放，使得原本超出模型训练长度的位置编码在插值后落入已训练位置区间。随机化位置编码通过在训练期间引入随机位置来将预训练的上下文窗口与较长的推理长度解耦，从而提高了较长上下文窗口中所有位置的曝光度。

6. ALiBi (Attention with Linear Biases)篇

6.1 ALiBi (Attention with Linear Biases) 思路是什么？

ALiBi（Attention with Linear Biases）是一种简化的高效位置编码方法，它不向词嵌入中添加位置嵌入，而是通过在线性偏置中引入位置信息来实现位置编码。

6.2 ALiBi (Attention with Linear Biases) 的偏置矩阵是什么？有什么作用？

ALiBi的偏置矩阵是一个线性递减的惩罚项，它与注意力分数成比例。这种偏置矩阵的作用是在线性偏置中引入位置信息，从而实现位置编码。

6.3 ALiBi (Attention with Linear Biases) 有什么优点？

ALiBi的优点在于其简单性和高效性。它不需要在输入层添加位置嵌入，而是通过在线性偏置中引入位置信息来实现位置编码，这种方法更快速且占用内存更少。

6.4 ALiBi (Attention with Linear Biases) 被哪些 LLMs 应用？

ALiBi被应用于多种大语言模型中，以增强模型的长度外推能力。

## LLMs Tokenizer 篇

### [LLMs Tokenizer 篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_c1wrizv0im1a.html)

- Byte-Pair Encoding(BPE)篇

- 1 Byte-Pair Encoding(BPE) 如何构建词典？

- WordPiece 篇

- 1 WordPiece 与 BPE 异同点是什么？

- SentencePiece 篇

- 简单介绍一下 SentencePiece 思路？

- 对比篇

- 1 举例 介绍一下 不同 大模型LLMs 的分词方式？
- 2 介绍一下 不同 大模型LLMs 的分词方式 的区别？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_c1wrizv0im1a.html)

### [怎么让英文大语言模型支持中文？（一） —— 构建中文tokenization](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_w0d2q29sueq7.html)

- 一、为什么需要 构建中文tokenization？
- 二、如何对 原始数据预处理？
- 三、如何构建中文的词库？
- 四、如何使用transformers库加载sentencepiece模型？
- 五、如何合并英文词表和中文词表？
- 六、怎么使用修改后的词表？
- 总结一下 构建中文tokenization？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_w0d2q29sueq7.html)

### [怎么让英文大语言模型支持中文？（二） —— 继续预训练篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_jprkwhrvf3tw.html)

- 一、为什么需要进行继续预训练？
- 二、如何对 继续预训练 数据预处理？
- 三、如何 构建模型？
- 四、如何 使用模型？

### [怎么让英文大语言模型支持中文？（三） —— 对预训练模型进行指令微调](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_p2wj7zadwxwb.html)

- 一、为什么需要对预训练模型进行指令微调？
- 二、对预训练模型进行指令微调 数据 如何处理？
- 三、对预训练模型进行指令微调 tokenization 如何构建？
- 四、对预训练模型进行指令微调 模型 如何构建？
- 五、是否可以结合 其他库 使用？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_p2wj7zadwxwb.html)

## [Layer normalization 篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_pzcgd4ovk098.html)

- Layer normalization-方法篇

- Layer Norm 篇

- Layer Norm 的计算公式写一下？

- RMS Norm 篇 （[均方根](https://zhida.zhihu.com/search?content_id=234582323&content_type=Article&match_order=1&q=%E5%9D%87%E6%96%B9%E6%A0%B9&zhida_source=entity) Norm）

- RMS Norm 的计算公式写一下？
- RMS Norm 相比于 Layer Norm 有什么特点？

- Deep Norm 篇

- Deep Norm 思路？
- 写一下 Deep Norm 代码实现？

- Deep Norm 有什么优点？

- Layer normalization-位置篇

- 1 LN 在 LLMs 中的不同位置 有什么区别么？如果有，能介绍一下区别么？

- Layer normalization 对比篇

- LLMs 各模型分别用了 哪种 Layer normalization？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_pzcgd4ovk098.html)

## [LLMs 激活函数篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_6xm3wzzice2s.html)

- 1 介绍一下 FFN 块 计算公式？
- 2 介绍一下 GeLU 计算公式？
- 3 介绍一下 Swish 计算公式？
- 4 介绍一下 使用 GLU 线性门控单元的 FFN 块 计算公式？
- 5 介绍一下 使用 GeLU 的 GLU 块 计算公式？
- 6 介绍一下 使用 Swish 的 GLU 块 计算公式？
- 各LLMs 都使用哪种激活函数？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_6xm3wzzice2s.html)

## [LLMs 激活函数篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_6xm3wzzice2s.html)

- 1 介绍一下 FFN 块 计算公式？
- 2 介绍一下 GeLU 计算公式？
- 3 介绍一下 Swish 计算公式？
- 4 介绍一下 使用 GLU 线性门控单元的 FFN 块 计算公式？
- 5 介绍一下 使用 GeLU 的 GLU 块 计算公式？
- 6 介绍一下 使用 Swish 的 GLU 块 计算公式？
- 各LLMs 都使用哪种激活函数？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_6xm3wzzice2s.html)

## 大模型（LLMs）加速篇

### [大模型（LLMs）加速篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_w9wewc152eux.html)

1. 当前优化模型最主要技术手段有哪些？
2. 推理加速框架有哪一些？都有什么特点？

- 3 vLLM 篇

- 3.1 vLLM 的 功能有哪些？
- 3.2 vLLM 的 优点有哪些？
- 3.3 vLLM 的 缺点有哪些？
- 3.4 vLLM 离线批量推理？
- 3.5 vLLM API Server？

- 4 Text generation inference 篇

- 4.1 介绍一下 Text generation inference？
- 4.2 Text generation inference 的 功能有哪些？
- 4.3 Text generation inference 的 优点有哪些？
- 4.4 Text generation inference 的 缺点有哪些？
- 4.5 Text generation inference 的 使用docker运行web server？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_w9wewc152eux.html)

### [LLM（大语言模型）部署加速方法——PagedAttention篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_p22mjq881n3n.html)

- 一、vLLM 用于大模型并行推理加速 存在什么问题？
- 二、vLLM 如何 优化 大模型并行推理加速？
- 三、什么是 PagedAttention？
- 四、 PagedAttention 如何存储 连续的key和value？
- 五、 PagedAttention 技术细节？
- 六、 PagedAttention 如何 实现安全共享？
- 七、 PagedAttention 源码介绍？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_p22mjq881n3n.html)

### [大模型推理加速工具 —— vLLM](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_zw5h9ogvac2w.html)

- 一、引言

- 1.1 前言
- 1.2 为什么 需要 vLLM ?
- 1.3 vLLM 具有哪些特点 ?
- 1.4 vLLM 支持哪些 Huggingface 模型 ?

- 二、vLLM 性能如何？
- 三、vLLM 依赖包
- 四、vLLM 如何安装？
- 五、vLLM 如何使用？
- 六、vLLM 分布式推理与服务
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_zw5h9ogvac2w.html)

### [LLM（大语言模型）部署加速方法——Faster Transformer篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_dd2gowztxtfg.html)

- 一、为什么需要 FasterTransformer？
- 二、FasterTransformer 介绍一下？
- 三、FasterTransformer 核心是什么？
- 四、FasterTransformer 优化？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_dd2gowztxtfg.html)

### [纯Python超轻量高性能LLM推理框架 —— LightLLM](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_9a643feq2b0b.html)

- 一、引言

- 1.1 前言
- 1.2 为什么 需要 LightLLM ?
- 1.3 目前 LLM推理框架 有 哪些?

- 二、LightLLM 介绍一下？

- 2.1 什么是 LightLLM ？
- 2.2 Token Attention 介绍？
- 2.3 Efficient Router 介绍？

- 三、LightLLM 性能表现 介绍？
- 四、LightLLM 依赖包 有哪些？
- 五、LightLLM 如何安装？

- 5.1 下载 LightLLM
- 5.2 安装 LightLLM 依赖
- 5.3 安装 LightLLM

- 六、LightLLM 如何使用？

- 6.1 启动 LightLLM 服务

- 填坑笔记

- LightLLM 支持模型 LLMs 模型？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_9a643feq2b0b.html)

## [Attention 升级面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_j0nwuo0frw2x.html)

- 1 传统 Attention 存在哪些问题？
- 2 Attention 优化方向
- 3 Attention 变体有哪些？
- 4 Multi-Query Attention 篇

- 4.1 Multi-head Attention 存在什么问题？
- 4.2 介绍一下 Multi-Query Attention？
- 4.3 对比一下 Multi-head Attention 和 Multi-Query Attention？
- 4.4 Multi-Query Attention 这样做的好处是什么？
- 4.5 有 哪些模型 是 使用 Multi-Query Attention？

- 5 Grouped-query Attention

- 5.1 什么是 Grouped-query Attention？
- 5.2 有哪些大模型使用 Grouped-query Attention？

- 6 FlashAttention 介绍一下
- 7 并行 transformer block 介绍一下？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_j0nwuo0frw2x.html)

## LLM幻觉面

### [大模型幻觉（LLM Hallucination）面](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_schwrdmvmhr7.html)

- 一、什么是大模型幻觉？
- 二、为什么LLM会产生幻觉？
- 三、为什么需要解决LLM的幻觉问题？
- 四、幻觉一定是有害的吗？
- 五、幻觉有哪些不同类型？
- 六、如何度量幻觉？
- 七、如何缓解LLM幻觉？

- 7.1 通过使用外部知识验证主动检测和减轻幻觉
- 7.2 事实核心采样
- 7.3 SelfCheckGPT

- 八、LLMs什么时候最容易产生幻觉？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_schwrdmvmhr7.html)

### [大模型的幻觉问题篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_8mr4mlhe5q1x.html)

- 一、什么是 大模型幻觉问题？
- 二、为什么 会 出现 大模型幻觉问题？
- 三、如何 评估 大模型幻觉问题？
- 四、如何 缓解 大模型幻觉问题？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_8mr4mlhe5q1x.html)

### [大模型的幻觉问题篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_tbezgzifowzp.html)

- 一、为什么 会 出现 大模型幻觉？
- 二、如何 缓解 大模型幻觉？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_tbezgzifowzp.html)

## LLMs对比篇

### [LLMs 对比篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_tbezgzifowzp.html)

- LLMs 训练数据 和 数据量 对比如何？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_tbezgzifowzp.html)

### [百川智能baichuan7B、13B、53B、baichuan2 总结篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ma6pw7v2g9pi.html)

- 一、baichuan-7B篇

1. 你了解baichuan-7B解构么？介绍一下？
2. baichuan-7B 如何 收集原始数据并 构建 训练数据？
3. baichuan-7B 如何 提高 训练稳定性和吞吐？

- 二、baichuan-13B篇

1. 相比于 baichuan-7B，baichuan-13B 的 特点体现在哪里？
2. 如何 对 baichuan-13B 进行推理和部署？
3. 如何 对 baichuan-13B 进行微调？

- 三、baichuan-53B篇

- 3.1 baichuan-53B 相比于 baichuan-7B 和 baichuan-13B 有哪些优势？
- 3.2 baichuan-53B 如何对 预训练数据 做处理？
- 3.3 baichuan-53B 如何进行 搜索增强？

- 四、baichuan2篇

- 4.1 baichuan2 与 其他大模型 对比

- 五、baichuan 数据构建篇

- 5.1 baichuan 进行微调时，领域数据：通用数据配比？

- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_ma6pw7v2g9pi.html)

## 思维链 Chain-of-Thought（COT）篇

### [思维链 Chain-of-Thought（COT）篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_1cjjxf95az70.html)

- 一、什么是思维链提示？
- 二、思维链提示本质是什么？
- 三、思维链提示 与 标准的提示学习方法有什么不同?
- 四、思维链提示 为什么可以提高语言模型的复杂推理能力?它的优势在哪里?
- 五、思维链提示 适用场景 有 哪些？
- 六、思维链提示 目前还存在哪些不足点？
- 七、思维链提示 对推动语言模型复杂推理能力研究有哪些启发和影响?
- 八、思维链提示 对实现真正的通用人工智能仍面临哪些挑战?
- 九、如何通过增加模型规模来获得语言模型强大的思路链推理能力的?这与模型获得的哪些能力有关?
- 十、你认为可以在哪些其他方面应用“思路链提示”这一思路来提升语言模型的能力?
- 十一、如果需要你对 思维链提示 进行改进，你觉得你会改进哪些地方？
- 十二、思维链提示 未来研究方向？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_1cjjxf95az70.html)

### [思维链 Chain-of-Thought（COT）变体篇](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_sw5aljfzswiv.html)

- 思维链 Chain-of-Thought（COT）：思维链的启蒙

1. 什么是 思维链 Chain-of-Thought（COT）？
2. 思维链 Chain-of-Thought（COT）是思路是什么？
3. 思维链 Chain-of-Thought（COT）存在问题？

- 思维树 Tree of Thoughts（TOT）：一种用树结构解决复杂问题的方法

1. 为什么需要 思维树 Tree of Thoughts（TOT）？
2. 什么是 思维树 Tree of Thoughts（TOT）？
3. 思维树 Tree of Thoughts（TOT）涉及问题有哪些？

- 思维图 Graph of Thoughts（GOT）：一种把思维链过程建模层图结构的方法

1. 为什么 需要 思维图 Graph of Thoughts（GOT）？
2. 什么是 思维图 Graph of Thoughts（GOT） ？
3. 思维图 Graph of Thoughts（GOT）核心思想是什么 ？

- 思维算法 Algorithm of Thoughts（AOT）：一种用DFS/BFS示例解决问题的方法

1. 为什么 需要 思维算法 Algorithm of Thoughts（AOT）？
2. 思维算法 Algorithm of Thoughts（AOT）思路是什么？
3. 思维算法 Algorithm of Thoughts（AOT） vs 其他 COT 的 区别？

- 思维链 Chain-of-Thought（COT） 有哪些 应用场景？
- 思维链 Chain-of-Thought（COT） 有哪些 局限性？
- [点击查看答案](https://link.zhihu.com/?target=https%3A//articles.zsxq.com/id_sw5aljfzswiv.html)

## [思维链 Chain-of-Thought（COT）变体篇](https://link.zhihu.com/?target=https%3A//file%2B.vscode-resource.vscode-cdn.net/f%3A/llms_interview_notes/llms_interview_notes_gitee/hhttps%3A/articles.zsxq.com/id_dwhonmw976n7.html)

- 一、为什么需要 Graph RAG？
- 二、什么是 Graph RAG？
- 三、Graph RAG 思路介绍？
- 四、用代码 介绍 Graph RAG ？
- 五、用 示例 介绍 Graph RAG ？
- 六、Graph RAG 排序优化方式？

## 豆包面经-1

src: [link](https://blog.csdn.net/Python_cocola/article/details/141143099)

1. *介绍和大模型相关项目*

~~我不是，我没有，别骂了~~

WIP

2. *介绍rope和位置外推*

RoPE（Rotary Position Embedding）简介

RoPE（Rotary Position Embedding）是一种用于Transformer模型的**位置编码策略**。它通过旋转操作将相对位置信息集成到self-attention机制中，从而提升Transformer架构的性能。RoPE广泛应用于大型模型，如LLaMA和ChatGLM等。RoPE的一个显著特点是其外推性，即在训练时和预测时输入长度不一致的情况下，RoPE能够较好地维持模型的泛化能力。

位置外推简介

位置外推是指在不需要对模型进行额外训练的情况下，使模型能够处理更长的序列。这种技术对于大模型尤其重要，因为它们通常在较小的上下文长度中进行训练，而在推理时可能需要处理更长的文本。位置外推技术包括基于RoPE的位置插值、NTK-aware、动态NTK等方法。这些方法通过不同的缩放策略来优化模型的外推能力。

RoPE的位置外推能力通过调整旋转角度来实现，这种方法允许模型在预训练长度之外取得更好的效果。

3. *sft阶段有什么重要的技术*

在大模型的指令微调阶段，有几种重要技术：
1. **指令微调（Instruction Tuning）**：这是一种针对大型预训练语言模型的微调技术，其核心目的是增强模型理解和执行特定指令的能力，使模型能够根据用户提供的自然语言指令准确执行任务。
2. **混合微调与压缩技术（Hybrid Fine-Tuning and Compression）**：未来的趋势是将微调和压缩技术相结合，在微调的同时进行模型压缩，例如结合微调与蒸馏或剪枝。
3. **强化微调（Reinforcement Fine-Tuning）**：与传统的微调相比，强化微调可以让开发者使用经过微调的模型进行更复杂的推理和任务执行。
4. **参数高效微调（PEFT, Parameter-Efficient Fine Tuning）**：这种方法只对部分的参数进行训练，主要有Prompt Tuning、Prefix Tuning、LoRA、QLoRA等方法，旨在降低计算和存储成本。
5. **高效指令微调技术**：为了降低计算成本，研究者们提出了多种高效微调方法，如LoRA、QLoRA、LOMO等，可以在较小的计算资源下实现指令微调。
6. **多阶段指令数据微调**：这种策略首先使用大规模NLP任务指令数据对模型进行微调，然后再使用相对多样的日常对话指令和合成指令进一步微调，以避免能力遗忘问题。

4. *为什么需要RLHF，能解决什么SFT解决不了的问题*

**RLHF**（Reinforcement Learning from Human Feedback）是大模型训练中的一种重要技术，特别是在生成式模型（如GPT类模型）的训练过程中，它可以解决**SFT**（Supervised Fine-Tuning）无法解决的一些问题。两者的核心区别在于训练目标和反馈来源：

**SFT的局限性：**
- **数据依赖：** SFT依赖于人工标注的数据集，这些数据集通常是静态的、有限的且可能无法覆盖所有可能的用户需求和场景。SFT的优化目标通常是基于传统的监督学习方法，通过已标注的训练样本调整模型参数。
- **不可捕捉长尾需求：** 由于人工标注的样本有限，SFT训练的模型可能无法应对很多未知场景或用户个性化需求，尤其是一些长尾问题和复杂的语境。

**RLHF的优势：**
- **动态反馈：** RLHF通过强化学习和人类反馈相结合的方式，可以动态调整模型的行为，而不仅仅依赖于静态的标注数据。这使得模型可以通过互动学习逐步改善其输出，尤其是在面对用户的实时反馈时。
- **优化行为而非仅仅是预测：** 在RLHF中，模型不仅仅是根据输入生成输出，而是学习如何优化行为，以最大化用户的满意度或任务的成功率。通过这种方法，模型的目标可以更加灵活和人性化，而不仅仅是预测“正确的答案”。
- **处理复杂多样的任务：** RLHF可以处理更多复杂的、含糊不清的任务，因为它允许模型通过逐步的尝试和错误来学习，而不需要预先定义所有可能的情况。这种方法尤其适合于那些没有明确标准答案、而是需要与用户进行互动和优化体验的场景。
- **反馈的个性化：** RLHF可以根据用户反馈不断调整模型，逐步适应特定用户或用户群体的需求，而SFT则难以做到这种层面的个性化优化。

**RLHF能解决SFT无法解决的问题：**
- **任务自适应：** RLHF能通过人类反馈不断调整模型，使其能够应对更多动态、复杂或开放性的问题，而SFT只能处理有限的、预先定义好的问题。
- **优化策略：** RLHF不仅仅依赖于标注数据来进行误差反向传播，而是通过奖励机制来指导模型行为。这个机制可以使得模型在训练过程中主动探索并发现更符合需求的策略。
- **长尾问题：** SFT面临的一个问题是，它通常依赖于标注数据的覆盖度，不能很好地应对长尾需求。而RLHF能够通过实际的用户反馈不断弥补这一不足，尤其在面对复杂、个性化的任务时，RLHF能更好地收集反馈并逐步改进模型。

5. PPO和DPO区别

**PPO**（Proximal Policy Optimization）和**DPO**（Direct Preference Optimization）都是强化学习中的算法，它们在模型优化的方式和目标上有所不同，特别是在与**RLHF**（Reinforcement Learning from Human Feedback）结合使用时。下面是它们的主要区别：

**基本概念与背景：**
- **PPO（Proximal Policy Optimization）：** PPO是一种强化学习算法，属于策略优化方法。它的目标是通过在每次更新时限制策略的变动幅度，来避免策略变化过大导致训练不稳定。PPO通过引入一个“剪切”目标（clipped objective）来限制新旧策略之间的差距，确保在优化过程中不会做出过于激进的调整。
- **DPO（Direct Preference Optimization）：** DPO是针对人类反馈优化的一种方法，专门设计用于优化模型生成的输出，使其更符合用户的偏好。DPO直接优化用户反馈或偏好，而不是通过奖励信号或策略值函数来优化。DPO的一个重要特点是它不依赖于强化学习中的奖励函数，而是直接利用来自用户的比较偏好（即哪种输出更符合用户需求）来调整模型。

**优化方式：**
- **PPO：** PPO通过强化学习的策略梯度方法来优化策略。它通过最大化某个目标函数，进而改进策略，这个目标函数包括了奖励信号和概率比的剪切约束。PPO在每次优化时会对策略进行一定程度的限制，确保每次更新不偏离原始策略太远，从而保持训练的稳定性。
- **DPO：** DPO直接优化人类偏好，通常不依赖于传统的奖励函数，而是通过用户的反馈来优化模型输出。例如，在对话生成任务中，DPO会根据用户对不同生成文本的偏好来进行优化。DPO方法利用的是人类对模型行为的直接偏好（例如，哪个回答更合适或更有帮助），而不是间接的奖励信号。

**适用场景：**
- **PPO：** PPO广泛应用于强化学习任务，尤其是在需要直接优化行为策略的场景中。它适用于各种任务，从机器人控制到游戏AI等领域。在RLHF中，PPO通常用于模型通过与环境或用户互动来学习和优化其行为。
- **DPO：** DPO特别适用于**人类反馈优化**的场景。例如，在自然语言处理任务（如对话系统）中，DPO通过优化用户偏好来调整模型的输出，使其更加符合用户的需求。DPO也非常适合用于那些没有明确奖励信号的任务，而是依赖于用户偏好的情境。

**优化目标：**
- **PPO：** PPO的优化目标是最大化期望奖励函数，通常是根据策略在环境中的表现来评估奖励。PPO在更新过程中会通过剪切目标函数来限制每次策略变化的幅度，以避免过度调整。
- **DPO：** DPO的优化目标是最大化用户的偏好反馈。它直接基于用户的反馈来调整模型行为，使得模型生成的输出更加符合用户的期望，而不依赖于传统的奖励信号。

**模型更新与稳定性：**
- **PPO：** PPO引入了“重要性采样”技术和策略“剪切”，使得在优化过程中策略变化不会过于剧烈。这一方法强调训练的稳定性，因此适用于需要多次迭代和较大策略更新的任务。
- **DPO：** DPO通常会直接基于用户偏好进行优化，因此它的更新方式与PPO有所不同。DPO的核心是利用用户比较偏好信息来直接引导优化过程，通常不会依赖于策略的精细调整，而是根据每次用户反馈来更新模型。

**计算复杂度：**
- **PPO：** PPO在训练过程中涉及到样本的采集和更新策略的多个步骤，因此计算复杂度相对较高。它需要进行多轮的采样和梯度更新，尤其在复杂的环境下，这种计算复杂度可能会很大。
- **DPO：** DPO通常不需要像PPO那样进行大量的环境交互，而是基于用户提供的偏好信息来进行优化。因此，DPO在计算上可能更加高效，尤其是在没有环境交互时，只需要处理偏好数据。

总结：
- **PPO**更偏向于传统的强化学习方法，依赖于环境反馈和奖励信号来优化策略，通常用于优化行为策略。
- **DPO**则是一个直接优化模型输出以符合用户偏好的方法，特别适用于没有明确奖励信号而需要根据人类反馈调整模型行为的任务。

6. 介绍一下对你做的大模型领域的看法

??

7. 介绍你的研究领域的评估方法，有没有什么问题

??

8. 说几个你最近觉得有意思的论文

??

## 理想LLM面经-1

[src](https://blog.csdn.net/2301_78285120/article/details/137398052)

1. LLaMA和ChatGLM的区别

| 模型        | LLaMA          | ChatGLM-6B     |
| --------- | -------------- | -------------- |
| 训练数据      | 英语为主           | 中英1:1          |
| 训练数据量     | 1T/1.4T tokens | 1T tokens      |
| 模型参数量     | 7B/13B/33B/65B | 6B             |
| 词表大小      | 32K            | ~130K          |
| 模型结构      | causal decoder | prefix decoder |
| 位置编码      | RoPE           | RoPE           |
| 激活函数      | SwiGLU         | GeGLU          |
| LayerNorm | Pre RMS Norm   | Post Deep Norm |

2. BatchNorm VS LayerNorm

layernorm和batchnorm的区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；

BN中则针对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。

LN不依赖于batch的大小和输入sequence的深度，因此可以用于batchsize为1和RNN中对变长的输入sequence的normalize操作。

由于NLP中的文本输入一般为变长，所以使用layernorm更好。

3. BERT参数量是如何决定的

Bert（Bidirectional Encoder Representations from Transformers）的参数量由其**模型结构以及隐藏层的大小、层数等超参数**所决定。具体来说，Bert 模型由多个 Transformer Encoder 层组成，每个 Encoder 层包含多个注意力头以及前馈神经网络层。因此，Bert 的参数量主要由**这些层的数量、每层的隐藏单元数、注意力头的数量等**因素决定。

4. P-Tuning V2对V1的改进

Prompt tuning是之前其他论文提出的一种方法，通过冻结语言模型仅去调整连续的prompts，在参数量超过10B的模型上，效果追上了fine-tune，但是在normal-sized模型上表现不好，并且无法解决序列标注任务。针对这两个问题，作者提出了P-tuning v2。

P-Tuning V2在P-Tuning V1的基础上进行了下述改进：
- 在每一层都加入了Prompts tokens作为输入，而不是仅仅加在输入层，这与Prefix Tuning的做法相同。这样得到了更多可学习的参数，且更深层结构中的Prompt能给模型预测带来更直接的影响。
- 去掉了重参数化的编码器。在 P-tuning v2 中，作者发现重参数化的改进很小，尤其是对于较小的模型，同时还会影响模型的表现。
- 针对不同任务采用不同的提示长度。提示长度在提示优化方法的超参数搜索中起着核心作用。在实验中，我们发现不同的理解任务通常用不同的提示长度来实现其最佳性能，这与Prefix-Tuning中的发现一致，不同的文本生成任务可能有不同的最佳提示长度。
- 可选的多任务学习。先在多任务的Prompt上进行预训练，然后再适配下游任务。一方面，连续提示的随机惯性给优化带来了困难，这可以通过更多的训练数据或与任务相关的无监督预训练来缓解；另一方面，连续提示是跨任务和数据集的特定任务知识的完美载体。

5. 多头注意力机制和单个注意力机制时间复杂度会变吗？

多头注意力机制和单个注意力机制的时间复杂度都是$O(n^2d)$，其中 n 是序列长度，d 是每个词向量的维度。因为注意力机制涉及计算注意力分数的所有词对，因此时间复杂度与序列长度的平方成正比。无论是多头还是单个注意力机制，时间复杂度都是相同的。

6. 大模型微调过程中如何避免灾难性遗忘？

在微调大模型的过程中，确实可能会遇到灾难性遗忘的问题，即模型在优化某一特定任务时，可能会忘记之前学到的其他重要信息或能力。为了缓解这种情况，可以采用以下几种策略：
- 重新训练：通过使用所有已知数据重新训练模型，可以使其适应数据分布的变化，从而避免遗忘。
- 增量学习：增量学习是一种在微调过程中逐步添加新数据的方法。通过增量学习，大模型可以在不忘记旧知识的情况下学习新数据。
- 知识蒸馏：知识蒸馏是一种将老模型的知识传递给新模型的方法。通过训练一个**教师模型**来生成数据标注或权重，然后将标注或权重传递给新模型进行训练，可以避免灾难性遗忘。
- 正则化技术：限制模型参数的变化范围，从而减少遗忘，使得大模型在微调过程中保持稳定性。
- 使用任务相关性数据：如果可能的话，尽量使用与原始任务相关或相似的数据进行微调。这样，模型在优化新任务时，更容易与先前学到的知识建立联系。

