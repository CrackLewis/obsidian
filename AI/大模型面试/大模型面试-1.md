
参考资料：[src]()

## 0-基本问题

### 0.1-自我介绍

`xx`：需要灵活修改的内容

面试官您好，我叫李博宇，目前就读于北京大学的软件工程专业，本科就读于同济大学的计算机科学与技术专业。我有比较好的编程基础，对C/C++、Python语言较熟练，理解常见的机器学习相关理论，有一定的深度学习开发基础。我的英语能力较好，能够熟练阅读英文文献和进行日常英文对话。我对贵司的前景十分看好，贵司在`大模型研发`方向的工作进展与我的研究兴趣十分契合，我也十分期待能够加入贵司，为贵司创造价值。谢谢。

### 0.2-其他

最快到岗时间：1个月

优缺点：
- 优点：
	- 有很强的自学能力，善于获取和运用知识
	- 善于与人沟通，理解他人的需求
- 缺点：
	- 有时过于完美主义而束缚了自己

## 1-大模型基础

### 1.1-CNN

一般结构：
- 输入
- `N`个卷积-激活复合
- `M`个池化
- `K`个全连接-激活复合
- 全连接层-输出

![[Drawing 2024-12-11 20.09.34.excalidraw|100%]]

卷积层参数：
- `W`：输入数据的空间尺寸（3维：通道\*宽度\*高度；4维：样本数\*通道\*宽度\*高度）
- `F`：卷积核（滤波器）尺寸
- `P`：填充大小（padding），在输入数据周围补齐的额外行列
- `S`：步长（stride），卷积核在输入上滑动的距离

输出规模：`output = (W - F + 2P) / S + 1`

单通道卷积

多通道卷积

*卷积层*：
- 参数：
	- 步长（stride）
	- 零填充（zero-padding）：对输出边缘进行填充，使输出规模与输入一致
	- 感受野（receptive field）：卷积核的尺寸
	- 输出通道数：如无特殊说明，则等于输入通道数
- 计算细节
- 特点：产生大量计算，平移不变性，参数共享，一般带激活函数

*池化层*：通过减少输入大小，降低输出值数量

*全连接层*：

经典CNN模型：
- LeNet-5
- AlexNet
- VGG-16
- ResNet
- GoogLeNet

### 1.2-PyTorch基础

张量=k维数组

Q1：创建向量的方法
- torch.zeros
- torch.ones
- torch.empty：返回一个指定规模的未初始化张量，不一定是全零
- torch.rand：`[0,1)`内均匀分布
- torch.randn：元素服从标准正态分布$N(0,1)$
	- torch.randn_like：取其他tensor的规模，分布取标准正态分布
- torch.randint：元素取`[low, high)`内的随机整数
	- low、high：数值下界和(上界+1)
	- size：tensor规模
- torch.tensor：从数组创建tensor
- torch.eye：单位矩阵
- torch.stack：堆叠相同规模的张量

*Q1：squeeze、unsqueeze和cat操作*

torch.squeeze(x)：降维。如果第x维规模为1，则削去该维，否则不作改动

torch.unsqueeze(x)：升维。在第x维处无条件插入一维

torch.cat((a,b))：在a,b的第一维内拼接

```python
a = torch.tensor([1, 2, 3])
b = torch.tensor([3, 2, 1])
torch.stack((a, b)) # [[1, 2, 3], [3, 2, 1]]
torch.cat((a, b)) # [1, 2, 3, 3, 2, 1]
```

*Q2：torch.nn.Conv2d*

二维卷积层

参数：
- in_channels：输入数据的通道数，如果输入数据是图像，则通道数为图像的颜色通道数（例如RGB图像为3）。
- out_channels：卷积核的数量，也是输出特征图的通道数。
- kernel_size：卷积核的大小，可以是整数或元组，如 (3, 3)。
- stride：卷积核在输入上移动的步长，默认为 1。
- padding：在输入的各个边界周围添加零值的层数。如果设置为 padding=(1, 2)，则在宽度方向上添加 1 层零值，高度方向上添加 2 层零值，默认为 0。
- dilation：控制卷积核的空洞卷积（dilated convolution）的空洞率，用于在卷积核元素之间插入间隔。默认为 1。
- groups：控制输入和输出之间的连接。当 groups 等于输入通道数和输出通道数时，每个输入通道都连接到输出通道。默认为 1。
- bias：是否添加偏置，默认为 True。
- padding_mode：当 padding 不为 0 时，指定如何填充。默认为 'zeros'，可以选择 'reflect' 或 'replicate'。

```python
import torch
import torch.nn as nn

# 创建一个二维卷积层，输入通道数为3，输出通道数为64，卷积核大小为3x3，padding为1
conv_layer = nn.Conv2d(3, 64, kernel_size=3, padding=1)
# 假设有一个输入张量 input_tensor，维度为 [batch_size, 3, height, width]
batch_size = 32
height = 128
width = 128
# 创建随机输入张量
input_tensor = torch.randn(batch_size, 3, height, width)
# 进行前向传播
output_tensor = conv_layer(input_tensor)
```

*Q3：size、shape、dtype、numpy转换*

x.size()、x.shape：返回张量各维度规模
- size是成员函数，shape是成员属性

x.dtype：元素类别
- int、float
- torch.float
- torch.double=torch.float64
- np.float32、np.float64
- np.int32、np.int64

x.numpy()：tensor转为ndarray

torch.from_numpy(y)：将ndarray转换为tensor

注意：numpy和from_numpy方法都会使tensor和ndarray共享内存

x.clone()：提供一个自身的复制，可避免数据共享

*Q3.1：转置、索引、切片*

```python
# 创建一个大小为 3x3 的张量
tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
# 转置：tensor.t()
tensor.t()
tensor([[1, 4, 7],
        [2, 5, 8],
        [3, 6, 9]])
# 索引
tensor[0, 1] # 2
# 切片：获取第一行的元素
first_row = tensor[0, :]
first_column = tensor[:, 0]
```

*Q4：nn.Embedding层*

```python
# nn.Embedding 层的实现基于矩阵乘法，其内部维护一个由随机初始化的词向量组成的矩阵。
# 假设词汇表大小为10000，每个词语被表示为一个长度为100的向量
vocab_size = 10000
embedding_dim = 100
# 创建一个 Embedding 层
embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)
# 假设有一个输入张量 input_indices，包含了一批词语的索引
input_indices = torch.LongTensor([[1, 2, 3], [4, 5, 6]])
# 将输入张量传递给 Embedding 层
embedded_vectors = embedding_layer(input_indices)
print(embedded_vectors.shape)  # 输出: torch.Size([2, 3, 100])
```

*Q5：torch.optim.AdamW、torch.optim.SGD等*

Adam算法=AdaGrad自适应学习率调整+Momentum动量法

AdamW算法=Adam算法+权重衰减（weight decay）

WIP

*Q6：torch.argmax*

```python
# torch.argmax() 是 PyTorch 中一个用于在指定维度上找到张量中最大值的索引的函数。
# 具体来说，torch.argmax() 返回沿着指定维度（dim 参数指定）上最大值的索引。
# 在这里，dim=1 表示在第一个维度（通常是列）上进行操作。
pred = [
    [0.1, 0.8, 0.1],   # 样本1的预测结果，最大值索引为1
    [0.3, 0.2, 0.5],   # 样本2的预测结果，最大值索引为2
    [0.5, 0.4, 0.1]    # 样本3的预测结果，最大值索引为0
]
torch.argmax(pred, dim=1)
# 将得到一个形状为 (3,) 的张量：
[1, 2, 0]
```

Q7：基础组件，如Model、Tokenizer、Datasets、Pipeline、Evaluate等

多GPU：nn.DataParallel(model)

微调：

```python
局部微调：加载了模型参数后，只想调节最后几层，其他层不训练，也就是不进行梯度计算
pytorch提供的requires_grad使得对训练的控制变得非常简单。

model = torchvision.models.resnet18(pretrained=True)
for param in model.parameters():
    param.requires_grad = False
# 替换最后的全连接层， 改为训练100类
# 新构造的模块的参数默认requires_grad为True
model.fc = nn.Linear(512, 100)
 
# 只优化最后的分类层
optimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9)


全局微调：对全局微调时，只不过我们希望改换过的层和其他层的学习速率不一样，
这时候把其他层和新层在optimizer中单独赋予不同的学习速率。

ignored_params = list(map(id, model.fc.parameters()))
base_params = filter(lambda p: id(p) not in ignored_params,
                     model.parameters())
 
optimizer = torch.optim.SGD([
            {'params': base_params},
            {'params': model.fc.parameters(), 'lr': 1e-3}
            ], lr=1e-2, momentum=0.9)
```

nn.Functional vs nn.Module：前者为低层函数，后者为高层结构

反向传播：
- loss.backward()：反向传播
- optimizer.step()：权重更新
- optimizer.zero_grad()：导数清零

### 1.3-PyTorch搭建神经网络

示例：基于MNIST数据集的手写数字识别

step1：导入必要的库

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
```

step2：定义神经网络结构

```python
# nn.Module类定义了一个全连接的神经网络（也被称为前馈神经网络-FFN）
# (feed-forward layer (前馈层)(feed-forward network)
class NeuralNetwork(nn.Module):
    def __init__(self, input_size, num_classes):
        super(NeuralNetwork, self).__init__()
        # 定义了两个全连接层
        # 将输入特征的维度从input_size降维到50，因此有50个隐藏单元
        self.fc1 = nn.Linear(in_features=input_size, out_features=50)
        # 在这里，50是由上一层的输出决定的，
        # 而num_classes通常是指分类问题中的类别数量。
        # 这意味着fc2层将上一层的输出（大小为50）映射到一个大小为num_classes的输出空间
        self.fc2 = nn.Linear(in_features=50, out_features=num_classes)
        
    # 定义了神经网络的前向传播过程
    def forward(self, x):
        return self.fc2(F.relu(self.fc1(x)))
```

step3：设置运行任务的设备

```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```

step4：定义超参数

```python
# 定义了一个学习率0.001，一个批量大小64，并训练了10个epochs。
input_size = 784  # 输入特征维度
num_classes = 10  # 输出类别
learning_rate = 0.001  # 学习率
batch_size = 64  # 批次大小
num_epochs = 10  # 迭代次数
```

step5：加载数据

```python
# 数据自动下载
train_data = datasets.MNIST(root = "dataset/", 
                            train=True, 
                            transform=transforms.ToTensor(), 
                            download=True
                            )

train_loader = DataLoader(dataset=train_data, 
                          batch_size=batch_size, 
                          shuffle=True
                          )

test_data = datasets.MNIST(root = "dataset/", 
                           train=False, 
                           transform=transforms.ToTensor(), 
                           download=True
                           )

test_loader = DataLoader(dataset=test_data, 
                         batch_size=batch_size, 
                         shuffle=True
                         )
```

step6：定义模型、损失函数、优化器

```python
# 使用.to()方法在设备上初始化我们的神经网络模型
model = NeuralNetwork(input_size=input_size, num_classes=num_classes).to(device)
# 定义交叉熵损失函数
criterion = nn.CrossEntropyLoss()
# PyTorch提供的Adam优化器
# 优化器是一种工具，用来帮助我们更新模型的参数，以使损失函数最小化
optimizer = optim.Adam(params= model.parameters(), lr=learning_rate)
# optim.SGD()
```

step7：训练模型

```python
# 已经定义了我们的模型，加载了数据，并初始化了必要的组件
# 我们的目标是最小化损失函数，这样模型的预测结果就会更接近真实标签。
# 通过分批迭代训练数据来训练我们的神经网络，并使用反向传播更新模型参数
# 反向传播是一种用于训练模型的优化算法。
# 它通过计算损失函数对模型参数的梯度，并利用这些梯度来更新模型参数，以使损失函数最小化
for epoch in range(num_epochs):
    for batch_idx, (data, labels) in enumerate(train_loader):
        data = data.to(device=device)
        labels = labels.to(device=device)
        data = data.reshape(data.shape[0], -1)
        
        # forward 函数体现 
        # 在每个训练批次中，通过将数据传递给模型，调用了模型的 forward 方法。
        # 这样做会执行神经网络的前向传播过程
        scores = model(data)
        
        loss = criterion(scores, labels)
        optimizer.zero_grad()
        
        # 这两行代码执行了反向传播(Backpropagation)的关键步骤
        # 梯度是损失函数对模型参数的变化率，或者说是损失函数关于模型参数的导数
        # 计算了损失函数对模型参数的梯度。将计算得到的梯度存储在模型的参数的.grad属性中
        loss.backward()
        # 根据损失函数的梯度来更新模型的参数
        # 优化器会根据规则Adam,以及损失函数的梯度，对模型的参数进行更新，使得损失函数的值减小
        optimizer.step()
```

step8：评估模型

```python
num_correct = 0
num_samples = 0
model.eval()

with torch.no_grad():
    for data, labels in test_loader:
        data = data.to(device=device)
        labels = labels.to(device=device)

        data = data.reshape(data.shape[0],-1)

        scores = model(data)

        _, predictions = torch.max(scores, dim=1)
        num_correct += (predictions == labels).sum()
        num_samples += predictions.size(0)

    print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct) / float(num_samples)*100:.2f}')

model.train()
# Got 9714 / 10000 with accuracy 97.14
```

step9：结果可视化

```python
import matplotlib.pyplot as plt

# 测试并绘制10张随机图像
model.eval()
with torch.no_grad():
    fig, axs = plt.subplots(2, 5, figsize=(12, 6))
    axs = axs.flatten()

    for i, (data, labels) in enumerate(test_loader):
        if i >= 10:  # Break after 10 images
            break

        data = data.to(device=device)
        labels = labels.to(device=device)

        data = data.reshape(data.shape[0], -1)

        scores = model(data)

        _, predictions = torch.max(scores, dim=1)

        # 绘制图像和预测结果
        img = data.cpu().numpy().reshape(-1, 28, 28)
        axs[i].imshow(img[0], cmap='gray')
        axs[i].set_title(f"Label: {labels[0]} - Prediction: {predictions[0]}")
    plt.tight_layout()
    plt.show()
    
model.train()
```

