
## ch01-绪论

计算机科学：输入+模型->输出

机器学习：输入+输出->模型

模型：从数据中学习得到的结果
- 模式：模型的一个局部

基本术语：
- *数据集*（data set）：数据记录集合
- *样本*（sample）、*实例*（instance）：数据记录的别称
- *属性*（attribute）、*特征*（feature）：数据记录的某方面表现、性质的反映
	- *属性值*（attribute value）：这一方面的数值
	- *属性空间*（attribute space）、*样本空间*（sample space）：属性张成的空间
- *特征向量*（feature vector）：数据记录的数学表示
- *维度*（dimensionality）：特征向量的维度数
- *学习*（learning）、*训练*（training）：从数据学得模型的过程
- *训练数据*（training data）、*训练样本*（training sample）
- *训练集*（training set）
- *假设*（hypothesis）：学得的模型对应了关于数据的某种潜在规律
	- *真实*（ground-truth）：这种潜在规律自身
	- *学习器*（learner）：模型别称
- *预测*（prediction）
- *标记*（label）、*样例*（example）
- *标记空间*（label space）
- *测试*（testing）：
	- *测试样本*（testing sample）
	- *测试集*（test set）
- 机器学习任务：
	- *分类*（classification）
		- *二分类*（binary classification）：*正类*（positive class）、*反类*（negative class）
		- *多分类*（multi-class classification）
	- *回归*（regression）
	- *聚类*（clustering）：将训练样本分为若干相近的组
		- *簇*（cluster）：每个划分形成的组
- *有监督学习*（supervised learning）：提供标记信息的学习过程
- *无监督学习*（unsupervised learning）：不提供标记信息的学习过程
- *泛化*（generalization）：学得模型适用于新样本的能力
- *分布*（distribution）：样本在样本空间中的分散情况
	- *独立同分布*（independent and identically distributed, i.i.d.）

归纳（induction）：从特殊到一般的泛化过程
- 从具体事实归结出一般规律
- 从样例中学习即是归纳过程
- 广义归纳学习样例特征，狭义归纳学习概念

演绎（deduction）：从一般到特殊的特化过程
- 从一般规律出发推演出具体状况

*归纳偏好*（inductive bias）：如果模型无法确定新样本属于哪个分类，则它必根据自己在学习过程中对某种类型假设的偏好进行归纳。
- 理解：学习算法自身在假设空间中学习成的“价值观”
- 例：模型倾向于认为各种属性均相近的西瓜，成熟程度也相似
- *“没有免费的午餐”定理*（no-free-lunch theorem, NFL）：
	- 各个学习算法在训练集外的期望性能相同，都约等于乱猜
	- 如果一个学习算法$L_a$在训练集上表现比$L_b$好，则必存在另一些数据集使得$L_b$的表现更好
	- 前提：所有问题的出现机会相等，但现实不是这样
	- 启示：谈论哪个学习算法更好时，*必须带上具体问题*

发展历程：
- 1950s-70s: 推理期：
	- Logic Theorist, General Problem Solving, etc.
	- 连接主义学习、感知机、符号主义学习
- 1970s-80s: 知识期
	- 逻辑推理不能实现AI，需要依赖知识
	- 专家系统、知识工程
- 1980s-90s: 机器学习形成
	- Michalski分类：*样例中学习*（广义归纳学习）、问题求解和规划中学习、观察和发现学习、指令中学习
	- 样例学习分支：
		- 符号主义学习：决策树（信息熵最小化）、基于逻辑学习（归纳逻辑程序设计）等
		- 连接主义学习：神经网络、BP算法等
- 1990s-: 统计学习
	- 支持向量机、核方法等
- 2000s-: 连接学习重新兴起
	- 背景：数据量增大，计算能力变强
	- 深度学习：多层神经网络

应用现状：
- 计算机：多媒体/图形学/通信/软工/体系结构/芯片设计
- 交叉学科：生信/CV/etc.
- 数据挖掘：since 90s

## ch02-模型评估与选择

### 2.1-经验误差与过拟合

错误率（error rate）：分类错误的样本数占总样本比例
- 精度（accuracy）：分类正确的样本数占总样本比例
	- 精度=1-错误率
- 误差（error）：实际预测与真实输出的差异
	- 训练误差（training error）、经验误差（empirical error）：在训练集上的误差
	- 泛化误差（generalization error）：在训练集外的误差
- 过拟合（overfitting）：训练样本学得“太好”了，以至于对训练集外的样本表现较差
- 欠拟合（underfitting）：连训练样本都没学好

学习目标：*降低泛化误差*，而非刻意降低训练误差

### 2.2-评估方法

测试集（testing set）：数据集的一个用于评估模型学习效果的子集
- 测试误差（testing error）：模型在测试集上的误差

*训练集-测试集划分手段*：数据集$D$分为训练集$S$，测试集$T$。
- 留出法（hold-out）：$D=S\cup T,S\cap T=\varnothing$。设定$|S|/|T|=k$，一般取$2\sim4$之间。
- 交叉验证法（cross validation）：$D=\bigcup_{i=1}^n D_i$，且$\forall 1\le i,j\le n$，$D_i\cap D_j=\varnothing$。每轮取$T=D_i,S=D-D_i$，总共$n$轮。
- 自助法（bootstrapping）：
	- 每轮随机挑选$x\in D$，将其拷贝入可重集合$S$，总共进行$|D|$轮
	- 当$m$足够大时，任意$x\in D$不在$S$中的概率为$P(x\notin S)=\lim_{m\rightarrow +\infty}(1-1/m)^m=1/e\approx 0.368$ 。
	- 取$T=D-S$，则$T$大约包含36.8%的$D$样本，属合理范围

手段比较：
- 留出法：简单，但偶然性大
- 交叉验证法：偶然性小，但时间成本大
- 自助法：在难于划分数据集时有用，但改变了数据集分布，会引入估计偏差

*调参*（parameter tuning）*与最终模型*：
- 绝大部分模型都有参数，需要调节
- 调参vs算法选择：调参需要试验大量参数，且对模型性能有决定性影响
- 验证集vs测试集：
	- 模型评估&选择时，需要预留一些数据用于验证模型性能，这部分数据称为*验证集*（validation set）
	- 测试集则是确定模型后测试训练效果的数据集

### 2.3-性能度量

模型性能=模型泛化能力

*均方误差*（mean squared error, MSE）：对于模型$f$和数据集$D=\{(x_i,y_i)|1\le i\le m\}$：
$$
E(f;D)=\dfrac{1}{m}\sum_{i=1}^m (f(x_i)-y_i)^2
$$
或更一般地：对于$f$和$D$，设样本$x$在$D$中概率密度$p(\cdot)$，标签为$y$，则：
$$
E(f;D)=\int_{x\sim D} (f(x)-y)^2 p(x)dx
$$

*错误率*（error rate）：设$\mathbb{I}(\cdot)$为单元函数，将布尔表达式映射为$0/1$：
$$
E(f;D)=\int_{x\sim D} \mathbb{I}(f(x)\neq y)\cdot p(x)dx
$$
*精度*（accuracy rate）：
$$
\text{acc}(f;D)=\int_{x\sim D} \mathbb{I}(f(x)= y)\cdot p(x)dx=1-E(f;D)
$$

查准率、查全率、F1：
- 背景：错误率/精度信息不全，可能需要研究误判类型（实际为正预测为反，实际为反预测为正）
- 分类结果：
	- TP/TN：预测、实际均为真/假，true positive/negative
	- FP/FN：预测为真/假，实际为假/真，false positive/negative
- *查准率*（precision）：
	- 概念：TP占所有预测为真样本的比率
	- 定义：$P=\dfrac{TP}{TP+FP}$
- *查全率*（recall）：
	- 概念：TP占所有实际为真样本的比率
	- 定义：$R=\dfrac{TP}{TP+FN}$
- 查准率vs查全率：
	- 两者矛盾，一者偏高则另一者常常偏低：提升查准率需要更保守地预测，但保守预测可能增加FN，从而降低查全率，反之亦然
	- *P-R图*：对所有样本按查全率升序排列，根据当前的实际查准率绘制曲线，形成查准率-查全率曲线
		- 意义：更凸出（或平衡点更高）的曲线表示更优秀的模型
		- 平衡点（break-event point，BEP）：$y=x$与曲线的交点
- F1度量：
	- 意义：查准率和查全率的加权调和平均
	- 一般形式：设查全率对查准率重要度为$\beta$，则$F_\beta=\dfrac{(1+\beta^2)PR}{\beta^2P+R}$。
		- $\beta=1$：*标准F1度量*
		- $\beta>1$表示查全率$R$影响更大，$\beta<1$表示查准率$P$影响更大
- 处理多组混淆矩阵：
	- 计算每组的$(P_i,R_i)$
	- 宏度量：
		- 计算宏查准率和宏查全率：$\text{M-}P=\sum_{i=1}^n P_i/n$，$\text{M-}R=\sum_{i=1}^n R_i/n$。
		- 计算宏F1：$\text{M-}F1=\dfrac{2\times \text{M-}P\times \text{M-}R}{\text{M-}P+\text{M-}R}$。
	- 微度量：
		- 计算每组的TP/FP/TN/FN，并取均值
		- 计算微查准率和微查全率：$\text{m-P}=\dfrac{\overline{TP}}{\overline{TP}+\overline{FP}}$，$\text{m-}R=\dfrac{\overline{TP}}{\overline{TP}+\overline{FN}}$。
		- 计算微F1：$\text{m-}F1=\dfrac{2\times \text{m-}P\times \text{m-}R}{\text{m-}P+\text{m-}R}$。

P-R图示例：

![[Pasted image 20241004172046.png]]

ROC与AUC：
- 背景：在分类任务中，特别是当数据集类别不平衡时，单纯依赖准确率或P-R可能会造成误导
	- 例如：数据集中95%为反例，5%为正例，即使模型总是预测为反例也有95%准确率，但这毫无意义
- 相关指标：
	- 真正例率（true positive rate，TPR）：$TPR=\dfrac{TP}{TP+FN}$。
	- 假正例率（false positive rate，FPR）：$FPR=\dfrac{FP}{TN+FP}$。
- *受试者工作特征*（ROC, receiver operating characteristic）曲线：横轴为样本的假正例率，纵轴为真正例率
	- 绘制过程：假设对每个样本会输出真实率$p_i\in [0,1]$。
		- 取阈值$P\in [0,1]$，对$p_i<P$的样本预测为反例，其余预测为正例，并计算其TPR和FPR。
		- 取足够多的阈值，计算出的TPR/FPR绘制成一条从$(0,0)$到$(1,1)$的曲线
- 曲线下面积（AUC, area under the curve）：ROC曲线下面积，又称AUC，可用于衡量一个学习器的效果
	- 计算方式：设阈值TPR-FPR为$\{(x_i,y_i)\}$，则$\text{AUC}=\dfrac{1}{2}\displaystyle\sum_{i=1}^{m-1} (x_{i+1}-x_i)\cdot (y_i+y_{i+1})$。
	- 衡量方式：AUC>0.5说明有一定学习效果，AUC=0.5说明和瞎猜没区别，AUC<0.5说明还不如瞎猜

![[Pasted image 20241005194225.png]]

代价敏感错误率、代价曲线：
- 背景：两类错误的代价有时不同：如果实际正常被误诊为患病，影响有限；实际患病被误诊为正常，则可能产生严重后果。因此两类错误可能需要指定*非均等代价*（unequal cost）
- 非均等代价：设置$k$分类代价矩阵$\{cost_{ij}\}$（$i,j\in \{0,1,\ldots,k-1\}$），对所有的$i\neq j$设置$cost_{ij}>0$，表示真实为$i$预测为$j$的代价。
- 代价敏感错误率（cost-sensitive error rate）：设判为反例的样例集为$D^-$，判为正例的样例集为$D^+$，则：
 $$E(f;D;cost)=\dfrac{1}{m}\left(\sum_{x_i\in D^+} \mathbb{I}(f(x_i)\neq y_i)\cdot cost_{01}+\sum_{x_i\in D^-} \mathbb{I}(f(x_i)\neq y_i)\cdot cost_{10}\right)$$
- *代价曲线*（cost curve）：反映学习器的真实损失情况，横轴为正例概率代价，纵轴为归一化代价
	- 正例概率代价：设样本正例概率为$p$：
$$
P(+)cost=\dfrac{p\times cost_{01}}{p\times cost_{01}+(1-p)\times cost_{10}}
$$
	- 归一化代价：设$\text{FPR}$为假正例率，$\text{FNR}$为假反例率
$$
cost_{norm}=\dfrac{\text{FNR}\times p\times cost_{01}+\text{FPR}\times (1-p)\times cost_{10}}{p\times cost_{01}+(1-p)\times cost_{10}}
$$
	- 绘制：
		- 对ROC曲线上任意一点$(\text{TPR},\text{FPR})$，计算$\text{FNR}=1-\text{TPR}$
		- 对每个点，在代价平面上绘制一条$(0,\text{FPR})\rightarrow (1,\text{FNR})$的线段
		- 所有线段与横轴围成一个小区域，如下图

![[Pasted image 20241006105635.png]]

### 2.4-比较检验

回顾：
- 评估方法：测试集、验证集
- 性能度量：
	- 错误率与精度、均方误差
	- 查准率、查全率、P-R图、F1度量
	- FPR、FNR、ROC-AUC
	- 代价敏感错误率、代价曲线

why：
- 希望比较的是泛化性能，与测试集上的性能未必一致
- 测试集的选择会影响结果
- 学习算法的随机性

*统计假设检验*（hypothesis test）：在测试集上观察到A优于B，那么A的泛化性能在统计意义上有多大把握优于B

本节默认性能度量为错误率，记作$\epsilon$

#### 假设检验

假设（hypothesis）：对泛化错误率$\epsilon$分布的某种猜想，如$\epsilon=\epsilon_0$
- 现实中不知道$\epsilon$，只知道测试集错误率$\hat{\epsilon}$
- $|\epsilon-\hat{\epsilon}|$较小的可能性大，较大的可能性小

对规模为$m$的测试集，出错的样本数为$\hat{\epsilon}\times m$，测试集错误率为$\hat{\epsilon}$的概率：
$$
P(\hat{\epsilon};\epsilon)=\text{C}_m^{\hat{\epsilon}\times m} \epsilon^{\hat{\epsilon}\times m}(1-\epsilon)^{m-\hat{\epsilon}\times m}
$$
由$\dfrac{\partial P(\hat{\epsilon};\epsilon)}{\partial \epsilon}=0$知：$P(\cdot)$在$\epsilon=\hat{\epsilon}$时最大。

*二项检验*（binomial test）：

考虑诸如$\epsilon\le \epsilon_0$这种假设，在$1-\alpha$的概率内所能观察到的最大错误率：
$$
\overline{\epsilon}=\max \epsilon\quad \text{s.t.} \quad \sum_{i=\epsilon_0\times m+1}^m\left(\begin{matrix}m\\i\end{matrix}\right)\epsilon^i (1-\epsilon)^{m-i} <\alpha
$$
$1-\alpha$称为结论（$\epsilon\le \epsilon_0$）的*置信度*（confidence）。若$\hat{\epsilon}<\overline{\epsilon}$，则可以认为：在$\alpha$的显著度下，假设$\epsilon\le \epsilon_0$不能被拒绝，即能以$1-\alpha$的置信度认为$\epsilon\le \epsilon_0$；否则假设可被拒绝，即在$\alpha$的显著度下可认为$\epsilon>\epsilon_0$。

![[Pasted image 20241006144641.png]]

对于多次验证得到的$\hat{\epsilon}_i$（$1\le i\le k$），考虑使用*t-检验*（t-test）：
- 平均错误率：$\mu=\dfrac{1}{k}\displaystyle\sum_{i=1}^k \hat{\epsilon}_i$
- 方差：$\sigma^2=\dfrac{1}{k-1}\displaystyle\sum_{i=1}^k (\hat{\epsilon}_i-\mu)^2$
- 此时，随机变量$\tau_t=\dfrac{\sqrt{k}(\mu-\epsilon_0)}{\sigma}\sim t(k-1)$。

对假设$\mu=\epsilon_0$和显著度$\alpha$，可计算出当测试错误率均值为$\epsilon_0$时，在$1-\alpha$概率内能观测到的最大错误率，即临界值。这里考虑双边假设：
- 若平均错误率$\mu$与$\epsilon_0$之差$|\mu-\epsilon_0|\in [t_{-\alpha/2},t_{\alpha/2}]$，则不能拒绝假设$\mu=\epsilon_0$，即可以认为$\epsilon=\epsilon_0$，置信度为$1-\alpha$
- 否则可拒绝该假设，即在该显著度下认为$\epsilon\neq \epsilon_0$

![[Pasted image 20241006170543.png]]

#### 交叉验证t检验

假如同样的数据集分别用于测试学习器A和B，经过$k$折交叉验证后得到的测试错误率分别为$\epsilon_i^A$和$\epsilon_i^B$（$1\le i\le k$），相同的$i$对应同一折的测试集。

*成对t检验*（paired t-tests）：

模型A/B的测试错误率差值$\Delta_i=\epsilon_i^A-\epsilon_i^B$。

若模型性能相同，则$\sum_{i=1}^k \Delta_i =0$。

考虑对“模型A与模型B性能相同”的假设作t检验，计算出差值的均值$\mu$和方差$\sigma$，再计算出正则化变量：
$$
\tau_t=\left|\dfrac{\sqrt{k}\mu}{\sigma}\right|\sim t(k-1)
$$
若：
- $\tau_t<t_{\alpha/2}$，则假设不能被拒绝，即有$1-\alpha$的置信度认为A/B性能无显著差异
- $\tau_t\ge t_{\alpha/2}$：在显著度为$\alpha$时，A/B性能有较明显的差异

#### McNemar检验

用于分析和比较两个相关或配对的二分类变量，如是或否、相关和无关等。常使用2x2列联表作为检验工具。

对二分类问题，模型A和B的表现可能不同，设分类正确为0、错误为1，并计算列联矩阵$e_{ij}$（$i,j\in \{0,1\}$），表示模型A/B分类结果分别为$i,j$的样本个数。例如$e_{01}$表示A分类正确而B分类错误的样本数。

若假设A、B性能一致，则有$e_{01}=e_{10}$。此时：
$$
|e_{01}-e_{10}|\sim N(1,e_{01}+e_{10})
$$
基于此计算McNemar检验的统计量：
$$
\tau_{\chi^2}=\dfrac{(|e_{01}-e_{10}|-1)^2}{e_{01}+e_{10}}\sim \chi^2(1)
$$
给定显著度$\alpha$：
- 当$\tau_{\chi^2}<\chi_\alpha^2$时，假设不能被拒绝，认为模型A、B的性能无明显差异
- 否则拒绝假设，认为A、B性能有显著差距，且平均错误率$\hat{\epsilon}$更小的模型性能更优

#### Friedman检验、Nemenyi后续检验

交叉验证t检验、McNemar检验：只能在一个数据集上比较两个算法

*Friedman检验*：

假定用$N$个数据集比较$k$个算法：

首先需要预处理，计算出各个算法在所有数据集中的平均序值：
- 第一步是用数据集测试每个算法，得到结果。
- 第二步是对每个数据集，将各个算法的性能排序，并根据性能好坏赋予不同序值，最好的赋序值1，次好的为2，最差的为$k$。并列的取平均，确保所有算法的序值总和为$\dfrac{k^2+k}{2}$。
- 第三步是对每个算法，求其在所有数据集中序值的平均值，记作$r_i$（$1\le i\le k$）。

例如：$N=4,k=3$：

![[Pasted image 20241006234905.png]]

当$k,N$足够大时，$r_i$服从$N(\mu,\sigma^2)$，其中：
$$
\mu=\dfrac{k+1}{2},\sigma^2=\dfrac{k^2-1}{12}
$$
卡方统计量：
$$
\tau_{\chi^2}=\dfrac{k-1}{k}\cdot \dfrac{N}{\sigma^2}\sum_{i=1}^k \left(r_i-\mu\right)^2\sim \chi^2(k-1)
$$
或者进一步使用F分布统计量：
$$
\tau_F = \dfrac{(N-1)\tau_{\chi^2}}{N(k-1)-\tau_{\chi^2}}\sim F(k-1,(k-1)(N-1))
$$
这些统计量也可以作为验证学习算法性能是否相同的依据。

如果算法被验证为具有显著不同的性能，则需进行*后续检验*（post-hoc test），例如*Nemenyi后续检验*。后续检验用于确认哪些算法是相同的，哪些是不同的。

计算平均序值差别的临界值域：
$$
CD=q_\alpha \sqrt{\dfrac{k(k+1)}{6N}}
$$
查书中的$q_a$表，计算出$CD$值。将$CD$值与任意两个算法的平均序值之差$|r_i-r_j|$作对比：
- 若$|r_i-r_j|<CD$，说明这两个算法性能是相同的。
- 否则说明这两个算法性能显著不同。

### 2.5-偏差与方差

*偏差-方差分解*（bias-variance decomposition）：对学习算法的期望泛化错误率进行拆解，分析出哪些错误是由数据分布引起的（方差），哪些是由数据集与现实数据偏差造成的（噪声），哪些是模型泛化能力造成的（偏差）。

设测试样本为$x$，$y$为$x$的真实标记，$y_D$为$x$在数据集$D$中的标记，$f$为学习模型，$f(x;D)$为训练集$D$上学得模型$f$在$x$上的预测输出。

学习算法的期望预测为：
$$
\overline{f}(x)=E_D[f(x;D)]
$$
使用不同训练集$D$产生的方差：
$$
var(x)=E_D\left[\left(f(x;D)-\overline{f}(x)\right)^2\right]
$$
*噪声*（noise）表示数据集标签与样本真实标签的偏差：
$$
\varepsilon^2 = E_D\left[(y_D-y)^2\right]
$$
*偏差*（bias）表示模型预测输出与真实标签的偏差：
$$
bias^2(x)=\left(\overline{f}(x)-y\right)^2
$$
为简化问题，假设噪声期望为0，即：
$$
E_D[y_D-y]=0
$$
则算法的期望泛化误差可作如下分解：
$$
\begin{split}
E(f;D)=& E_D\left[(f(x;D)-y_D)^2\right] \\
=& E_D\left[(f(x;D)-\overline{f}(x)+\overline{f}(x)-y_D)^2\right] \\
=& E_D\left[(f(x;D)-\overline{f}(x))^2\right]+E_D\left[(\overline{f}(x)-y_D)^2\right] \\
&+E_D\left[2(f(x;D)-\overline{f}(x))(\overline{f}(x)-y_D)\right] \\
=& E_D\left[(f(x;D)-\overline{f}(x))^2\right]+E_D\left[(\overline{f}(x)-y_D)^2\right] \\
=& E_D\left[(f(x;D)-\overline{f}(x))^2\right]+E_D\left[(\overline{f}(x)-y+y-y_D)^2\right] \\
=& E_D\left[(f(x;D)-\overline{f}(x))^2\right]+E_D\left[(\overline{f}(x)-y)^2\right]+E_D\left[(y-y_D)^2\right] \\
&+ 2E_D\left[(\overline{f}(x)-y)(y-y_D)\right] \\
=& E_D\left[(f(x;D)-\overline{f}(x))^2\right]+(\overline{f}(x)-y)^2+E_D\left[(y_D-y)^2\right] \\
=& bias^2(x)+var(x)+\varepsilon^2
\end{split}
$$

说明算法的期望泛化误差为偏差、方差和噪声的总和：
- 偏差：衡量算法拟合能力，表示算法的期望预测与真实结果的偏离度
- 方差：刻画数据扰动影响，表示相同规模训练集变动导致的性能变化
- 噪声：刻画学习问题难度，表示任何算法在当前学习任务中的误差下界

*偏差-方差窘境*（bias-variance dilemma）：

偏差和方差一般存在冲突。如果模型欠拟合，则偏差大，此时模型受数据扰动较小，方差也较小。如果模型过拟合，则偏差小，但模型对数据过于敏感，受数据扰动影响大，方差也偏大。

![[Pasted image 20241007113610.png]]

