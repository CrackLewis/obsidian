
所有的深度学习内容基于PyTorch。

## 环境配置

参考：[doc](https://zh-v2.d2l.ai/chapter_installation/index.html#d2l)

首先创建对应的Conda环境，指定Python版本。再在创建好的环境中安装`d2l`包。

```sh
$ conda create -n d2l -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ python=3.10.6
$ conda activate d2l
$ pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
$ pip install d2l
```

安装`d2l`之后，还需要安装一个深度学习框架。我们选择PyTorch：

```sh
$ pip install torch==1.12.0
$ pip install torchvision==0.13.0
```

从d2l的GitHub仓库上可以下载一份代码包。

代码是Jupyter Notebook形式的。如果系统里有不止一套Python环境，那么自动kernel选择可能会失效。解决方案参考：[doc](https://github.com/microsoft/vscode/issues/130946#issuecomment-1899389049)

## torch.tensor

```python
import torch
```

- 自然数序列：`x = torch.arange(n)`
- 张量形状：`x.shape`
- 张量元素个数：`x.numel()`
- 张量形状变换：`x.reshape(...)`
- 张量复制：`x.clone()`
- 张量生成：
	- 全零张量：`torch.zeros(shape)`
	- 全1张量：`torch.ones(shape)`
	- 服从正态分布的随机张量：`torch.randn(shape)`
	- 基于Python列表：`torch.tensor(arr)`
- 基本运算：对应位置元素参与运算
- 张量连接：
	- 水平拼接：`torch.cat((x, y), dim=1)`
	- 垂直拼接：`torch.cat((x, y), dim=0)`
- 元素求和：`x.sum(...)`
- 索引：`x[1, 2]`（第1行第2列）
- 切片：`x[1:3]`（第1-2行）、`x[:, 0:3]`（第0-2列）

对tensor进行形状变换时，不一定需要显式指定每一维的大小。如果某些维是-1，表示这些维度的大小是自动推断的。

```python
x = torch.randn((3, 4))
x.reshape(-1, 3) # 将会变形为(4,3)
```

`sum`函数在不指定参数时表示全体求和。指定`axis`时，表示对某一方向求和：

```python
x = torch.arange(16).reshape(4, 4)
x.sum(axis=0) # 逐列求和
# tensor([24, 28, 32, 36])
x.sum(axis=1) # 逐行求和
# tensor([ 6, 22, 38, 54])
x.sum(axis=1, keepdims=True) # 逐行求和，不转置答案
# tensor([[6], [22], [38], [54]])
```

## pandas

```python
import pandas as pd
```

- 读取数据表：`x = pd.read_csv(f)`
- 截取切片：`x.iloc[:, 0:2]`（前两列）
- 填充空值为统一数值：`x.fillna(value)`
- 对每列取平均：`x.mean(...)`
- 拆分离散列：`pd.get_dummies(x, ...)`
- 转换为ndarray：`x.values`
- 转换为torch张量：先转换为ndarray，再取`torch.tensor(arr)`

如果某些列是字符串而取不了平均值，考虑只转换数值列：

```python
x.mean(numeric_only=True)
```

拆分离散列尤其适合只包含字符串和NaN两种类型的列。如果包含NaN，则需要指定`dummy_na`为真：

```python
y = pd.get_dummies(x, dummy_na=True)
```

values属性可以返回一个ndarray。但如果要用ndarray转换为torch张量，则可能需要统一ndarray的数据类型：

```python
z = torch.tensor(y.values.astype(float))
```

## 基于tensor的线性代数

- 转置：`x.T`
- 矩阵乘法：`x.matmul(y)`、`torch.mm(x, y)`
- 向量点积：`torch.dot(a, b)`
- 矩阵-向量积：`torch.mv(A, x)`
- 向量模/L2范数：`torch.norm(a)`
	- L1范数：`torch.abs(a).sum()`

## 自动微分

自动微分是一种计算导数的技术，可以边运算边求解一些复杂函数的导数，在深度学习中（尤其是以梯度下降为核心的优化过程中）特别常用。

`torch.tensor`支持在运算过程中自动计算梯度，但这个功能默认是关闭的，需要人为打开：

```python
x = torch.arange(4)
x.requires_grad_(True) # 表示张量x需要更新梯度
```

假如需要对函数$y=2x^Tx$关于向量$x$求导。一个量（数、向量、矩阵）对向量求导的结果是，对向量各成员偏导组成的向量：
$$
y_x'=\left(\dfrac{\partial y}{\partial x_1},\dfrac{\partial y}{\partial x_2},\dfrac{\partial y}{\partial x_3}\right)^T
$$
基于此，求导结果为：
$$
y_x'=(4x_1,4x_2,4x_3)^T
$$

在tensor创建之时，由于tensor尚未参与任何计算，所以梯度记录为空：

```python
print(x.grad) # None
```

现在计算`y`。运算过程中，发生了梯度的正向传播：

```python
# x = torch.Tensor([0, 1, 2, 3])
y = 2 * torch.dot(x, x)
print(y) # tensor(28, grad_fn=<...>)
```

执行梯度的反向传播。此时`x`获得梯度：

```python
y.backward()
print(x.grad) # tensor([0, 4, 8, 12])
```

这些梯度值实际上就是$4x$在$x=0,1,2,3$处分别的取值。

注意，梯度默认会累加，所以在下一次运算前，如果希望重新计算梯度，则需要将其清除：

```python
x.grad.zero_()
```

## 查API

最基础的方式是查博客+文档。

在Python内，可以通过`dir`函数查看一个模块对外提供了哪些接口：

```python
import torch

print(dir(torch.distributions))
print(dir(torch.nn))
```