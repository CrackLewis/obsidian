
多层感知机是最简单的深度神经网络。

核心问题：
- 网络结构：隐藏层、激活函数
- 过拟合、欠拟合、模型选择
- 正则化技术：权重衰减、暂退法等
- 数值稳定性、参数初始化

## 简介

线性模型的弊端：输出和输入不一定是单调线性关系。

解决方案：在网络中引入一或多个隐藏层。隐藏层在输入层和输出层之间，引入隐藏层可以克服线性模型的限制，从而使网络能够处理更复杂的函数关系类型。

![[Pasted image 20240829104418.png]]

具有1个输入层、1个输出层和至少一个隐藏层的神经网络称为*多层感知机*（multi-layer perceptron，MLP）。这种网络可以在一定的输入范围内，拟合非线性的函数关系。

对于单个隐藏层的MLP，假设：
- 批次大小为$n$，输入维度为$d$，模型输入为$X_{n\times d}$；
- 隐藏层特征数为$h$，隐藏层输出为$H_{n\times h}$；
- 输出维度为$q$，模型输出为$O_{n\times q}$。

则对于隐藏层输出$H$，有：
$$
H_{n\times h} = X_{n\times d} \cdot W_{d\times h}^{(1)}+b_{n\times h}^{(1)}
$$
对于模型输出$O$，有：
$$
O_{n\times q}=H_{n\times h}\cdot W_{h\times q}^{(2)}+b_{n\times q}^{(2)}
$$
基于上述两步的方程是：
$$
\begin{split}
O&=XW+b \\
W&=W^{(1)}W^{(2)} \\
b&=b^{(1)}W^{(2)}+b^{(2)}
\end{split}
$$

进一步，为发挥多层架构潜力，引入*激活函数*（activation function），将其应用于隐藏层输出中。激活函数输出称为*活性值*（activations）：
$$
\begin{split}
H&=\sigma(XW^{(1)}+b^{(1)}) \\
O&=HW^{(2)}+b^{(2)}
\end{split}
$$
注意：
- 激活函数$\sigma(\cdot)$单次只计算一行。
- 应用了激活函数的模型不可能退化回线性模型。
- 激活函数可以应用到每个隐藏层，而不必是单个；每层的激活函数可以不同。

## 激活函数

最常用的有ReLU、sigmoid、tanh。

*ReLU函数*：又称修正线性单元（rectified linear unit，ReLU）函数，它是元素和0取最大值：
$$
\text{ReLU}(x)=\max(x,0)
$$
该函数最受欢迎，实现简单，表现也较良好。它的导数非0即1，要么让参数消失，要么通过，可以解决梯度消失问题。

有一种称为参数化ReLU（parameterized ReLU，pReLU）的变体，允许部分负面参数通过：
$$
\text{pReLU}(x)=\max(x,0)+\alpha \min(x,0)
$$
绘制函数：

```python
x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.relu(x)
d2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))
```

![[Pasted image 20240829165724.png]]

*sigmoid函数*：又称挤压函数，它可以将参数挤压至$(0,1)$区间：
$$
\text{sigmoid}(x)=\dfrac{1}{1+\exp(-x)}
$$
它的导数满足：
$$
\dfrac{d}{dx}\text{sigmoid}(x)=\dfrac{\exp(-x)}{(1+\exp(-x))^2}=\text{sigmoid}(x)\cdot (1-\text{sigmoid}(x))
$$
绘制函数：
```python
x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.sigmoid(x)
d2l.plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', figsize=(5, 2.5))
```

![[Pasted image 20240829165925.png]]

*tanh函数*：双曲正切函数。与sigmoid类似，它也压缩实数区间，但是压到$(-1,1)$区间：
$$
\tanh(x)=\dfrac{1-\exp(-2x)}{1+\exp(-2x)}
$$
其导数满足：
$$
\dfrac{d}{dx}\tanh(x)=1-\tanh^2(x)
$$
绘制函数：
```python
x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.tanh(x)
d2l.plot(x.detach(), y.detach(), 'x', 'tanh(x)', figsize=(5, 2.5))
```

![[Pasted image 20240829170319.png]]

## 多层感知机实现

MLP的实现仍然是基于Fashion-MNIST的图片分类模型。

### 数据集、模型参数

导入包、读取数据集、设置批次大小：

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```

输入参数为784个，输出参数为10个。将隐藏层参数设置为256个，并赋予初值：

```python
num_inputs, num_outputs, num_hiddens = 784, 10, 256

W1 = nn.Parameter(torch.randn(
    num_inputs, num_hiddens, requires_grad=True) * 0.01)
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))
W2 = nn.Parameter(torch.randn(
    num_hiddens, num_outputs, requires_grad=True) * 0.01)
b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))

params = [W1, b1, W2, b2]
```

### 模型

激活函数采用ReLU：

```python
def relu(X):
    a = torch.zeros_like(X)
    return torch.max(X, a)
```

损失函数采用交叉熵损失：

```python
loss = nn.CrossEntropyLoss(reduction='none')
```

模型采用隐藏层，所以需要声明一个张量存储隐藏层的运算结果：

```python
def net(X):
    X = X.reshape((-1, num_inputs))
    H = relu(X@W1 + b1)  # 这里“@”代表矩阵乘法
    return (H@W2 + b2)
```

### 训练和评估

```python
num_epochs, lr = 10, 0.1
updater = torch.optim.SGD(params, lr=lr)
train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)

predict_ch3(net, test_iter)
```

### 基于PyTorch的改进

PyTorch以一种声明式的方式组建网络模型。`torch.nn.Sequential`将多个网络层串联起来，`torch.nn.Flatten`是扁平化层，`nn.Linear(in_feats, out_feats)`是全连接层，`nn.ReLU`则是ReLU套壳。

组建图片分类模型可以通过下列源码简单实现：

```python
# 声明网络模型为net
net = nn.Sequential(nn.Flatten(),
                    nn.Linear(784, 256),
                    nn.ReLU(),
                    nn.Linear(256, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

# 将init_weights函数递归地作用于网络模型内的所有
net.apply(init_weights)
```

其余的部分改动不大。

## 模型选择、欠拟合、过拟合

背景：
- 模型需要真正发现泛化的模式，而非简单地记忆数据。