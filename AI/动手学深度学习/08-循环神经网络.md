
WIP

## 8.1-序列模型

对序列数据的各项进行预测，可以依据对应项前面若干项的实际值计算：
$$
x_t\sim P(x_t\mid x_1,x_2,\cdots,x_{t-1})
$$

*自回归模型*：开辟一个长度为$\tau$的窗口，根据最近$\tau$个数据预测：
$$
x_t \sim P(x_{t} \mid x_{t-\tau},x_{t-\tau+1},\cdots,x_{t-1})
$$

*隐变量自回归模型*：保留一些对过去观测的总结$h_t$，同时更新$\hat{x}_t,h_t$：
$$
\begin{split}
\hat{x}_t & = P(x_t\mid h_t) \\
h_t &= g(h_{t-1},x_{t-1})
\end{split}
$$


*马尔可夫模型*：若$\tau=1$，则可通过链式计算推断结果
$$
\begin{split}\begin{aligned}
P(x_{t+1} \mid x_{t-1})
&= \frac{\sum_{x_t} P(x_{t+1}, x_t, x_{t-1})}{P(x_{t-1})}\\
&= \frac{\sum_{x_t} P(x_{t+1} \mid x_t, x_{t-1}) P(x_t, x_{t-1})}{P(x_{t-1})}\\
&= \sum_{x_t} P(x_{t+1} \mid x_t) P(x_t \mid x_{t-1})
\end{aligned}\end{split}
$$

D2L官网提供了一个序列预测的代码，在8.1小节，有空看一下

## 8.2-文本预处理

*读取数据集*：

*词元化*（tokenize）：以文本行为输入，输出组成该行的单词序列。

*词表*（vocabulary）：单词序列到数字ID的映射。只包含出现次数足够多的单词。
- 单词序列可通过词表转化为ID序列，方便后续学习。

注意：
- 词元化的单位也有可能是字符，而非单词。
- 每个文本行不一定是段落，也有可能是句子、单词等。

## 8.3-语言模型和数据集

语言模型的目标是估计序列的联合概率：
$$
P(x_1,x_2,\cdots,x_T)
$$

语言模型的好处：避免一些明显错误的语言模式

### 学习语言模型

设连续单词序列$x_1,x_2,\cdots,x_T$的出现频数是$n(x_1,x_2,\cdots,x_T)$，出现频率是$P(x_1,x_2,\cdots,x_T)$。

注意到当$T=1$时，频率即单个单词占语料总词数的比例；$T\ge 2$时则可根据条件概率推算：
$$
P(x_1,x_2,x_3)=P(x_3\mid x_1,x_2)\cdot P(x_2\mid x_1)\cdot P(x_1)
$$

考虑到当$n$（语料总词数）很大时，概率值会极小，可考虑运用*Laplace平滑*方法，为概率分式添加一个小量：
$$
\begin{split}\begin{aligned}
    \hat{P}(x) & = \frac{n(x) + \epsilon_1/m}{n + \epsilon_1}, \\
    \hat{P}(x' \mid x) & = \frac{n(x, x') + \epsilon_2 \hat{P}(x')}{n(x) + \epsilon_2}, \\
    \hat{P}(x'' \mid x,x') & = \frac{n(x, x',x'') + \epsilon_3 \hat{P}(x'')}{n(x, x') + \epsilon_3}.
\end{aligned}\end{split}
$$

其中$\epsilon_1,\epsilon_2,\epsilon_3$等超参数是极小的常量。

但这种语言模型有效程度不高：
- 所有的计数都要储存
- 单词的含义被忽略，很难辨析哪些单词总会共同出现在上下文中
- 长单词序列大部分是没出现过的，所以纯概率统计模型8太行

### 马尔可夫n元语法

这个其实前文出现过：
$$
P(x_1,x_2,x_3)=P(x_3\mid x_1,x_2)\cdot P(x_2\mid x_1)\cdot P(x_1)
$$
出现了几个单词变量就称作几元语法，如这个公式是三元语法。

### 自然语言统计

自然语料的词频大致符合*齐普夫定律*（Zipf’s law）：单词频度$n_i$大致与常见度排名$i$的某个幂呈反比关系。
$$
n_i \propto \frac{1}{i^\alpha},
$$

