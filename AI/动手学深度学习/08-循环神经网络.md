
WIP

## 8.1-序列模型

对序列数据的各项进行预测，可以依据对应项前面若干项的实际值计算：
$$
x_t\sim P(x_t\mid x_1,x_2,\cdots,x_{t-1})
$$

*自回归模型*：开辟一个长度为$\tau$的窗口，根据最近$\tau$个数据预测：
$$
x_t \sim P(x_{t} \mid x_{t-\tau},x_{t-\tau+1},\cdots,x_{t-1})
$$

*隐变量自回归模型*：保留一些对过去观测的总结$h_t$，同时更新$\hat{x}_t,h_t$：
$$
\begin{split}
\hat{x}_t & = P(x_t\mid h_t) \\
h_t &= g(h_{t-1},x_{t-1})
\end{split}
$$


*马尔可夫模型*：若$\tau=1$，则可通过链式计算推断结果
$$
\begin{split}\begin{aligned}
P(x_{t+1} \mid x_{t-1})
&= \frac{\sum_{x_t} P(x_{t+1}, x_t, x_{t-1})}{P(x_{t-1})}\\
&= \frac{\sum_{x_t} P(x_{t+1} \mid x_t, x_{t-1}) P(x_t, x_{t-1})}{P(x_{t-1})}\\
&= \sum_{x_t} P(x_{t+1} \mid x_t) P(x_t \mid x_{t-1})
\end{aligned}\end{split}
$$

D2L官网提供了一个序列预测的代码，在8.1小节，有空看一下

## 8.2-文本预处理

*读取数据集*：

*词元化*（tokenize）：以文本行为输入，输出组成该行的单词序列。

*词表*（vocabulary）：单词序列到数字ID的映射。只包含出现次数足够多的单词。
- 单词序列可通过词表转化为ID序列，方便后续学习。

注意：
- 词元化的单位也有可能是字符，而非单词。
- 每个文本行不一定是段落，也有可能是句子、单词等。

## 8.3-语言模型和数据集

语言模型的目标是估计序列的联合概率：
$$
P(x_1,x_2,\cdots,x_T)
$$

语言模型的好处：避免一些明显错误的语言模式

### 学习语言模型

设连续单词序列$x_1,x_2,\cdots,x_T$的出现频数是$n(x_1,x_2,\cdots,x_T)$，出现频率是$P(x_1,x_2,\cdots,x_T)$。

注意到当$T=1$时，频率即单个单词占语料总词数的比例；$T\ge 2$时则可根据条件概率推算：
$$
P(x_1,x_2,x_3)=P(x_3\mid x_1,x_2)\cdot P(x_2\mid x_1)\cdot P(x_1)
$$

考虑到当$n$（语料总词数）很大时，概率值会极小，可考虑运用*Laplace平滑*方法，为概率分式添加一个小量：
$$
\begin{split}\begin{aligned}
    \hat{P}(x) & = \frac{n(x) + \epsilon_1/m}{n + \epsilon_1}, \\
    \hat{P}(x' \mid x) & = \frac{n(x, x') + \epsilon_2 \hat{P}(x')}{n(x) + \epsilon_2}, \\
    \hat{P}(x'' \mid x,x') & = \frac{n(x, x',x'') + \epsilon_3 \hat{P}(x'')}{n(x, x') + \epsilon_3}.
\end{aligned}\end{split}
$$

其中$\epsilon_1,\epsilon_2,\epsilon_3$等超参数是极小的常量。

但这种语言模型有效程度不高：
- 所有的计数都要储存
- 单词的含义被忽略，很难辨析哪些单词总会共同出现在上下文中
- 长单词序列大部分是没出现过的，所以纯概率统计模型8太行

### 马尔可夫n元语法

这个其实前文出现过：
$$
P(x_1,x_2,x_3)=P(x_3\mid x_1,x_2)\cdot P(x_2\mid x_1)\cdot P(x_1)
$$
出现了几个单词变量就称作几元语法，如这个公式是三元语法。

### 自然语言统计

自然语料的词频大致符合*齐普夫定律*（Zipf’s law）：单词（或词组）的频度$n_i$大致与其常见度排名$i$的某个幂呈反比关系。
$$
\log n_i+\alpha \log i=C
$$


例如：最常见的单词会在语料内出现成千上万次，较常见的可能出现数十或数百次，而绝大部分单词出现的次数不足10次。

最常见的词一般比较无聊（如：the、and、of、I、to等），称作*停用词*（stop words），在有些任务内会过滤掉。但它们本身仍是有意义的。

最常见的词组也有很大概率包含一或多个停用词。

### 读取长序列

当序列变得太长而不能被模型一次性全部处理时， 可能希望拆分这样的序列方便模型读取。

总体策略：
- 假设用神经网络训练语言模型，则网络每次都处理一个具有预定义长度（如$n$个时间步）的小批量序列
- 考虑划分原始文本为若干批次，每个批次包含若干段长度为$n$的子序列作为样本，样本往后移位一个词元的子序列作为标签，喂给模型学习

*随机采样*：在随机采样中，每个样本都是在原始的长序列上任意捕获的子序列。 在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻。 对于语言建模，目标是基于到目前为止我们看到的词元来预测下一个词元， 因此标签是移位了一个词元的原始序列。

*顺序分区*：在迭代过程中，除了对原始序列可以随机抽样外， 我们还可以保证两个相邻的小批量中的子序列在原始序列上也是相邻的。 这种策略在基于小批量的迭代过程中保留了拆分的子序列的顺序，因此称为顺序分区。

## 8.4-循环神经网络

隐变量模型：用一个*隐藏变量*（隐状态，hidden variable）$h_t$存储历史信息（前$n-1$个词元），替代原先的条件概率模型：
$$
\begin{split}
P(x_t \mid h_{t-1})&\approx P(x_t \mid x_{t-1}, \ldots, x_1) \\
h_t&=f(x_t,h_{t-1})
\end{split}
$$

与输入、输出间的隐藏层不同，隐藏变量直接参与后续隐变量的生成。

### 无隐状态的神经网络

前面所提[[04-多层感知机|多层感知机]]和[[06-卷积神经网络|CNN]]都属于无隐状态的神经网络。

*注意*：隐藏层变量不算隐状态。

### 有隐状态的神经网络