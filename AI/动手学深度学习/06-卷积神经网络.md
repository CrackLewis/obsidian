
全连接神经网络并不能很好地处理图像数据：由于数据被展平为一维向量，网络不能有效学习图像的空间结构信息。

*卷积神经网络*（convolutional neural network，CNN）：一类为图像数据处理设计的神经网络。

内容：
- 卷积层
- 填充、步幅
- 汇聚层（池化层）
- 多通道
- 现代卷积网络架构

## 从全连接层到卷积

假如图像采用全连接层：
- 1080P图像=1920x1080x3个变量=$6.22\times 10^6$维样本
- 输出为1000维的隐藏层：$10^9$级别的结点数，占用数十或数百GB
- 缩小图像：图像特征损失

*不变性*（invariance）：
- 平移不变性（translation invariance）：图像的特征与位置无关，正确的模型应当能够在图像的任何位置发现已学习的特征。
- 局部性（locality）：神经网络的前面几层应当只探索图像的局部，而不过渡在意图像中相距较远区域的关系。

### 多层感知机的局限性

设MLP的输入是二维图像$X_{n\times m}$，其隐藏表示为$H_{n\times m}$，均为二维张量。

假设从图像输入到隐藏表示需要由$W_{n\times m\times n\times m}$加权，偏置为$U_{n\times m}$，则隐藏表示$H_{ij}$可如下确定：
$$
\begin{split}
H_{ij}&=U_{ij}+\sum_{k}\sum_{l} W_{ijkl} \cdot X_{kl} \\
&= U_{ij}+\sum_{a}\sum_{b} V_{ijab} \cdot X_{i+a,j+b}
\end{split}
$$
其中$V$是$W$的形式变换，$V_{i,j,(i+a),(j+b)}=W_{ijkl}$。

考虑前面所说的*平移不变性*原则：如果特征在$X$中发生了平移，应当只导致$H$中的对应平移。也就是说，$V,U$不依赖于$(i,j)$的值，即$\mathbf{V}_{ab}=V_{ijab}$，且$u=U_{ij}$是常数。此时定义变为：
$$
H_{ij}=u+\sum_{a}\sum_{b} \mathbf{V}_{ab} \cdot X_{i+a,j+b}
$$
这便是通俗意义上的*卷积*（convolution），其中$\mathbf{V}$是应用于$X$的权值，权重数则由$O(n^2m^2)$减少到$O(nm)$。

再考虑*局部性*原则：为了收集$H_{ij}$的相关信息，局部性原则要求不能收集距离$(i,j)$太远的信息。因此可以设置一个限界$\Delta$，当$|a|>\Delta$或$|b|>\Delta$时，设$\mathbf{V}_{ij}=0$。此时定义变为：
$$
H_{ij}=u+\sum_{a=-\Delta}^\Delta \sum_{b=-\Delta}^\Delta \mathbf{V}_{ab} X_{i+a,j+b}
$$
负责这种变换的神经网络层称为卷积层：
- $\mathbf{V}_{ab}$称为*卷积核*（convolution kernel）、*滤波器*（filter）或权重，$u$被称为偏置
- 如果特征较小，模型需要的参数也非常少

参数减少的代价：特征现在是平移不变的，并且当确定每个隐藏活性值时，每一层只包含局部的信息。以上所有的权重学习都将依赖于归纳偏置：
- 偏置与现实相符时，能得到样本有效的模型，模型能很好地泛化到未知数据中。
- 偏置与现实不符时，比如当图像不满足平移不变时，模型可能难以拟合我们的训练数据。

### 数学卷积

数学上，两个函数$f,g: \mathbb{R}^d \rightarrow \mathbb{R}$的卷积为：
$$
(f*g)(\mathbf{x})=\int f(\mathbf{z})g(\mathbf{x}-\mathbf{z})d\mathbf{z}
$$
当定义域退化为离散集合时，卷积退化为求和：
$$
(f*g)(i)=\sum_{a} f(a)g(i-a)
$$
对于二维张量，卷积为：
$$
(f*g)(i,j)=\sum_{a} \sum_{b} f(a,b)g(i-a,j-b)
$$
这和卷积层的表达式比较类似，但后者更准确描述了*互相关*（cross-correlation）。

### 通道

图像并非局限于宽、高的二维对象，实际上还有第三维：*通道*（channel）。最广泛应用的RGB图像的通道数为3，分别表示一个颜色分量。

在应用了通道的模型中：
- 输入$X_{i,j,k}$和输出$H_{i,j,k}$的第三维都表示通道。
- 中间参数需要添加两维：$\text{V}_{i,j,k,l}$中的$k,l$表示源通道和目标通道。

此时，（不考虑偏置的）$H$计算式：
$$
H_{i,j,d}=\sum_{a=-\Delta}^{\Delta} \sum_{b=-\Delta}^{\Delta} \sum_{c} \text{V}_{a,b,c,d}\cdot X_{i+a,j+b,c}
$$

## 图像卷积

### 互相关运算

卷积层执行的实际上并不是数学意义上的卷积运算，而是*互相关运算*（cross-correlation）：输入张量和卷积核张量通过这种运算产生输出张量。

在二维的互相关运算中，卷积核在输入张量上从左到右、从上到下滑动，依次生成结果：
$$
\begin{split}
\left(\begin{matrix}
0&1&2\\3&4&5\\6&7&8
\end{matrix}\right) \otimes 
\left(\begin{matrix}
0&1\\2&3
\end{matrix}\right)&=
\left(\begin{matrix}
0\times 0+1\times 1+3\times 2+4\times 3 & 1\times 0+2\times1+4\times 2+5\times 3 \\
3\times 0+4\times 1+6\times 2+7\times 3 & 4\times0+5\times 1+7\times 2+8\times 3
\end{matrix}\right) \\
&=\left(\begin{matrix}
19&25\\37&43
\end{matrix}\right)
\end{split}
$$

设输入张量规模为$n_h\times n_w$，卷积核规模为$k_h\times k_w$，若不考虑后面章节的填充、步幅等，则输出规模为：
$$
(n_h-k_h+1)\times (n_w-k_w+1)
$$

在PyTorch上实现互相关计算：

```python
import torch
from torch import nn
from d2l import torch as d2l

def corr2d(X, K):  #@save
    """计算二维互相关运算"""
    h, w = K.shape
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    # 模拟卷积窗口的滑动
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()
    return Y

X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])
K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])
corr2d(X, K)
# 输出：tensor([[19., 25.], [37., 43.]])
```

### 卷积层

PyTorch的内置卷积层是`torch.nn.Conv2d`。这个层在后续会大量使用。

可以利用上面实现的`corr2d`函数实现一个自定义的卷积层，但它当然远不如PyTorch的内置卷积层强大：

```python
class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super().__init__()
        self.weight = nn.Parameter(torch.rand(kernel_size))
        self.bias = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        return corr2d(x, self.weight) + self.bias
```

### 目标边缘检测

假设输入矩阵$X_{8\times 8}$为一个空心正方形：
$$
X_{ij}=\left\{\begin{matrix}
0, & 3\le i\le 6, \\
1, & \text{otherwise}.
\end{matrix}\right.
$$
构造一个卷积核$K_{1\times 2}=(1,-1)$。则隐藏输出为：
$$
H_{ij}=\sum_{a=0}^1 K_{1,a+1}\cdot X_{i,j+a}
$$
对于$X_{8\times 8}$，输出为：
$$
\begin{split}
H&=\left(\begin{matrix}
1&1&1&1&1&1&1&1\\
1&1&1&1&1&1&1&1\\
1&1&0&0&0&0&1&1\\
1&1&0&0&0&0&1&1\\
1&1&0&0&0&0&1&1\\
1&1&0&0&0&0&1&1\\
1&1&1&1&1&1&1&1\\
1&1&1&1&1&1&1&1
\end{matrix}\right)\otimes \left(\begin{matrix}1&-1\end{matrix}\right)\\
&=\left(\begin{matrix}
0&0&0&0&0&0&0\\
0&0&0&0&0&0&0\\
0&1&0&0&0&-1&0\\
0&1&0&0&0&-1&0\\
0&1&0&0&0&-1&0\\
0&1&0&0&0&-1&0\\
0&0&0&0&0&0&0\\
0&0&0&0&0&0&0
\end{matrix}\right)
\end{split}
$$

可见卷积核$K$能够探测水平方向上的数值突变（或者称为：垂直边界），并反映到隐藏输出。

对应Python的代码如下：

```python
X = torch.ones((8, 8))
X[2:6, 2:6] = 0

K = torch.tensor([[1.0, -1.0]])

Y = corr2d(X, K)
print(Y)
```

### 学习卷积核

如果只知道输入$X$和隐藏输出$Y$，而不知道卷积核$K$，则可以通过若干轮梯度下降的方式学习出卷积核。

实现：

```python
# 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核
conv2d = nn.Conv2d(1,1, kernel_size=(1, 2), bias=False)

# 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），
# 其中批量大小和通道数都为1
X = torch.ones((8, 8))
X[2:6, 2:6] = 0
K = torch.tensor([[1.0, -1.0]])
Y = corr2d(X, K)
print(Y)

X = X.reshape((1, 1, 8, 8))
Y = Y.reshape((1, 1, 8, 7))
lr = 1.2e-2  # 学习率

for i in range(30):
    Y_hat = conv2d(X)
    l = (Y_hat - Y) ** 2
    conv2d.zero_grad()
    l.sum().backward()
    # 迭代卷积核
    conv2d.weight.data[:] -= lr * conv2d.weight.grad
    if (i + 1) % 2 == 0:
        print(f'epoch {i+1}, loss {l.sum():.3f}')

print(conv2d.weight.data)
```

诡异的是，上述代码一旦将学习率`lr`稍微调高一点，就会导致极其严重的过拟合。这可能是未使用优化器导致的。

### 互相关和卷积

互相关运算并不完全等价于严格卷积运算。但为了与文献术语保持一致，这里将继续称互相关运算为卷积运算。

### 特征映射、感受野

*特征映射*（feature map）：根据卷积核执行卷积运算，由输入特征形成输出的卷积层。

对于某一层的任意元素$x$，*感受野*（receptive field）是指在前向传播过程中可能影响$x$计算的所有元素（来自先前层）。感受野可以大于输入的实际大小。

## 填充、步幅

本节中，假设输入规模为$n_h\times n_w$，卷积核规模为$k_h\times k_w$。

### 填充

*填充*（padding）：卷积层输出一般比输入小，对输出进行填充可以解决边缘像素随着卷积层前向传播而丢失的问题。

如果填充$p_h$行和$p_w$列，则卷积层的输出规模为：
$$
(n_h-k_h+p_h+1)\times (n_w-k_w+p_w+1)
$$

<div style="background: white">
<img src="https://zh-v2.d2l.ai/_images/conv-pad.svg"/>
</div>

如果$p_h=k_h-1,p_w=k_w-1$，则输出规模等于输入规模，此时模型的输出形状更易于预测。

填充通常会在输入的顶部和底部同时填充，行列数尽量相等。所以卷积和规模$k_h,k_w$一般采用奇数。

例如：给定输入输出通道均为1、卷积核为$3\times 3$、填充为1的卷积层，假设输入为$8\times 8$，则输出规模也是$8\times 8$：

```python
import torch
from torch import nn

# 为了方便起见，我们定义了一个计算卷积层的函数。
# 此函数初始化卷积层权重，并对输入和输出提高和缩减相应的维数
def comp_conv2d(conv2d, X):
    # 这里的（1，1）表示批量大小和通道数都是1
    X = X.reshape((1, 1) + X.shape)
    Y = conv2d(X)
    # 省略前两个维度：批量大小和通道
    return Y.reshape(Y.shape[2:])

# 请注意，这里每边都填充了1行或1列，因此总共添加了2行或2列
# 前两个参数表示输入和输出通道数，均为1
conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1)
X = torch.rand(size=(8, 8))
comp_conv2d(conv2d, X).shape
# 输出：torch.Size([8, 8])
```

如果卷积核改成$5\times 3$，但填充改为$2\times 1$，则输出规模仍旧不变：

```python
conv2d = nn.Conv2d(1, 1, kernel_size=(5, 3), padding=(2, 1))
comp_conv2d(conv2d, X).shape
# 输出：torch.Size([8, 8])
```

### 步幅

*步幅*（stride）：卷积核每次默认滑动一个位置。但有时为了减少采样次数或高效计算，可能需要通过提升*单次滑动距离*，从而减少输出规模。步幅即指卷积核单次滑动的元素数。

如果水平步幅为$s_w$，垂直步幅为$s_h$，则输出规模为：
$$
\left\lfloor\dfrac{n_h-k_h+p_h+s_h}{s_h}\right\rfloor\times \left\lfloor\dfrac{n_w-k_w+p_w+s_w}{s_w}\right\rfloor
$$

<div style="background: white">
<img src="https://zh-v2.d2l.ai/_images/conv-stride.svg"/>
</div>

特别地，如果$p_h=k_h-1,p_w=k_w-1$，则输出规模为：
$$
\left\lfloor\dfrac{n_h+s_h-1}{s_h}\right\rfloor\times \left\lfloor\dfrac{n_w+s_w-1}{s_w}\right\rfloor
$$
若$s_h|n_h$，则输出规模恰好为：$(n_h/s_h)\times (n_w/s_w)$。

例如：若卷积核为$3\times 3$，填充为$2\times 2$，步幅为$2\times 2$，则输出规模恰为输入规模的一半。

```python
X = torch.rand(size=(8, 8))
# padding=1表示首尾各填充一行一列
conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2)
comp_conv2d(conv2d, X).shape
# 输出：torch.Size([4, 4])
```

若卷积核为$3\times 5$，填充为$0\times 2$，步幅为$3\times 4$，则：

```python
X = torch.rand(size=(8, 8))
conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))
comp_conv2d(conv2d, X).shape
# 输出：torch.Size([2, 2])
```

### 相关用语

- 填充：
	- 填充为$p$：首尾行（列）均填充$p$行（列）。
	- 填充为$(p_h,p_w)$：首尾行填充$p_h$行，首尾列填充$p_w$列。
- 步幅：
	- 步幅为$s$：水平和垂直步幅均为$s$。
	- 步幅为$(s_h,s_w)$：水平步幅为$s_h$，垂直步幅为$s_w$。

## 多通道

图像可能由多个*通道*（channel）组成，每个通道描述图像的一个分量。

### 多输入通道

假设输入数据包含$c_i$个通道，输出只有一个通道。

那么为了产生输出，必须对每个输入通道都应用卷积核，因此卷积层的卷积核也应当有$c_i$个通道，其规模为$c_i\times k_h\times k_w$。

下图中：输入为$2\times 3\times 3$，核为$2\times 2\times 2$，输出为$1\times 2\times 2$。

<div style="background: white">
<img src="https://zh-v2.d2l.ai/_images/conv-multi-in.svg"/>
</div>

等价于下列代码：

```python
import torch
from d2l import torch as d2l

def corr2d_multi_in(X, K):
    # 先遍历“X”和“K”的第0个维度（通道维度），再把它们加在一起
    # zip用于融合两个序列
    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))

X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],
               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])
K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])

corr2d_multi_in(X, K)
# 输出：tensor([[ 56.,  72.], [104., 120.]])
```

### 多输出通道

多个输出通道是必要的：
- 近年流行深度网络而非广度网络，前者的学习能力更强。
- 每个通道都可以看作模型对数据中不同特征的响应，为学习到足够的内容，必须增加通道数量。

假设输出通道数为$c_o$，输入通道数为$c_i$，则必须对每一个输入和输出通道都维护一个卷积核，因此模型的卷积核规模为：
$$
c_o\times c_i\times k_h\times k_w
$$

计算多通道输出的互相关函数：

```python
def corr2d_multi_in_out(X, K):
    # 迭代“K”的第0个维度，每次都对输入“X”执行互相关运算。
    # 最后将所有结果都叠加在一起
    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)

X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],
               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])

K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])
# 此时，K初始化为(2,2,2)的张量
K = torch.stack((K, K + 1, K + 2), 0)
# K和K+1,K+2复合，形状变为(3,2,2,2)
K.shape
# 输出：torch.Size([3, 2, 2, 2])

corr2d_multi_in_out(X, K)
```

### 1x1卷积层

$1\times 1$卷积层没有提取相邻像素间特征的能力。它的作用为：
- 对不同通道上的像素点进行线性组合，然后进行非线性操作，可以完成升降维。
- 可以调整网络层的通道数量、控制模型的复杂度。

## 汇聚层（池化层）

机器学习任务和全局图像有关（寻求图像内的目标特征）->末端元素的感受野应当包含全局元素->手动降低分辨率可以增大单个元素的感受野

*汇聚层*（pooling layer）目的：
- 降低卷积层对位置的敏感性，保持特征的平移不变性
- 降低对空间降采样表示的敏感性

汇聚操作：将规模为$p\times q$的*汇聚窗口*在图像上滑动，每个滑动位置计算一次窗口内的*汇聚指标*。又称$p\times q$汇聚

### 最大汇聚层、平均汇聚层

最大汇聚层计算窗口内最大值，平均汇聚层计算窗口内平均值

![](https://zh-v2.d2l.ai/_images/pooling.svg)

```python
import torch
from torch import nn
from d2l import torch as d2l

def pool2d(X, pool_size, mode='max'):
    p_h, p_w = pool_size
    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            if mode == 'max':
                Y[i, j] = X[i: i + p_h, j: j + p_w].max()
            elif mode == 'avg':
                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()
    return Y

X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])
pool2d(X, (2, 2))        # tensor([[4., 5.], [7., 8.]])
pool2d(X, (2, 2), 'avg') # tensor([[2., 3.], [5., 6.]])
```

### 汇聚层的填充、步幅

设填充为$(p_h,p_w)$，步幅为$(s_h,s_w)$，输入图像规模为$c\times h\times w$。则汇聚层输出规模为：
$$
c\times \left\lfloor\frac{h-p_h+s_h}{s_h}\right\rfloor\times \left\lfloor\frac{w-p_w+s_w}{s_w}\right\rfloor
$$

如无特殊说明，步幅与汇聚窗口规模一致，padding包含两端的填充元素数量。

可在`nn.MaxPool2d`或`nn.AvgPool2d`的参数中指定填充、步幅参数：

```python
pool2d = nn.MaxPool2d((3, 3), pooling=(1, 0), stride=(4, 4))
# pooling不允许超过汇聚窗口的一半大小
# stride默认与汇聚窗口等大
# 两者和窗口参数也可以是单个整数，表示宽高相同
```

### 汇聚层的多个通道

PyTorch中要求，汇聚层的输入/输出都必须包含至少一个高维度，即宽高两维+其他维度。高维度一般视作通道数和批次内编号，输入和输出的维度一致。

## 6.6-LeNet

since 1989, by LeCun et al.

### LeNet结构

总体结构：
- 卷积编码层：两个卷积单元组成
	- 卷积单元：卷积层+sigmoid激活函数+平均汇聚
- 全连接层密集块：三个全连接层组成

<div style="background-color: white">
<img src="https://zh-v2.d2l.ai/_images/lenet.svg"/>
</div>

简化结构：

<div style="background-color: white">
<img src="https://zh-v2.d2l.ai/_images/lenet-vert.svg"/>
</div>

### LeNet实现

LeNet-5的PyTorch实现：

```python
import torch
from torch import nn
from d2l import torch as d2l

net = nn.Sequential(
	# 卷积层1：1@28*28 -> 6@14*14
    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    # 卷积层2：6@14*14 -> 16@5*5
    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    # 扁平化：16@5*5 -> (400)
    nn.Flatten(),
    # 全连接层1：400 -> 120
    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),
    # 全连接层2：120 -> 84
    nn.Linear(120, 84), nn.Sigmoid(),
    # 全连接层3：84 -> 10
    nn.Linear(84, 10)
	# 原始模型中这里最后有一层高斯激活，被去掉了
)
```

数据集加载：

```python
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)
```

训练：

```python
def evaluate_accuracy_gpu(net, data_iter, device=None): #@save
    """使用GPU计算模型在数据集上的精度"""

	# 将数据强制移动到模型所在的计算设备（一般是GPU）上
    if isinstance(net, nn.Module):
        net.eval()  # 设置为评估模式
        if not device:
            device = next(iter(net.parameters())).device
            
    # metric = 正确预测的数量 / 总预测的数量
    metric = d2l.Accumulator(2)
    with torch.no_grad():
        for X, y in data_iter:
            if isinstance(X, list):
                # BERT微调所需的（之后将介绍）
                X = [x.to(device) for x in X]
            else:
                X = X.to(device)
            y = y.to(device)
            metric.add(d2l.accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]

def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):
    """用GPU训练模型(在第六章定义)"""

	# 用Xavier初始化方式设置参数
    def init_weights(m):
        if type(m) == nn.Linear or type(m) == nn.Conv2d:
            nn.init.xavier_uniform_(m.weight)
    net.apply(init_weights)

	# 在device上定义模型、损失函数、优化器
    print('training on', device)
    net.to(device)
    optimizer = torch.optim.SGD(net.parameters(), lr=lr)
    loss = nn.CrossEntropyLoss()

	# 什么花里胡哨的
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
                            legend=['train loss', 'train acc', 'test acc'])
    timer, num_batches = d2l.Timer(), len(train_iter)
    
    for epoch in range(num_epochs):
        # 训练损失之和，训练准确率之和，样本数
        metric = d2l.Accumulator(3)
        # 切换到训练模式：开启Dropout和BatchNorm
        net.train()
        for i, (X, y) in enumerate(train_iter):
            timer.start()
            optimizer.zero_grad() # 清除优化器梯度，防止梯度累积
            X, y = X.to(device), y.to(device)
            y_hat = net(X)
            l = loss(y_hat, y)
            l.backward() # 反向传播，计算loss对模型参数的梯度
            optimizer.step() # 由优化器更新模型参数
            with torch.no_grad():
                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])
            timer.stop()

			# 记录训练指标
            train_l = metric[0] / metric[2]
            train_acc = metric[1] / metric[2]
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (train_l, train_acc, None))
        test_acc = evaluate_accuracy_gpu(net, test_iter)
        animator.add(epoch + 1, (None, None, test_acc))

	# 训练完毕，输出结果
    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '
          f'test acc {test_acc:.3f}')
    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '
          f'on {str(device)}')

lr, num_epochs = 0.9, 10
train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```