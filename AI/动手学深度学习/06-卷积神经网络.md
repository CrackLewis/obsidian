
全连接神经网络并不能很好地处理图像数据：由于数据被展平为一维向量，网络不能有效学习图像的空间结构信息。

*卷积神经网络*（convolutional neural network，CNN）：一类为图像数据处理设计的神经网络。

内容：
- 卷积层
- 填充、步幅
- 汇聚层（池化层）
- 多通道
- 现代卷积网络架构

## 从全连接层到卷积

假如图像采用全连接层：
- 1080P图像=1920x1080x3个变量=$6.22\times 10^6$维样本
- 输出为1000维的隐藏层：$10^9$级别的结点数，占用数十或数百GB
- 缩小图像：图像特征损失

*不变性*（invariance）：
- 平移不变性（translation invariance）：图像的特征与位置无关，正确的模型应当能够在图像的任何位置发现已学习的特征。
- 局部性（locality）：神经网络的前面几层应当只探索图像的局部，而不过渡在意图像中相距较远区域的关系。

### 多层感知机的局限性

设MLP的输入是二维图像$X_{n\times m}$，其隐藏表示为$H_{n\times m}$，均为二维张量。

假设从图像输入到隐藏表示需要由$W_{n\times m\times n\times m}$加权，偏置为$U_{n\times m}$，则隐藏表示$H_{ij}$可如下确定：
$$
\begin{split}
H_{ij}&=U_{ij}+\sum_{k}\sum_{l} W_{ijkl} \cdot X_{kl} \\
&= U_{ij}+\sum_{a}\sum_{b} V_{ijab} \cdot X_{i+a,j+b}
\end{split}
$$
其中$V$是$W$的形式变换，$V_{i,j,(i+a),(j+b)}=W_{ijkl}$。

考虑前面所说的*平移不变性*原则：如果特征在$X$中发生了平移，应当只导致$H$中的对应平移。也就是说，$V,U$不依赖于$(i,j)$的值，即$\mathbf{V}_{ab}=V_{ijab}$，且$u=U_{ij}$是常数。此时定义变为：
$$
H_{ij}=u+\sum_{a}\sum_{b} \mathbf{V}_{ab} \cdot X_{i+a,j+b}
$$
这便是通俗意义上的*卷积*（convolution），其中$\mathbf{V}$是应用于$X$的权值，权重数则由$O(n^2m^2)$减少到$O(nm)$。

再考虑*局部性*原则：为了收集$H_{ij}$的相关信息，局部性原则要求不能收集距离$(i,j)$太远的信息。因此可以设置一个限界$\Delta$，当$|a|>\Delta$或$|b|>\Delta$时，设$\mathbf{V}_{ij}=0$。此时定义变为：
$$
H_{ij}=u+\sum_{a=-\Delta}^\Delta \sum_{b=-\Delta}^\Delta \mathbf{V}_{ab} X_{i+a,j+b}
$$
负责这种变换的神经网络层称为卷积层：
- $\mathbf{V}_{ab}$称为*卷积核*（convolution kernel）、*滤波器*（filter）或权重，$u$被称为偏置
- 如果特征较小，模型需要的参数也非常少

参数减少的代价：特征现在是平移不变的，并且当确定每个隐藏活性值时，每一层只包含局部的信息。以上所有的权重学习都将依赖于归纳偏置：
- 偏置与现实相符时，能得到样本有效的模型，模型能很好地泛化到未知数据中。
- 偏置与现实不符时，比如当图像不满足平移不变时，模型可能难以拟合我们的训练数据。

### 数学卷积

数学上，两个函数$f,g: \mathbb{R}^d \rightarrow \mathbb{R}$的卷积为：
$$
(f*g)(\mathbf{x})=\int f(\mathbf{z})g(\mathbf{x}-\mathbf{z})d\mathbf{z}
$$
当定义域退化为离散集合时，卷积退化为求和：
$$
(f*g)(i)=\sum_{a} f(a)g(i-a)
$$
对于二维张量，卷积为：
$$
(f*g)(i,j)=\sum_{a} \sum_{b} f(a,b)g(i-a,j-b)
$$
这和卷积层的表达式比较类似，但后者更准确描述了*互相关*（cross-correlation）。

### 通道

图像并非局限于宽、高的二维对象，实际上还有第三维：*通道*（channel）。最广泛应用的RGB图像的通道数为3，分别表示一个颜色分量。

在应用了通道的模型中：
- 输入$X_{i,j,k}$和输出$H_{i,j,k}$的第三维都表示通道。
- 中间参数需要添加两维：$\text{V}_{i,j,k,l}$中的$k,l$表示源通道和目标通道。

此时，（不考虑偏置的）$H$计算式：
$$
H_{i,j,d}=\sum_{a=-\Delta}^{\Delta} \sum_{b=-\Delta}^{\Delta} \sum_{c} \text{V}_{a,b,c,d}\cdot X_{i+a,j+b,c}
$$

## 图像卷积

### 互相关运算

卷积层执行的实际上并不是数学意义上的卷积运算，而是*互相关运算*（cross-correlation）：输入张量和卷积核张量通过这种运算产生输出张量。

在二维的互相关运算中，卷积核在输入张量上从左到右、从上到下滑动，依次生成结果：
$$
\begin{split}
\left(\begin{matrix}
0&1&2\\3&4&5\\6&7&8
\end{matrix}\right) \otimes 
\left(\begin{matrix}
0&1\\2&3
\end{matrix}\right)&=
\left(\begin{matrix}
0\times 0+1\times 1+3\times 2+4\times 3 & 1\times 0+2\times1+4\times 2+5\times 3 \\
3\times 0+4\times 1+6\times 2+7\times 3 & 4\times0+5\times 1+7\times 2+8\times 3
\end{matrix}\right) \\
&=\left(\begin{matrix}
19&25\\37&43
\end{matrix}\right)
\end{split}
$$

设输入张量规模为$n_h\times n_w$，卷积核规模为$k_h\times k_w$，若不考虑后面章节的填充、步幅等，则输出规模为：
$$
(n_h-k_h+1)\times (n_w-k_w+1)
$$

在PyTorch上实现互相关计算：

```python
import torch
from torch import nn
from d2l import torch as d2l

def corr2d(X, K):  #@save
    """计算二维互相关运算"""
    h, w = K.shape
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    # 模拟卷积窗口的滑动
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()
    return Y

X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])
K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])
corr2d(X, K)
# 输出：tensor([[19., 25.], [37., 43.]])
```

### 卷积层

PyTorch的内置卷积层是`torch.nn.Conv2d`。这个层在后续会大量使用。

可以利用上面实现的`corr2d`函数实现一个自定义的卷积层，但它当然远不如PyTorch的内置卷积层强大：

```python
class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super().__init__()
        self.weight = nn.Parameter(torch.rand(kernel_size))
        self.bias = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        return corr2d(x, self.weight) + self.bias
```

### 目标边缘检测

假设输入矩阵$X_{8\times 8}$为一个空心正方形：
$$
X_{ij}=\left\{\begin{matrix}
0, & 3\le i\le 6, \\
1, & \text{otherwise}.
\end{matrix}\right.
$$
构造一个卷积核$K_{1\times 2}=(1,-1)$。则隐藏输出为：
$$
H_{ij}=\sum_{a=0}^1 K_{1,a+1}\cdot X_{i,j+a}
$$
对于$X_{8\times 8}$，输出为：
$$
\begin{split}
H&=\left(\begin{matrix}
1&1&1&1&1&1&1&1\\
1&1&1&1&1&1&1&1\\
1&1&0&0&0&0&1&1\\
1&1&0&0&0&0&1&1\\
1&1&0&0&0&0&1&1\\
1&1&0&0&0&0&1&1\\
1&1&1&1&1&1&1&1\\
1&1&1&1&1&1&1&1
\end{matrix}\right)\otimes \left(\begin{matrix}1&-1\end{matrix}\right)\\
&=\left(\begin{matrix}
0&0&0&0&0&0&0\\
0&0&0&0&0&0&0\\
0&1&0&0&0&-1&0\\
0&1&0&0&0&-1&0\\
0&1&0&0&0&-1&0\\
0&1&0&0&0&-1&0\\
0&0&0&0&0&0&0\\
0&0&0&0&0&0&0
\end{matrix}\right)
\end{split}
$$

可见卷积核$K$能够探测水平方向上的数值突变（或者称为：垂直边界），并反映到隐藏输出。

对应Python的代码如下：

```python
X = torch.ones((8, 8))
X[2:6, 2:6] = 0

K = torch.tensor([[1.0, -1.0]])

Y = corr2d(X, K)
print(Y)
```

### 学习卷积核

