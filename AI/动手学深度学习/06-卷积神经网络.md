
全连接神经网络并不能很好地处理图像数据：由于数据被展平为一维向量，网络不能有效学习图像的空间结构信息。

*卷积神经网络*（convolutional neural network，CNN）：一类为图像数据处理设计的神经网络。

内容：
- 卷积层
- 填充、步幅
- 汇聚层（池化层）
- 多通道
- 现代卷积网络架构

## 从全连接层到卷积

假如图像采用全连接层：
- 1080P图像=1920x1080x3个变量=$6.22\times 10^6$维样本
- 输出为1000维的隐藏层：$10^9$级别的结点数，占用数十或数百GB
- 缩小图像：图像特征损失

*不变性*（invariance）：
- 平移不变性（translation invariance）：图像的特征与位置无关，正确的模型应当能够在图像的任何位置发现已学习的特征。
- 局部性（locality）：神经网络的前面几层应当只探索图像的局部，而不过渡在意图像中相距较远区域的关系。

### 多层感知机的局限性

设MLP的输入是二维图像$X_{n\times m}$，其隐藏表示为$H_{n\times m}$，均为二维张量。

假设从图像输入到隐藏表示需要由$W_{n\times m\times n\times m}$加权，偏置为$U_{n\times m}$，则隐藏表示$H_{ij}$可如下确定：
$$
\begin{split}
H_{ij}&=U_{ij}+\sum_{k}\sum_{l} W_{ijkl} \cdot X_{kl} \\
&= U_{ij}+\sum_{a}\sum_{b} V_{ijab} \cdot X_{i+a,j+b}
\end{split}
$$
其中$V$是$W$的形式变换，$V_{i,j,(i+a),(j+b)}=W_{ijkl}$。

考虑前面所说的*平移不变性*原则：如果特征在$X$中发生了平移，应当只导致$H$中的对应平移。也就是说，$V,U$不依赖于$(i,j)$的值，即$\mathbf{V}_{ab}=V_{ijab}$，且$u=U_{ij}$是常数。此时定义变为：
$$
H_{ij}=u+\sum_{a}\sum_{b} \mathbf{V}_{ab} \cdot X_{i+a,j+b}
$$
这便是通俗意义上的*卷积*（convolution），其中$\mathbf{V}$是应用于$X$的权值，权重数则由$O(n^2m^2)$减少到$O(nm)$。

再考虑*局部性*原则：为了收集$H_{ij}$的相关信息，局部性原则要求不能收集距离$(i,j)$太远的信息。因此可以设置一个限界$\Delta$，当$|a|>\Delta$或$|b|>\Delta$时，设$\mathbf{V}_{ij}=0$。此时定义变为：
$$
H_{ij}=u+\sum_{a=-\Delta}^\Delta \sum_{b=-\Delta}^\Delta \mathbf{V}_{ab} X_{i+a,j+b}
$$
负责这种变换的神经网络层称为卷积层：
- $\mathbf{V}_{ab}$称为*卷积核*（convolution kernel）、*滤波器*（filter）或权重，$u$被称为偏置
- 