
## 层、块

只具有单个输出的模型不涉及层和块的结构。

具有多个输出的模型将每一个阶段的神经元视为一*层*（layer），例如：输入层、输出层、隐藏层、激活函数层、Dropout层等。

当今的深度学习更热衷于讨论不小于单个层、不大于整个模型的层组，也就是*块*（block）。块由一或多个网络层拼接成，块间拼接可以形成更复杂的网络结构，并最终组成一整个模型。

<div style="background-color: white">
<img src="https://zh-v2.d2l.ai/_images/blocks.svg"/>
</div>

PyTorch中，`nn.Module`表示一个层，而`nn.Sequential`将各个层串联起来，组成一个块：

```python
import torch
from torch import nn
from torch.nn import functional as F

net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))

X = torch.rand(2, 20)
net(X)
```

块的输入为第一层的输入，块内每一层的输出作为下一层输入，块的输出即为最后一层的输出。

PyTorch等框架允许自定义一个块（而不是用`nn.Sequential`），用户需要：
1. 将输入数据作为其前向传播函数的参数。
2. 通过前向传播函数来*生成输出*。请注意，输出的形状可能与输入的形状不同。例如，我们上面模型中的第一个全连接的层接收一个20维的输入，但是返回一个维度为256的输出。
3. 计算其输出关于输入的*梯度*，可通过其反向传播函数进行访问。通常这是自动发生的。
4. 存储和访问前向传播计算所需的参数。
5. 根据需要*初始化*模型参数。

例如：一个包含隐藏层和输出层的块：

```python
class MLP(nn.Module):
    # 用模型参数声明层。这里，我们声明两个全连接的层
    def __init__(self):
        # 调用MLP的父类Module的构造函数来执行必要的初始化。
        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）
        super().__init__()
        self.hidden = nn.Linear(20, 256)  # 隐藏层
        self.out = nn.Linear(256, 10)  # 输出层

    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出
    def forward(self, X):
        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。
        return self.out(F.relu(self.hidden(X)))

X = torch.rand(2, 20)
net = MLP()
print(net(X))
```

基于这种思路，可以自己写一个连接各层的顺序块：

```python
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员
            # 变量_modules中。_module的类型是OrderedDict
            self._modules[str(idx)] = module

    def forward(self, X):
        # OrderedDict保证了按照成员添加的顺序遍历它们
        for block in self._modules.values():
            X = block(X)
        return X

X = torch.rand(2, 20)
net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
net(X)
```

已经定义的块（无论是自定义的还是`nn.Sequential`）在外部看来类似于一个层，尽管内部的实现逻辑可能是极其复杂的。

## 参数管理

训练之前，需要：
- 设计并实现网络架构
- 设置超参数：迭代次数、批次大小、学习率、权重衰减系数等

内容：
- 访问模型内的参数，用于调试、诊断、可视化
- 参数初始化
- 在不同模型组件间共享参数

### 访问模型参数

单个隐藏层MLP的前向传播：

```python
import torch
from torch import nn

net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))
X = torch.rand(size=(2, 4))
net(X)
```

对于上面的`net`，可以通过`net[2]`访问索引为2的网络层/块，并通过`state_dict()`方法获取其参数：

```python
print(net[2].state_dict())
```

（一种可能的）输出：

```
OrderedDict([('weight', tensor([[ 0.3016, -0.1901, -0.1991, -0.1220, 0.1121, -0.1424, -0.3060, 0.3400]])), ('bias', tensor([-0.0291]))])
```

输出表明：
- 全连接层包括权重（weight）和偏置（bias）两个参数。
- 所有参数都以浮点数存储。

访问具体某个参数（以`bias`为例）：

```python
# 输出类型：<class 'torch.nn.parameter.Parameter'>
print(type(net[2].bias))
# 输出tensor对象
print(net[2].bias)
# 输出tensor对象（纯数据）
print(net[2].bias.data)
# 输出梯度
print(net[2].bias.grad)
```

如果需要一次性访问所有参数，可以借助`named_parameters()`或`parameters()`方法：

```python
print(*[(name, param.shape) for name, param in net.named_parameters()])
# 输出：('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))
```

根据每个元组指明的参数名称，可以通过`state_dict()`直接访问：

```python
net.state_dict()['2.bias'].data
```

如果模型是由多个块相互嵌套而形成的，例如下面这种：

```python
def block1():
    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                         nn.Linear(8, 4), nn.ReLU())

def block2():
    net = nn.Sequential()
    for i in range(4):
        # 在这里嵌套
        net.add_module(f'block {i}', block1())
    return net

rgnet = nn.Sequential(block2(), nn.Linear(4, 1))
print(*[(name, param.shape) for name, param in rgnet.named_parameters()])
```

输出如下所示：

```
('0.block 0.0.weight', torch.Size([8, 4])) 
('0.block 0.0.bias', torch.Size([8])) 
('0.block 0.2.weight', torch.Size([4, 8])) 
('0.block 0.2.bias', torch.Size([4])) 
('0.block 1.0.weight', torch.Size([8, 4])) 
('0.block 1.0.bias', torch.Size([8])) 
('0.block 1.2.weight', torch.Size([4, 8])) 
('0.block 1.2.bias', torch.Size([4])) 
('0.block 2.0.weight', torch.Size([8, 4])) 
('0.block 2.0.bias', torch.Size([8])) 
('0.block 2.2.weight', torch.Size([4, 8])) 
('0.block 2.2.bias', torch.Size([4])) 
('0.block 3.0.weight', torch.Size([8, 4])) 
('0.block 3.0.bias', torch.Size([8])) 
('0.block 3.2.weight', torch.Size([4, 8])) 
('0.block 3.2.bias', torch.Size([4])) 
('1.weight', torch.Size([1, 4])) 
('1.bias', torch.Size([1]))
```

直接执行`print(rgnet)`则是这个结果：

```
Sequential(
  (0): Sequential(
    (block 0): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 1): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 2): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 3): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
  )
  (1): Linear(in_features=4, out_features=1, bias=True)
)
```

访问这种模型的参数有两种手段，第一种是具名访问，第二种则是下标访问：

```python
# 第一种方式：具名访问
rgnet.state_dict()['0.block 1.0.bias'].data
# 第二种方式：下标访问
rgnet[0][1][0].bias.data
```

### 参数初始化

如[[04-多层感知机#参数初始化]]所述，正确的初始化