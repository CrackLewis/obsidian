
## 层、块

只具有单个输出的模型不涉及层和块的结构。

具有多个输出的模型将每一个阶段的神经元视为一*层*（layer），例如：输入层、输出层、隐藏层、激活函数层、Dropout层等。

当今的深度学习更热衷于讨论不小于单个层、不大于整个模型的层组，也就是*块*（block）。块由一或多个网络层拼接成，块间拼接可以形成更复杂的网络结构，并最终组成一整个模型。

<div style="background-color: white">
<img src="https://zh-v2.d2l.ai/_images/blocks.svg"/>
</div>

PyTorch中，`nn.Module`表示一个层，而`nn.Sequential`将各个层串联起来，组成一个块：

```python
import torch
from torch import nn
from torch.nn import functional as F

net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))

X = torch.rand(2, 20)
net(X)
```

块的输入为第一层的输入，块内每一层的输出作为下一层输入，块的输出即为最后一层的输出。

PyTorch等框架允许自定义一个块（而不是用`nn.Sequential`），用户需要：
1. 将输入数据作为其前向传播函数的参数。
2. 通过前向传播函数来*生成输出*。请注意，输出的形状可能与输入的形状不同。例如，我们上面模型中的第一个全连接的层接收一个20维的输入，但是返回一个维度为256的输出。
3. 计算其输出关于输入的*梯度*，可通过其反向传播函数进行访问。通常这是自动发生的。
4. 存储和访问前向传播计算所需的参数。
5. 根据需要*初始化*模型参数。

例如：一个包含隐藏层和输出层的块：

```python
class MLP(nn.Module):
    # 用模型参数声明层。这里，我们声明两个全连接的层
    def __init__(self):
        # 调用MLP的父类Module的构造函数来执行必要的初始化。
        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）
        super().__init__()
        self.hidden = nn.Linear(20, 256)  # 隐藏层
        self.out = nn.Linear(256, 10)  # 输出层

    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出
    def forward(self, X):
        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。
        return self.out(F.relu(self.hidden(X)))

X = torch.rand(2, 20)
net = MLP()
print(net(X))
```

基于这种思路，可以自己写一个连接各层的顺序块：

```python
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员
            # 变量_modules中。_module的类型是OrderedDict
            self._modules[str(idx)] = module

    def forward(self, X):
        # OrderedDict保证了按照成员添加的顺序遍历它们
        for block in self._modules.values():
            X = block(X)
        return X

X = torch.rand(2, 20)
net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
net(X)
```

已经定义的块（无论是自定义的还是`nn.Sequential`）在外部看来类似于一个层，尽管内部的实现逻辑可能是极其复杂的。

## 参数管理

训练之前，需要：
- 设计并实现网络架构
- 设置超参数：迭代次数、批次大小、学习率、权重衰减系数等

内容：
- 访问模型内的参数，用于调试、诊断、可视化
- 参数初始化
- 在不同模型组件间共享参数

### 访问模型参数

单个隐藏层MLP的前向传播：

```python
import torch
from torch import nn

net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))
X = torch.rand(size=(2, 4))
net(X)
```

对于上面的`net`，可以通过`net[2]`访问索引为2的网络层/块，并通过`state_dict()`方法获取其参数：

```python
print(net[2].state_dict())
```

（一种可能的）输出：

```
OrderedDict([('weight', tensor([[ 0.3016, -0.1901, -0.1991, -0.1220, 0.1121, -0.1424, -0.3060, 0.3400]])), ('bias', tensor([-0.0291]))])
```

输出表明：
- 全连接层包括权重（weight）和偏置（bias）两个参数。
- 所有参数都以浮点数存储。

访问具体某个参数（以`bias`为例）：

```python
# 输出类型：<class 'torch.nn.parameter.Parameter'>
print(type(net[2].bias))
# 输出tensor对象
print(net[2].bias)
# 输出tensor对象（纯数据）
print(net[2].bias.data)
# 输出梯度
print(net[2].bias.grad)
```

如果需要一次性访问所有参数，可以借助`named_parameters()`或`parameters()`方法：

```python
print(*[(name, param.shape) for name, param in net.named_parameters()])
# 输出：('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))
```

根据每个元组指明的参数名称，可以通过`state_dict()`直接访问：

```python
net.state_dict()['2.bias'].data
```

如果模型是由多个块相互嵌套而形成的，例如下面这种：

```python
def block1():
    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                         nn.Linear(8, 4), nn.ReLU())

def block2():
    net = nn.Sequential()
    for i in range(4):
        # 在这里嵌套
        net.add_module(f'block {i}', block1())
    return net

rgnet = nn.Sequential(block2(), nn.Linear(4, 1))
print(*[(name, param.shape) for name, param in rgnet.named_parameters()])
```

输出如下所示：

```
('0.block 0.0.weight', torch.Size([8, 4])) 
('0.block 0.0.bias', torch.Size([8])) 
('0.block 0.2.weight', torch.Size([4, 8])) 
('0.block 0.2.bias', torch.Size([4])) 
('0.block 1.0.weight', torch.Size([8, 4])) 
('0.block 1.0.bias', torch.Size([8])) 
('0.block 1.2.weight', torch.Size([4, 8])) 
('0.block 1.2.bias', torch.Size([4])) 
('0.block 2.0.weight', torch.Size([8, 4])) 
('0.block 2.0.bias', torch.Size([8])) 
('0.block 2.2.weight', torch.Size([4, 8])) 
('0.block 2.2.bias', torch.Size([4])) 
('0.block 3.0.weight', torch.Size([8, 4])) 
('0.block 3.0.bias', torch.Size([8])) 
('0.block 3.2.weight', torch.Size([4, 8])) 
('0.block 3.2.bias', torch.Size([4])) 
('1.weight', torch.Size([1, 4])) 
('1.bias', torch.Size([1]))
```

直接执行`print(rgnet)`则是这个结果：

```
Sequential(
  (0): Sequential(
    (block 0): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 1): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 2): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 3): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
  )
  (1): Linear(in_features=4, out_features=1, bias=True)
)
```

访问这种模型的参数有两种手段，第一种是具名访问，第二种则是下标访问：

```python
# 第一种方式：具名访问
rgnet.state_dict()['0.block 1.0.bias'].data
# 第二种方式：下标访问
rgnet[0][1][0].bias.data
```

### 参数初始化

如[[04-多层感知机#参数初始化]]所述，正确的参数初始化可以有助于避免或减轻梯度消失和梯度爆炸问题。

PyTorch内定义的初始化器在`nn.init`中，如正态初始化（`nn.init.normal_`）、全零初始化（`nn.init.normal_`）、常量初始化（`nn.init.constant_`）等。

利用初始化器可以较方便地初始化某一层或某一块的参数。注意要用`<模型名>.apply`应用初始化方法：

```python
def init_normal(m):
    if type(m) == nn.Linear:
	    # 权重初始化：正态分布，方差0.01
        nn.init.normal_(m.weight, mean=0, std=0.01)
        # 偏移初始化：全0
        nn.init.zeros_(m.bias)
net.apply(init_normal)
# 输出模型权重和偏移
net[0].weight.data[0], net[0].bias.data[0]
```

或者权重可以采用常量初始化：

```python
nn.init.constant_(m.weight, 1)
```

另外，之前提到的*Xavier初始化*也是内置的初始化器，它的分布方差为$\dfrac{2}{n+m}$，其中$n,m$分别为输入、输出维度：

```python
nn.init.xavier_uniform_(m.weight)
```

也可以通过修改参数的`data`成员，实现对参数值的修改。例如实现参数的如下分布：
$$
w\sim \left\{\begin{matrix}
U(5,10), & p=\dfrac{1}{4}, \\
0, & p=\dfrac{1}{2}, \\
U(-10,-5), & p=\dfrac{1}{4}.
\end{matrix}\right.
$$

```python
def my_init(m):
    if type(m) == nn.Linear:
        print("Init", *[(name, param.shape)
                        for name, param in m.named_parameters()][0])
        nn.init.uniform_(m.weight, -10, 10)
        # 对不超过5的输出实施截断
        m.weight.data *= m.weight.data.abs() >= 5

net.apply(my_init)
net[0].weight[:2]
```

### 参数绑定

如果用同一个全连接层分别初始化某个块的若干层，那么这些层将共用这个全连接层的参数。这种机制称为*参数绑定*（argument binding）。

例如：

```python
shared = nn.Linear(8, 8)
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                    shared, nn.ReLU(),
                    shared, nn.ReLU(),
                    nn.Linear(8, 1))
```

随机执行一次前向传播：

```python
X = torch.rand(size=(2, 4))
net(X)
```

对比两个网络层的参数，发现它们一致：

```python
print(net[2].weight.data[0] == net[4].weight.data[0])
# 输出：tensor([True, True, ...])

net[2].weight.data[0, 0] = 100
print(net[2].weight.data[0] == net[4].weight.data[0])
# 修改其中一个层的参数后，对比仍然一致
```

可见：由同一个网络层初始化的各个网络层参数值相等，并且由相同的张量绑定在一起。

当参数绑定时，由于模型参数包含梯度，因此在反向传播期间第二个隐藏层（即第三个神经网络层）和第三个隐藏层（即第五个神经网络层）的梯度会加在一起。

## 延后初始化

*延后初始化*（deferred initialization）：模型初始时并不知道具体的输入规模，直到数据第一次通过模型传递时，框架才会动态地推断每个层的大小。

遗憾的是，PyTorch的延后初始化层（`nn.LazyLinear`）仍然在验证中，并不能稳定使用。

## 自定义层

与自定义块类似，自定义层有时也是必要的，用于执行一些特定职能。

### 不带参数的层

定义一个自定义的”居中化“层，在不改变输入的相对差值时，使输入的平均值归0：

```python
import torch
from torch import nn

class CenteredLayer(nn.Module):
	def __init__(self):
		super().__init__()

	def forward(self, X):
		return X - X.mean()

layer = CenteredLayer()
layer(torch.FloatTensor([1, 2, 3, 4, 5]))
```

这是一个典型的不带参数的网络层，有多少维输入就有多少维输出。

### 带参数的层

例如：自行利用`nn.Parameter`实现一个全连接层：

```python
class MyLinear(nn.Module):
    def __init__(self, in_units, units):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, units))
        self.bias = nn.Parameter(torch.randn(units,))
    def forward(self, X):
        linear = torch.matmul(X, self.weight.data) + self.bias.data
        return F.relu(linear)

linear = MyLinear(5, 3)
linear.weight
```

无论是带参数的还是不带参数的层，它们都：
- 必须派生`nn.Module`。
- 必须实现构造方法和`forward`方法。
- 可以用于更高层次网络结构的搭建。

## 模型本地存储

存储模型为什么是必要的：
- 为了下一次训练/预测时不必重新训练整个模型
- 为了保存已有的训练成果

### 存储和读取张量

对于一个PyTorch张量`x`：

```python
x = torch.arange(4)
```

可通过`torch.save(x, fname)`的方法将其转存在名为`fname`的本地文件中：

```python
torch.save(x, 'x-file')
```

对于存储在本地的张量，可以通过`torch.load(fname)`方法将其加载到程序中：

```python
x2 = torch.load('x-file')
x2.equal(x) # True
```

存储、读取*张量列表*也是允许的：

```python
y = torch.zeros(4)
torch.save([x, y], 'x-file2')
x2, y2 = torch.load('x-file2')
(x2.equal(x), y2.equal(y))
```

PyTorch还允许存储、读取*字符串到张量的字典*。这常用于后面存储模型的`state_dict`：

```python
mydict = {'x': x, 'y': y}
torch.save(mydict, 'mydict')
mydict2 = torch.load('mydict')
mydict2
```

### 存储和读取模型参数

对于一个PyTorch网络结构，无论是PyTorch内置的还是自定义的网络层、块、模型，都可以通过`state_dict`访问它所有的可见参数：

```python
net = nn.Sequential(
	 nn.Linear(10, 20),
	 nn.ReLU(),
	 nn.Linear(20, 10),
	 nn.ReLU()
)

net.state_dict()
```

假设经过了大量训练后，模型需要暂存到本地，以便后续加载用于继续训练或预测任务。这时适用`torch.save`方法保存`state_dict`：

```python
torch.save(net.state_dict(), 'net.params')
```

如果需要从本地文件加载模型，则先执行`torch.load`方法加载数据，再执行`load_state_dict`方法将数据转存至模型。注意，本地文件所存模型必须和定义的模型一致：

```python
net.load_state_dict(torch.load('net.params'))
```

## 使用GPU

in a nutshell: CPU=poop, GPU=good

### 计算设备

计算设备指操作数据的计算机组件，主要包括CPU和GPU。CPU更适合执行一串指令，但不太适合大量冗余重复的数据计算。GPU目前为止比CPU更适合重复数据计算。

每个PyTorch张量都有所属的*计算设备*（device），也称为*环境*（context）。所有张量默认是分配到CPU的，