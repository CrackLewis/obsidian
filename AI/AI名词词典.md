
## A

## B

- BatchNorm：批次内各通道归一化
- BP：梯度反向传播算法

## C

- [CLIP](https://proceedings.mlr.press/v139/radford21a/radford21a.pdf)：Contrastive Language-Image Pre-training，图文对比预训练方法
- CNN：Convolutional Neural Network，卷积神经网络

## D

## E

## F

## G

## H

## I

## J

## K

## L

- LayerNorm：样本内特征值归一化

## M

- MLP：Multi-Layer Perceptron，多层感知机

## N

## O

## P

## Q

## R

- ResNet：Residual deep neural Network，带残差连接的深层网络
- ReLU：一种非饱和激活函数，满足$\text{ReLU}(x)=\max(0,x)$.
- RNN：recurrent neural network，循环神经网络

## S

- Sigmoid：一种饱和激活函数，满足$\text{Sigmoid}(x)=1/(1+e^{-x})$.
- Softmax：柔性最大传递函数，满足$\text{Softmax}(x_1,\cdots,x_n)=1/\sum_{i=1}^n e^{x_i} \cdot(e^{x_1},\cdots,e^{x_n})$
- SVM：support vector machine，支持向量机

## T

- tanh：双曲正切函数
- Transformer：2017年提出的一种仅依赖注意力机制的序列转录模型

## U

## V

## W

## X

## Y

## Z