
参考资料：
- [Prompting Guide](https://www.promptingguide.ai/zh)

## 模型设置

- token多样性：
	- `temperature`：参数越小，结果越确定，反之越多样化
	- `top_p`：与temperature类似，越小表示选择越确定
	- 区别：tokens中只有包含`top_p`概率质量的才会被考虑用于响应
	- 一般调一个超参就可以
- 响应长度控制：
	- `max_length`：输出最大长度。防止LLM生成冗长文本
	- `stop_sequence`：停止序列，防止LLM继续生成token
- 重复token控制：
	- `freq_penalty`：对下一个生成token的惩罚，与token出现的次数成比例。抑制*停用词*（最常出现词元）的过度出现
	- `pres_penalty`：对重复token的惩罚，但每个重复token的惩罚相同。抑制响应中重复词元的类别数
	- 单次调一个参数就可以

以Ollama的Modelfile为例：

```dockerfile
FROM ./deepseek-r1-distill-qwen-1.5b

PARAMETER stop <|eot|>
PARAMETER top_p 0.9
PARAMETER temperature 1.0
```

## 提示词设计

提示词常见格式：
- Q/A格式：`Q: xxx; A: xxx; Q: xxx; A: xxx; Q: yyy; A:`。最后一个Q是你想要问的问题，最后一个A留白，让LLM补充
- 以问号结束的一个问题
- 一条祈使句表示指令

*要素*：
- 指令：你想让LLM干什么
- 上下文：与指令相关的问题背景、上下文信息
- 输入数据
- 输出提示：引导LLM如何输出

要素示例：第一行为指令和输出提示，第二行为输入和上下文。

```
请将文本分为中性、否定或肯定
文本：我觉得食物还可以。
情绪：
```

设计提示的通用技巧：
- 明确指令类型（写入、分类、翻译、总结、排序等）
- 具体明确你希望模型执行的指令和任务。提示越具描述性，结果越好
- 更好的提示应当是非常具体、简洁并且切中要点的
- 避免说不要做什么，而应该说要做什么

### 提示词示例

文本概括：

```
Antibiotics are a type of medication used to treat bacterial infections. They work by either killing the bacteria or preventing them from reproducing, allowing the body’s immune system to fight off the infection. Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions, or sometimes administered intravenously. They are not effective against viral infections, and using them inappropriately can lead to antibiotic resistance.

Explain the above in one sentence:
```

信息提取：

```
Author-contribution statements and acknowledgements in research papers should state clearly and specifically whether, and to what extent, the authors used AI technologies such as ChatGPT in the preparation of their manuscript and analysis. They should also indicate which LLMs were used. This will alert editors and reviewers to scrutinize manuscripts more carefully for potential biases, inaccuracies and improper source crediting. Likewise, scientific journals should be transparent about their use of LLMs, for example when selecting submitted manuscripts.

Mention the large language model based product mentioned in the paragraph above:
```

问答：

```
Answer the question based on the context below. Keep the answer short and concise. Respond "Unsure about answer" if not sure about the answer.

Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986, it was approved to help prevent organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use.

Question: What was OKT3 originally sourced from?

Answer:
```

文本分类：

```
Classify the text into neutral, negative or positive. 

Text: I think the vacation is okay.
Sentiment: neutral

Text: I think the food was okay. 
Sentiment:
```

对话：

```
The following is a conversation with an AI research assistant. The assistant tone is technical and scientific.

Human: Hello, who are you?
AI: Greeting! I am an AI research assistant. How can I help you today?

Human: Can you tell me about the creation of blackholes?
AI:
```

代码生成：

```
"""
Table departments, columns = [DepartmentId, DepartmentName]
Table students, columns = [DepartmentId, StudentId, StudentName]
Create a MySQL query for all students in the Computer Science Department
"""
```

推理：

```
The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. 
Solve by breaking the problem into steps. First, identify the odd numbers, add them, and indicate whether the result is odd or even. 
```

## 提示技术

### 零样本提示（zero-shot）

提示词中不包含任何样本。

零样本提示需要一些技术手段支持：
- 指令调整：在通过指令描述的数据集上微调模型
- RLHF：来自人类反馈的强化学习手段

### 少样本提示（few-shot）

随着任务复杂度提升，零样本prompt可能无法得到预期结果，需要添加少量示例样本，使LLM尽量理解任务并返回正确的结果。

对于最先进的模型，1-shot对于一些日常任务已经足够。一些更困难的任务可能需要3-shot、5-shot甚至是10-shot。

*few-shot的限制*：在处理一些复杂任务尤其是推理任务时，它不能真正理解任务的要义，有可能生成错误甚至不相关的答案。
- 突破此类限制的手段：微调LLM，思维链（chain of thought, CoT）提示

### 链式思考（CoT）提示

*使用部分具有足够规模或推理功能的模型时*，在提示样本内增加得到答案的推理过程，可以使LLM更好地理解任务要义，从而能够生成正确结果。

![](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&w=1920&q=75)

例如当提供如下输入时：

```
Q: 这组数中的奇数加起来是偶数：4、8、9、15、12、2、1。
A：将所有奇数相加（9、15、1）得到25。答案为False。
Q: 这组数中的奇数加起来是偶数：15、32、5、13、82、7、1。
A：
```

稍微出名的LLM都会返回如下输出：

```
将所有奇数相加（15、5、13、7、1）得到41。答案为False。
```

而一些训练欠佳或无法推理的LLM，仍可能返回错误结果。

*零样本CoT*：在输入提示中显式要求模型启用逐步思考，如：“让我们逐步思考：xxx”
- 在没有或者很少有样本可供参考时，这个prompt很有用

*自动思维链*（automatic chain-of-thought, Auto-CoT）：
- 背景：人工prompt可能导致次优解，零样本CoT可能在生成链内出错
- 目的：对具有多样性的问题进行采样，并生成推理链来构建演示
- 两个阶段：
	- **问题聚类**：将给定问题划分为几个聚类
	- **演示抽样**：从每类中选择一个具有代表性的问题，并使用带有*简单启发式*的Zero-Shot-CoT生成其推理链
		- 简单启发式方法可以是问题长度（e.g. 60 tok's）和推理步骤数（e.g. 5 steps）

![[Pasted image 20250225180851.png]]

thesis: [2210.03493](https://arxiv.org/abs/2210.03493)

code: [amazon-science/auto-cot](https://github.com/amazon-science/auto-cot)

### 自我一致性

*自我一致性*（self consistency）：
- 旨在“替换CoT提示中使用的天真贪婪解码方法”
- 想法是通过少样本 CoT 采样多个不同的推理路径，并使用生成结果选择最一致的答案。这有助于提高 CoT 提示在涉及算术和常识推理的任务中的性能

例如，这个问题有可能得到35岁之类的荒谬回答：

```
当我6岁时，我的妹妹是我的一半年龄。现在我70岁了，我的妹妹多大？
```

但可以使用自我一致性，通过少样本CoT采用不同推理路径，使LLM利用这些样本选择最合适的答案：

```
Q：林中有15棵树。林业工人今天将在林中种树。完成后，将有21棵树。林业工人今天种了多少棵树？
A：我们从15棵树开始。后来我们有21棵树。差异必须是他们种树的数量。因此，他们必须种了21-15 = 6棵树。答案是6。

Q：停车场有3辆汽车，又来了2辆汽车，停车场有多少辆汽车？
A：停车场已经有3辆汽车。又来了2辆。现在有3 + 2 = 5辆汽车。答案是5。

Q：Leah有32块巧克力，她的姐姐有42块。如果他们吃了35块，他们总共还剩多少块？
A：Leah有32块巧克力，Leah的姐姐有42块。这意味着最初有32 + 42 = 74块巧克力。已经吃了35块。因此，他们总共还剩74-35 = 39块巧克力。答案是39。

Q：Jason有20个棒棒糖。他给Denny一些棒棒糖。现在Jason只有12个棒棒糖。Jason给Denny多少棒棒糖？
A：Jason有20个棒棒糖。因为他现在只有12个，所以他必须把剩下的给Denny。他给Denny的棒棒糖数量必须是20-12 = 8个棒棒糖。答案是8。

Q：Shawn有五个玩具。圣诞节，他从他的父母那里得到了两个玩具。他现在有多少个玩具？
A：他有5个玩具。他从妈妈那里得到了2个，所以在那之后他有5 + 2 = 7个玩具。然后他从爸爸那里得到了2个，所以总共他有7 + 2 = 9个玩具。答案是9。

Q：服务器房间里有9台计算机。从周一到周四，每天都会安装5台计算机。现在服务器房间里有多少台计算机？
A：从周一到周四有4天。每天都添加了5台计算机。这意味着总共添加了4 * 5 =
20台计算机。一开始有9台计算机，所以现在有9 + 20 = 29台计算机。答案是29。

Q：Michael有58个高尔夫球。星期二，他丢失了23个高尔夫球。星期三，他又丢失了2个。星期三结束时他还剩多少个高尔夫球？
A：Michael最初有58个球。星期二他丢失了23个，所以在那之后他有58-23 = 35个球。星期三他又丢失了2个，所以现在他有35-2 = 33个球。答案是33。

Q：Olivia有23美元。她用每个3美元的价格买了五个百吉饼。她还剩多少钱？
A：她用每个3美元的价格买了5个百吉饼。这意味着她花了15美元。她还剩8美元。

Q：当我6岁时，我的妹妹是我的一半年龄。现在我70岁了，我的妹妹多大？
A：
```

所有输出都会参与一个*多数投票*（majority vote）以遴选最终输出。大部分的输出都是正确的，因此正确答案作为最终输出：

```
当叙述者6岁时，他的妹妹是他年龄的一半，也就是3岁。现在叙述者70岁了，他的妹妹应该是70-3 = 67岁。答案是67。
```

### 生成知识提示

进一步改进的思路是，生成知识以作为提示的一部分。

![[Pasted image 20250225233857.png]]

例如模型可能不知道高尔夫球的目标是得分最小化。因此它会对这个问题回答“是”：

```
高尔夫球的一部分是试图获得比其他人更高的得分。是或否？
```

通过few-shot方法生成一些关于高尔夫球的知识：

```
输入：希腊比墨西哥大。
知识：希腊的面积约为131,957平方公里，而墨西哥的面积约为1,964,375平方公里，使墨西哥比希腊大了1,389%。

输入：眼镜总是会起雾。
知识：当你的汗水、呼吸和周围的湿度中的水蒸气落在冷的表面上，冷却并变成微小的液滴时，会在眼镜镜片上产生冷凝。你看到的是一层薄膜。你的镜片相对于你的呼吸会比较凉，尤其是当外面的空气很冷时。

输入：鱼有思考能力。
知识：鱼比它们看起来更聪明。在许多领域，如记忆力，它们的认知能力与或超过非人类灵长类动物等“更高级”的脊椎动物。鱼的长期记忆帮助它们跟踪复杂的社交关系。

输入：一个人一生中吸烟很多香烟的常见影响是患肺癌的几率高于正常水平。
知识：那些一生中平均每天吸烟不到一支香烟的人，患肺癌的风险是从不吸烟者的9倍。在每天吸烟1到10支香烟之间的人群中，死于肺癌的风险几乎是从不吸烟者的12倍。

输入：一块石头和一颗卵石大小相同。
知识：卵石是一种根据Udden-Wentworth沉积学尺度的颗粒大小为4到64毫米的岩屑。卵石通常被认为比颗粒（直径2到4毫米）大，比卵石（直径64到256毫米）小。

输入：高尔夫球的一部分是试图获得比其他人更高的得分。
知识：
```

输出为：

```
高尔夫球的目标是以最少的杆数打完一组洞。一轮高尔夫球比赛通常包括18个洞。每个洞在标准高尔夫球场上一轮只打一次。每个杆计为一分，总杆数用于确定比赛的获胜者。
```

利用这个输出的知识构造新的prompt：

```
问题：高尔夫球的一部分是试图获得比其他人更高的得分。是或否？

知识：高尔夫球的目标是以最少的杆数打完一组洞。一轮高尔夫球比赛通常包括18个洞。每个洞在标准高尔夫球场上一轮只打一次。每个杆计为一分，总杆数用于确定比赛的获胜者。

解释和答案： 
```

此时模型则可以正确回答：

```
不是，高尔夫球的目标不是获得比其他人更高的得分。相反，目标是以最少的杆数打完一组洞。总杆数用于确定比赛的获胜者，而不是总得分。
```

### Prompt Chaining

*链式提示*（prompt chaining）：确定子任务后，将子任务的提示词提供给语言模型，得到的结果作为新的提示词的一部分。

考虑到LLM可能无法用一个非常详细的提示完成任务，拆分为多个提示可以让提示链对生成的回应执行转换或其他处理，直到达到期望结果。

例如：根据大型文本文档回答问题的任务中，单个提示词有可能达不到预期的效果。可以考虑设计两个不同提示：第一个要求提取引文，第二个要求根据原文和提取的引文回答问题。

以OpenAI Playground为例，在第一个提示中，构建`system`输入如下：

```
你是一个很有帮助的助手。你的任务是根据文档回答问题。第一步是从文档中提取与问题相关的引文，由####分隔。请使用<quotes></quotes>输出引文列表。如果没有找到相关引文，请回应“未找到相关引文！”。
####
{{文档}}
####
```

构建`user`输入如下：

```
问题：文档中提到了哪些提示词技巧？
```

第一个提示的输出：

```
<quotes>
- Chain-of-thought (CoT) prompting[27]
- Generated knowledge prompting[37]
- Least-to-most prompting[38]
- Self-consistency decoding[39]
- Complexity-based prompting[41]
- Self-refine[42]
- Tree-of-thought prompting[43]
- Maieutic prompting[45]
- Directional-stimulus prompting[46]
- Textual inversion and embeddings[59]
- Using gradient descent to search for prompts[61][62][63][64]
- Prompt injection[65][66][67]
</quotes>
```

利用第一个提示及其输出，构造第二个提示：

```
根据从文档中提取的相关引文（由<quotes></quotes>分隔）和原始文档（由####分隔），请构建对问题的回答。请确保答案准确、语气友好且有帮助。
####
{{文档}}
####
<quotes>
- Chain-of-thought (CoT) prompting[27]
- Generated knowledge prompting[37]
- Least-to-most prompting[38]
- Self-consistency decoding[39]
- Complexity-based prompting[41]
- Self-refine[42]
- Tree-of-thought prompting[43]
- Maieutic prompting[45]
- Directional-stimulus prompting[46]
- Textual inversion and embeddings[59]
- Using gradient descent to search for prompts[61][62][63][64]
- Prompt injection[65][66][67]
</quotes>
```

第二个提示的输出：

```
文档中提到的提示技术包括：
1. Chain-of-thought (CoT) prompting[27]
2. Generated knowledge prompting[37]
3. Least-to-most prompting[38]
4. Self-consistency decoding[39]
5. Complexity-based prompting[41]
6. Self-refine[42]
7. Tree-of-thought prompting[43]
8. Maieutic prompting[45]
9. Directional-stimulus prompting[46]
10. Textual inversion and embeddings[59]
11. Using gradient descent to search for prompts[61][62][63][64]
12. Prompt injection[65][66][67]
每一种技术都采用独特的策略来增强或明确与大型语言模型的互动，以产生期望的结果。
```

### 思维树（ToT）

思维树（tree-of-thought, ToT）是[Yao et al. (2023)](https://arxiv.org/abs/2305.10601) 提出的一种提示框架，基于CoT提示进行总结，引导语言模型探索把思维作为中间步骤来解决通用问题。

ToT 维护着一棵思维树，思维由连贯的语言序列表示，这个序列就是解决问题的中间步骤。使用这种方法，LM 能够自己对严谨推理过程的中间思维进行评估。LM 将生成及评估思维的能力与搜索算法（如广度优先搜索和深度优先搜索）相结合，在系统性探索思维的时候可以向前验证和回溯。

![[Pasted image 20250226114923.png]]

对于不同的任务，ToT会定义不同的思维数、步骤数和每步的候选项数。

例如：解24点游戏任务内，LM会使用3个思维步骤，每个步骤保留5个最优候选项。ToT对该任务采取BFS策略，每一步思维的候选项都要求LM给出能否得到24的评估（sure/maybe/impossible），根据数论特征或绝对大小分类；本轮的最优候选将作为下一轮的输入。

### 检索增强生成（RAG）

一些不需要额外背景知识的常见任务（如：分析情绪、识别命名实体）可通过对预训练的LLM进行微调的方式完成。

*检索增强生成*（Retrieval-Augmented Generation, RAG）：
- 背景：对于部分更复杂、知识密集型的任务，模型本身不足以存储全部的有关知识
- 思路：给予LM访问外部知识源的能力（信息检索组件），该组件可以单独微调以修改内部知识，而不需要重新训练整个模型

RAG 会接受输入并检索出一组相关/支撑的文档，并给出文档的来源（例如维基百科）。这些文档作为上下文和输入的原始提示词组合，送给文本生成器得到最终的输出。这样 RAG 更加适应事实会随时间变化的情况。这非常有用，因为 LLM 的参数化知识是静态的。RAG 让语言模型不用重新训练就能够获取最新的信息，基于检索生成产生可靠的输出。

[Lewis et al. (2021)](https://arxiv.org/pdf/2005.11401.pdf)提出的一个通用RAG方法：
- 使用预训练的seq2seq作为参数记忆，用维基百科的密集向量索引作为非参数记忆

![[Pasted image 20250227014020.png]]

### 自动推理并使用工具（ART）

LLM完成任务除了依赖CoT还需要依赖各类工具

*自动推理并使用工具*（Automatic Reasoning and Tool-use）：
- 接到一个新任务时，从任务库中选择多步推理和使用工具的示范
- 在测试中，调用外部工具时，先暂停生成，将工具输出整合后继续接着生成

![[Pasted image 20250228153349.png]]

