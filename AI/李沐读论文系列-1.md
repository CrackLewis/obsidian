
## 1-如何读论文

[link](https://www.bilibili.com/video/BV1H44y1t75x)

| sections   | pass1 | pass2 | pass3 |
| ---------- | ----- | ----- | ----- |
| title      | +     |       |       |
| abs'       | +     |       |       |
| intro'     |       | +     |       |
| methods    |       | +     | +     |
| exp'       |       | +     | +     |
| conclusion | +     |       |       |

pass1：知道这个论文是干啥的。可以带过一些重要图表

pass2：弄清重要图表的意义，圈出重要文献

pass3：精读，弄清楚方法和实验细节

## 2,3-AlexNet

[link](https://www.bilibili.com/video/BV1ih411J7Kz)

对一个人来讲，你需要读到很多文章，然后去总结很多不同的优秀的研究者对这个世界的认识，然后形成自己独特的观点。

我觉得这个是一个研究者最重要的事情，每个人都需要有自己的观点，你的观点不对没关系，**没有人的观点是对的**，但是你一定有自己独特的观点，这样子才能够不一样。

如果你跟别人的观点都一样，那么你对这个世界贡献，可能就没那么多了。

——李沐

第一遍：
- 卖点很好：在ImageNet数据集上的top-1错误率只有17%
- conv, pool, fc, softmax, ...

第二遍：
- 训练耗时5-6天
- 一个比较复杂的网络结构：分gpu

## 4,5-ResNet

- [link](https://www.bilibili.com/video/BV1Fb4y1h73E)
- [blog-1](https://blog.csdn.net/weixin_44001371/article/details/134192776)

卖点：用更低成本训练更深网络

假设需要的映射是$H(x)$。残差连接的方案是：学习$F(x)=H(x)-x$，而非直接学习$H(x)$。

rel'd work:
- residual representation: 并非首次提出，而是*首次应用于深层网络训练*
- shortcut conn': 短路=非残差（identity），主路=残差

tricks:
- 深层网络训练：1000层+
- 残差结构
- BatchNorm

resnet blocks:
- BasicBlock：两个3x3卷积层，参数众多
- Bottleneck：1x1+3x3+1x1。两个1x1可增减feature map维度，减少参数量：
	- 前一个将256通道缩为64通道，后一个将64通道还原为256通道

![](https://i-blog.csdnimg.cn/blog_migrate/48a8fb75ea39bc78542668c6a7e4f5f7.png)

网络结构：
- 特征尺寸递减，通道数递增
- resnet18和resnet34采用BasicBlock结构，resnet50和更深的结构采用Bottleneck结构
	- 结构前到结构后有残差连接

![](https://i-blog.csdnimg.cn/blog_migrate/5c3b68225dbada38f671fdbe624658d7.png)

## 6-Transformer

- [link](https://www.bilibili.com/video/BV1pu411o7BE)
- [thesis-1](https://arxiv.org/pdf/2108.07258.pdf)：一篇200页大综述，建议将transformer作为基础模型

继MLP、CNN、RNN后的第四大类模型

### pass1

idea: 序列转录，输入文本序列，输出另一种文本序列

abstract:
- 主流序列转录模型：RNN encoder+decoder
- 表现好的主流模型：使用了attention机制
- 基于attention的transformer结构
- 实验总结：并行化、更少训练时间
- BLEU提分：
	- 英语-德语：提升2 BLEU
	- 英语-法语：SOTA，41.8 BLEU
- 能够很好的泛化到其他任务

conclusion:
- 本文介绍了transformer模型，把encoder/decoder结构替换为了*多头自注意力*
- 任务表现：机器翻译、纯注意力模型的其他任务

### pass2

Transformer VS CNN/RNN:
- Transformer **仅依赖 self-attention** 计算输入输出的表征，没有使用 sequence-aligned RNNs or convolution.

encoder-decoder架构：序列转录模型公认的较好结构
- encoder一次可能看到一整个句子
- auto-regressive：decoder输出又作为输入，产生下一个输出
- N个encoder/decoder玩叠叠乐

BN vs LN:
- BatchNorm是把同一批次内、*单个通道*（特征）的所有样本值视为同一分布
- LayerNorm是把*单一样本*的所有通道（特征）值视为同一分布

![](https://i0.hdslb.com/bfs/note/57586df9146de6af02bd9ca49644487a9c1141b5.png)

注意力机制：与query越相似的key，其对应value的比重越高
- 不同的相似度函数代表不同的注意力机制

scaled dot-product attention: 最简单的注意力机制
$$
\text{Attention}(Q,K,V)=\text{Softmax}\left(\dfrac{QK^T}{\sqrt{d_k}}\right)\cdot V
$$
- 为啥要除$\sqrt{d_k}$：防止softmax函数的梯度消失。$d_k$小的话，除不除无所谓

