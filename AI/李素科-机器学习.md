
讲义：course.pku.edu.cn

refs:
- Machine Learning Refined
- 
- Probabilistic Machine Learning

requirements:
- 每人一篇5k字论文：学术论文/综述/技术报告/案例报告
- 考勤

## lec1

向量范数：L1\<L2\<...\<L∞

矩阵范数：

## lec4-二阶优化方法

ToC:
- 二阶优化技术介绍
- 二阶泰勒级数的几何性质
- 牛顿法
- 使用牛顿法进行模型优化
- 不可微分优化函数

凸函数定义：P4。多元函数$f(x)$，若$\forall x_1,x_2\in \mathbb R^{n}, \lambda\in[0,1]$，使得：
$$
f(\lambda x_1+(1-\lambda)x_2)\le \lambda f(x_1)+(1-\lambda) f(x_2)
$$

单点凸性判断：该点$x=x_0$是否满足
- $f(x)$在$x=x_0$处可导
- Hessian矩阵$H(x_0)$半正定，即$\forall v\in R^{n}$，$v^T H(x_0)v\ge 0$

凹函数定义相反，判断要求可导+Hessian矩阵半负定

### 4.1&4.2

P12/101

*Jacobian矩阵*：设向量函数$f(x)=(f_1(x),f_2(x),\cdots,f_m(x))^T$，其中$x\in R^{n\times 1}$。Jacobian矩阵是一个矩阵，满足：
- 分子布局Jacobian矩阵：$J(f,x)=\left\{\dfrac{\partial f_i}{\partial x_j}\right\}_{m\times n}$。
- 分母布局Jacobian矩阵：$J'(f,x)=(J(f,x))^T=\left\{\dfrac{\partial f_j}{\partial x_i}\right\}_{n\times m}$。

*Hessian矩阵*：梯度的Jacobian矩阵。以分子布局为例，设$f:R^n\rightarrow R,x\in R^n$：
$$
H(f,x)=J\left(\dfrac{\partial f}{\partial x},x\right)=\left\{\dfrac{\partial^2 f}{\partial x_i \partial x_j}\right\}_{n\times n}
$$
- Hessian矩阵是一类特殊的Jacobian矩阵
- H矩阵的目标函数是标量函数，J矩阵的目标函数是向量函数

二阶优化技术：对于函数$g:R^n\rightarrow R^m$，若$v\in R^n$是平稳点，则判断$H(g,v)$的特征值：
- 若所有特征值均为正：极小值
- 若所有特征值均为负：极大值
- 若有正有负：鞍点

二阶泰勒级数：对于$g:R^n\rightarrow R^m$，在$v$点逼近函数$g(w)$：
$$
h(w)=g(v)+\nabla g(v)^T\cdot (w-v)+\dfrac{1}{2}(w-v)^T\nabla^2 g(v) (w-v)
$$
- 其中：$\nabla=\dfrac{\partial}{\partial w}$表示梯度。
- 几何性质：若$g(w)$在$w=v$处是凸的，则$h(w)$是凸函数

### 4.3-牛顿法

牛顿法vs梯度下降法：
- 牛顿法使用二阶导，梯度下降法使用一阶导
- 牛顿法也有弱点

标量函数的牛顿法：对于$g(w)$的二阶泰勒级数$h(w)$取一阶导，求导后令其为零，得到平稳点$w^*$：
$$
w_k\leftarrow w_{k-1}-\alpha\dfrac{\frac{d}{dw} g(w_{k-1})}{\frac{d^2}{dw^2} g(w_{k-1})}
$$
向量函数的牛顿法：
$$
w_k \leftarrow w_{k-1}-\alpha(\nabla^2 g(w_{k-1}))^{-1} \nabla g(w_{k-1})
$$
直观理解：如果$g(v)$在点$v=v_0$是凸的（一边陡一边缓），它会向缓的一边偏移，直到接近平稳点。
- 其中：$\alpha$为步长，控制了牛顿法收敛的速度

实际应用中，为了防止二阶导为零从而不可求逆，会加一个单位矩阵：
$$
w_k \leftarrow w_{k-1}-\alpha(\nabla^2 g(w_{k-1})+\varepsilon I)^{-1} \nabla g(w_{k-1})
$$

牛顿法 vs 梯度下降法：
- 没有梯度下降的“之字形”问题
- 处理非凸函数略显吃力
- 需要计算Hessian矩阵，空间复杂度高

### 4.4-牛顿法优化模型

P40-68

#### 用牛顿法优化模型

对*线性回归*运用牛顿法：
$$
\begin{split}
J(w)&=\dfrac{1}{2} [Dw-y]^T[Dw-y] \\
\nabla J(w)&= D^T Dw - D^T y \\
H&= \left[\dfrac{\partial\nabla J(w)}{\partial w}\right]^T=D^TD \\
w&\leftarrow w-H^{-1} [\nabla J(w)] = w-(D^TD)^{-1} [D^TD w - D^T y] \\
&=w-w+(D^TD)^{-1}D^T y \\ 
&=(D^TD)^{-1}D^T y
\end{split}
$$

用牛顿法优化*SVM*：设样本特征$x_i\in R^d$，$y_i\in \{-1,+1\}$，$i=1,2,\cdots,n$。

代价函数：
$$
J(w)=\sum_{i=1}^n \max \left\{0,1-y_i\left(x_i w\right)\right\}^2+\frac{\lambda}{2}\|w\|^2
$$
梯度与Hessian：
$$
\begin{split}
\nabla J(w)&=D^T \Delta_w(D w-y)+\lambda w \\
H&=D^T \Delta_w D+\lambda I
\end{split}
$$
更新公式：
$$
\begin{split}
w&\leftarrow w-H^{-1} \nabla J(w) \\
&=w-\left(D^T \Delta_w D+\lambda I\right)^{-1}\left[D^T \Delta_w(D w-y)+\lambda w\right]
\end{split}
$$

用牛顿法优化*逻辑回归*：
$$
J(w)=\sum_{i=1}^n \ln \left(1+\exp \left(-y_i x_i w\right)\right)
$$
详见P46-48

#### 共轭梯度法

牛顿法时空成本太高了

背景：
- 任何二次函数都能通过基变换，转变为若干单变量函数之和
- 二阶偏导连续函数的Hessian对称。对Hessian矩阵可求的特征向量彼此正交

共轭梯度法运用前提：目标函数必须是二次凸函数，且可计算梯度

共轭梯度法核心思想：
- 利用Hessian正交方向迭代，避免显式存储Hessian。
- 更新方向：$q_{t+1}=-\nabla J\left(w_{t+1}\right)+\beta_t q_t$。