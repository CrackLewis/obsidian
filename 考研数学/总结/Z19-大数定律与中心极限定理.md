
## 导图

![[Pasted image 20230512231010.png]]

## 知识点

### 依概率收敛

定义：随机变量$X$与随机变量序列$\{X_n\},n=1,2,3,\cdots$，若对任意的$\varepsilon >0$，有
$$
\displaystyle \lim_{n\rightarrow +\infty} P\{|X_n-X|\ge \varepsilon\}=0
$$
则称随机变量序列$\{X_n\}$**依概率收敛**于随机变量$X$，记作$X_n\overset{P}{\rightarrow}X(n\rightarrow \infty)$。

### 大数定律

意义：当试验次数足够多时，样本的均值会无限逼近期望值。例如，随机变量$X\sim U(0,1)$，而100次$X$试验的结果叠加起来不是$U(0,100)$，而在概率上会非常接近$100EX=50$。大数定律对这种现象进行了量化。

**切比雪夫大数定律**：
假设随机变量序列$\{X_n\},n=1,2,\cdots$相互独立，若方差$DX_i$存在且一致有上界（$\exists C,\forall i, DX_i\le C$），则$\{X_n\}$服从大数定律：
$$
\frac{1}{n} \sum_{i=1}^n X_i\overset{P}{\rightarrow} \frac{1}{n}\sum_{i=1}^n EX_i
$$
意义：只要[[Z18-随机变量的数字特征#^db47c7|方差]]存在且有界，不同分布、不同相关性的**随机变量的均值**在变量数量足够多的时候，都会趋近于这些变量的**期望均值**。

**伯努利大数定律**：
假设$\mu_n$是$n$重伯努利试验中事件$A$发生的次数，在每次试验中事件$A$发生的概率为$p(0\le p\le 1)$，则
$$
\displaystyle \lim_{n\rightarrow +\infty} P\left\{\left|\frac{\mu_n}{n}-p\right|<\varepsilon\right\}=1
$$
记作$\frac{\mu_n}{n}\overset{P}{\rightarrow}p$。
意义：切比雪夫大数定律的特例。

**辛钦大数定律**：
假设$\{X_n\},n=1,2,\cdots$是独立同分布的随机变量序列，若$EX_i=\mu$存在，则
$$
\displaystyle \lim_{n\rightarrow +\infty} P\left\{\left|\frac{1}{n} \sum_{i=1}^n X_i - \mu\right|<\varepsilon\right\}=1
$$
记作$\frac{1}{n} \sum X_i\overset{P}{\rightarrow} \mu$。
意义：切比雪夫大数定律的特例。

### 中心极限定理

**列维-林德伯格定理**：
假设$\{X_n\}$是独立同分布的随机变量序列，满足$EX_i=\mu$，$DX_i=\sigma^2$，则对任意的实数$x$，有
$$
\displaystyle \lim_{n\rightarrow +\infty} P\left\{\frac{\sum_{i=1}^n X_i-n\mu}{\sqrt{n}\sigma}\le x\right\}=\frac{1}{\sqrt{2\pi}} \int_{-\infty}^{x} e^{-\frac{t^2}{2}}dt=\Phi(x)
$$
意义：无论$X_i$服从何种分布，$\sum X_i$都会近似服从正态分布。

**棣莫弗-拉普拉斯定理**：
假设$Y_n\sim B(n,p)$，则对任意实数$x$，有
$$
\displaystyle \lim_{n\rightarrow +\infty} P\left\{\frac{Y_n-np}{\sqrt{np(1-p)}}\le x\right\}=\frac{1}{\sqrt{2\pi}} \int_{-\infty}^{x} e^{-\frac{t^2}{2}}dt=\Phi(x)
$$
意义：是列维-林德伯格定理的特例。

